Data testing
	D:\PraiseTheLord\HSBGInfotech\DataTesting

SQL
---------------------------------------------------------------------
---------------------------------------------------------------------

Module 1	Module 1: Introduction to RDBMS
---------------------------------------------------------------------

RDBMS (Relational Database Management System)
	Type of database system 
	Stores and manages data 
		in a structured and organized way. 
	Follows the relational model
		emphasizes relationships between different pieces of data. 

Key concepts of RDBMS:

	Tables: 
		Data is stored in tables
		Each table represents a specific entity 
			(e.g., customers, products, orders).
	Rows and Columns: 
		Tables consist of 
			rows (records) 
		and 
			columns (attributes). 
		Each row represents a single instance of the entity 
			(e.g., a specific customer record). 
		Columns define the characteristics of that entity 
			(e.g., customer name, address, email).
	Keys: 
		Each table has a 
			primary key, 
				a unique identifier for each record. 
				eliminates duplicate data entries. 
				e.g. empId in emp table 
				cannot be null
			foreign keys 
				reference primary keys in other tables
				establishing relationships between them.
				e.g. deptId in emp table 
			unique keys
				unique and null 
				one data can be null 
	Relationships: 
		Tables are not isolated data silos. 
		RDBMS allows you to define relationships between tables. 
		This enables you to efficiently retrieve and manipulate data 
			across multiple tables based on these connections.
	
	SQL (Structured Query Language): 
		Standard language for interacting with relational databases. 
		Allows you to perform various operations like 
			query data
			insert new records
			updat existing data
			delete data.
			CRUD:
				create 
				read 
				update 
				delete
	
Benefits of RDBMS:
	Structured Data Organization: 
		Data is organized in a clear and well-defined manner
			easier to 
				understand, 
				manage, and 
				query.
				
	Data Integrity: 
		ACID properties 
			ensure 
				data consistency and 
				accuracy.
	Data Relationships: 
		Ability to define relationships between 
			tables 
				simplifies complex data retrieval and manipulation.
	SQL Support: 
		SQL 
			widely used and 
			standardized language for 
				querying and 
				managing data.
			Modify the schema 
				DDL
					data definition language 
					create, drop, truncate 
				DML 
					data manupulation language 
					insert, update, delete
				DCL
					data control language 
					grant and revoke 
				DQL
					data query language 
					select 
				TCL
					transaction control language 
					
	Scalability: 
		RDBMS can scale to accommodate 
			large datasets and 
			high volumes 
				of transactions.
		But cannot scale like no-sql 
		


Transaction: 
	Collection of queries (CRUD)
		should work as a single unit 
	begin transaction 
		do operation 
	end transaction 
		commit 
		or 
		rollback 
		
Phantom Reads
-------------
	In database systems
		a Phantom Read 
			type of concurrency anomaly 
				that can occur when 
					multiple transactions are executed concurrently.   

	Here's how it happens:

	Transaction 1 
		reads a set of rows from a table that meet certain criteria 
			(e.g., all employees in the "Sales" department).   
	Transaction 2 
		inserts new rows into the table that also meet the criteria of Transaction 1.   
	Transaction 1 
		reads the same set of rows again (potentially within the same transaction).   
	Transaction 1 
		now sees the 
			new rows that were inserted by Transaction 2, 
				which were not present in the first read. 
				These "new" rows are like phantoms that appeared unexpectedly.   
	In simpler terms:

		Imagine you're looking for all red cars in a parking lot. 
		You make a note of all the red cars you see. 
		While you're still looking, 
			someone else drives a red car into the parking lot. 
		When you look again, 
			you see a new red car that wasn't there before â€“ 
				it's like a phantom car appeared.

	Why is this a problem?

		Phantom reads can lead to 
			inconsistent results and 
			data integrity issues, 
				especially in applications that rely on 
					accurate counts or 
					summaries of data.   

	How to prevent Phantom Reads:

		Higher Isolation Levels: 
			Using higher isolation levels like 
				Serializable or 
				Repeatable Read 
					can prevent phantom reads. 
			These levels 
				typically employ techniques like 
					locking to prevent other transactions 
						from modifying data that is 
							being accessed by a current transaction.   



Serializable Vs Repeatable read 
---------------------------------------
Repeatable Read
---------------
	Prevents: 
		Non-repeatable reads 
			(where a single transaction reads the same row multiple times, 
			and gets different values each time). 
	Allows: 
		Phantom reads 
			(where new rows meeting the same criteria as a previous read 
				appear in subsequent reads 
					within the same transaction).
	How it works: 
		Typically 
			uses row-level locks 
				to ensure that 
					no other transaction can modify a 
						row that has been read by the current transaction.
Serializable
---------------
	Prevents: 
		Both non-repeatable reads and phantom reads.
	How it works: 
		Employs 
			stricter locking mechanisms, 
			often involving predicate locks or 
			snapshot isolation. 
		This ensures that the 
			execution of concurrent transactions is 
				equivalent to 
				executing them serially 
					(one after the other), 
					preventing any anomalies. 
	Key Differences

		Phantom Reads: 
			Repeatable Read 
				allows phantom reads, 
				while Serializable prevents them. 
		Locking: 
			Serializable 
				typically uses more restrictive locking mechanisms, 
				potentially leading to 
					higher contention and 
					lower concurrency. 
		Performance: 
			Repeatable Read 
				generally provides 
					better performance than Serializable 
						due to less stringent locking.
	When to Use

		Repeatable Read: 
			Suitable for most applications 
				where strong consistency is required, 
				but the risk of phantom reads is acceptable.
		Serializable: 
			Necessary for applications that require the 
				highest level of isolation, 
					such as financial systems or 
					applications where data integrity is paramount. 
			However, it can impact performance due to increased locking.
	In Summary:

		Both isolation levels 
			aim to prevent inconsistencies in 
				concurrent transactions. 
		Repeatable Read 
			provides a good balance between 
				consistency 
				and 
				performance, 
		Serializable 
			offers the highest level of consistency 
			but may have a higher performance overhead. 
		The choice of isolation level depends on the specific requirements of the application.


Eventual Consistency 
--------------------------
	Eventual Consistency 
		consistency model used in distributed systems, 
			particularly in those that prioritize 
				availability and 
				fault tolerance 
					over immediate consistency.

	Here's a breakdown:

	Core Principle:

	Guarantees that 
		if no new updates are made to a given data item, 
			all accesses to that item will eventually 
				return the last updated value.

	Essentially, 
		it means that data will 
			eventually be consistent 
				across all replicas in the system, 
				but there might be 
					temporary inconsistencies 
					during the replication process.
	Key Characteristics:

		High Availability: 
			Systems with (e.g. NoSQL)
				eventual consistency 
					can tolerate network partitions and 
					node failures, 
					ensuring 
						high availability 
							even in challenging environments.
				but cannot gurantee consistency  			
		Scalability: 
			Well-suited for distributed systems that 
				need to handle 
					large volumes of data and 
					high write throughput.
		Limited Consistency Guarantees:
			Does not guarantee immediate consistency.
			May experience temporary inconsistencies during replication.
			Reads may return stale data for a brief period.
	Examples:

		DNS (Domain Name System): 
			DNS servers replicate data, 
			but changes may take some time to propagate across all servers.
		Social Media Feeds: 
			Updates to a user's status 
				may not be immediately visible to all followers.
		Distributed Caching Systems: 
			Caches may have slightly outdated data 
				while they are being updated from the primary source.
	When to Use Eventual Consistency:

		Applications that can tolerate temporary inconsistencies.
		Systems that prioritize 
			high availability and 
			fault tolerance 
				over strong consistency.
		Distributed systems with large datasets and high write throughput.

---------------------------------------------------------------------
	Master/slave architecture 
	Primary/secondary/territiary deployment
---------------------------------------------------------------------

Examples of Popular RDBMS:
	MySQL
	Oracle Database
	Microsoft SQL Server
	PostgreSQL

---------------------------------------------------------------------

	Definition and Purpose of RDBMS
---------------------------------------------------------------------


RDBMS: Definition and Purpose
RDBMS stands for Relational Database Management System. 
	Specific type of database 
		designed to 
			store and 
			manage 
				data in a structured and organized way
		following a set of principles called the relational model.

Definition:

	Relational: 
		Data is organized into related tables, 
		allow connections and comparisons 
			between different data points.
	Database Management System (DBMS): 
		Software that interacts with the database
			allow users to 
				create, 
				access, and 
				manipulate the data.
Purpose:

	Efficient Data Storage and Retrieval:
		Data 
			organized into tables 
				well-defined structures
				easy to find and retrieve specific information.
	Relationships between tables 
		enable you to query and analyze data 
			across multiple tables efficiently.
Data Integrity:
	RDBMS enforces data consistency 
		through features like 
			primary keys (unique identifiers) and 
			data types, 
				minimize errors and 
				ensure data accuracy.
	ACID properties
			(more on this latter)
		Atomicity, 
		Consistency, 
		Isolation, 
		Durability
			guarantee data integrity during transactions
			prevent data loss or corruption.
Scalability and Manageability:
	RDBMS can support 
		large datasets 
		high volumes of transactions
			suitable for complex applications.
			
		but cannot support very large dataset 	
	Structured data organization 
		simplifies 
			data management and 
			maintenance.
Standardized Access:
	SQL (Structured Query Language) 
		standard language 
			for interacting with RDBMS. 
		This allows for 
			easier data manipulation and 
			querying across different software systems 
				that support SQL.

	
---------------------------------------------------------------------
	Evolution and History of RDBMS
---------------------------------------------------------------------

The story of RDBMS 
	started in 1970s, 
	evolving from 
		more efficient and organized way to manage data. 
	
	Here's a glimpse into its history:

1. The Dawn of the Relational Model (1970):
Edgar F. Codd, a computer scientist at IBM, 
	published a paper titled 
		"A Relational Model of Data for Large Shared Banks." 
	This paper laid the foundation for the 
		relational model, 
		outline the core principles of 
		
		data organization in 
			tables with rows and columns
			establish relationships between them.

2. Prototype Development (1970s):
	Two key projects emerged:
	System R at IBM: 
		Developed by IBM as a research project 
			create a prototype RDBMS. 
			This eventually led to the 
				development of 
					DB2, a prominent commercial RDBMS.
	INGRES at University of California, Berkeley: 
		Initially an academic project, it later became a commercially available RDBMS.

3. The Rise of Commercial RDBMS (1970s - 1980s):

	The 1970s 
		first commercial RDBMS born, 
		Oracle, 
			released in 1979 by Relational Software (now Oracle Corporation). 
			followed by other 
				Ingres, 
				DB2, 
				SAP Sybase ASE, and 
				Informix.
	RDBMS gained significant traction due to its 
		advantages in 
			data 
				organization, 
				management, and 
				querying capabilities 
					compared to earlier hierarchical and network database models.
4. Standardization and SQL (1980s - 1990s):

	The 1980s 
		standardization of SQL (Structured Query Language) 
		by organizations like 
			ISO (International Organization for Standardization) and 
			ANSI (American National Standards Institute). 
		Standardized language for 
			interacting with relational databases.
	RDBMS matured significantly 
		during this era
		advancements in features like 
			indexing 
				for efficient data retrieval, 
			table joins 
				for combining data from multiple tables, and 
			transaction management 
				for data consistency.
5. The Internet Age and Beyond (1990s - Present):

	With the rise of the internet, 
		RDBMS continued to be a dominant force 
			due to its scalability and 
			ability to handle large datasets.
	Cloud-based RDBMS solutions 
		emerged, 
		offering flexible deployment options and 
		easier data management.
	While NoSQL (Not Only SQL) databases 
		gained popularity for 
			specific use cases with 
				unstructured or big data, 
		RDBMS remains a cornerstone for many applications 
			due to its 
				structured data organization and 
				strong data integrity features.


	
---------------------------------------------------------------------
	ACID properties
---------------------------------------------------------------------

ACID stands for 
	Atomicity, 
	Consistency, 
	Isolation, and 
	Durability. 

	These are four key properties that 
		guarantee the 
			reliability and 
			consistency of 
				transactions in a database management system (DBMS), 
			particularly RDBMS (Relational Database Management System).

Let's break down each property and its significance:

Atomicity:

	Ensures 
		transaction is treated as a single unit.
	Either 
		all changes within the transaction succeed, 
		or 
		none do.
	If one statement fails 
		previous should be rollback 
	This prevents partial updates to the database, 
		maintain data integrity.
	Lack of atomicity 
		inconsistency 
		
	Imagine transferring money between two accounts. 
		Atomicity ensures either both accounts are updated successfully (money transferred), 
		or neither is affected (no transfer occurs).
Consistency:

	Data is said to be consistent if it remains the same through the life of a transaction 
	Guarantees 
		transaction maintains the data according 
			to pre-defined rules (constraints).
	These rules can involve 
		data types, 
		relationships between tables, or 
		specific business logic.
	Consistency ensures the 
		data remains in a valid state 
			after a transaction.
	ways to achieve
		constraints 
		atomicity 
		isolation 
			
	For example, 
		a transaction updating customer information 
			might enforce a 
				rule that all emails must have a valid format (e.g., [email address removed]).
				
	Eventual consistency 
	
Isolation:

	Ensures 
		concurrent transactions are isolated 
			from each other, 
		prevents conflicts and 
		maintaining data integrity.
		
		can my inflight transaction see changes 
			made by other transactions 
	When multiple transactions 
		access or modify the same data simultaneously, 
		isolation 
			guarantees that each transaction 
				sees a consistent view of the data and 
				doesn't interfere with others.
	Imagine two users updating the same product quantity at the same time. 
	Isolation ensures 
		each update happens independently, 
		avoid conflicts 
			that could lead to incorrect inventory levels.
			
	---------------------------------------------------------------------
	Isolation leve types 
	READ UNCOMMITTED (READ UNCOMMITTED) / DIRTY READ:

		least restrictive level
		highest potential for concurrency.
		Transactions can see changes made by other transactions 
			that haven't even been committed yet (uncommitted data).
		This can lead to inconsistencies like 
			"dirty reads" 
			where a transaction reads data another transaction might eventually rollback.


	READ COMMITTED (READ COMMITTED) / NON-REPEATABLE READ:

		Transactions can only see changes committed by other transactions 
			by the time the reading query starts.
		eliminates "dirty reads" 
		allows "non-repeatable reads."
		Non-repeatable reads occur when 
			a transaction reads the same data twice and 
				gets different results because 
					another transaction committed a change in between.
		Default isolation level in Oracle Database 		
			prevents dirty reads 
				(reading uncommitted data) but 
				allows non-repeatable reads and phantom reads.

 
					
	REPEATABLE READ (REPEATABLE READ):

		Transactions can see data committed by 
			other transactions as 
				of the moment the reading transaction started.
		"repeatable reads" 
			transaction will always read the same data 
				for the same row within its execution.
		allows "phantom reads" 
			transaction might not see rows 
				inserted by other transactions 
					after it began reading.
		Default isolation level in MySQL database 			
			prevents dirty reads and non-repeatable reads, 
			but may allow phantom reads.		
			MySQL snapshot read for implementing this.
		
		two ways to achieve 
			1. Lock the row
				Serializability (next option)
			2. Snapshot 
				transaction will see the data that was snapshotted when it started
	SERIALIZABLE (SERIALIZABLE):

		This is the most restrictive level, 
			enforcing serializability.
		Transactions are executed one at a time
			simulating a serial execution order.
		This guarantees the highest level of data consistency 
			but significantly reduces concurrency.
	
	Choosing the Right Isolation Level:

		The appropriate isolation level depends on your specific application requirements. 
			Here's a general guideline:

		Use READ UNCOMMITTED 
			for read-heavy applications 
				where occasional inconsistencies are acceptable 
					for improved performance.
		Use READ COMMITTED for a balance between 
			consistency and concurrency in most situations.
		Use REPEATABLE READ for scenarios where consistent read results within a transaction are crucial.
		Use SERIALIZABLE for situations requiring absolute data consistency, 
			even at the cost of reduced concurrency.
	Setting the Isolation Level:

		You can set the isolation level globally for all sessions or for individual sessions using the following commands:

		Set globally (in my.cnf):
		[mysqld]
		transaction-isolation = level_name  -- Replace 'level_name' with the desired level (READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE)
		Set for a session:
		SQL
		SET SESSION TRANSACTION ISOLATION LEVEL level_name;

	------------------------
		
	
Durability:

	Guarantees 
		that once a transaction is committed (finalized), 
			the changes are permanently saved to the database.
	Even in case of system failures 
		(e.g., power outages), 
			database ensures the committed transactions persist.
	Durability protects against data loss and 
		ensures the database reflects the latest updates even after a restart.
		
		
	Durability Techniques
	----------------------
		MySQL's InnoDB storage engine
			default for most use cases
			utilizes a combination of durability techniques 
				to ensure data persistence:

				Write-Ahead Logging (WAL): 
					Cornerstone of durability in InnoDB. 
					All data modifications 
						(inserts, updates, deletes) 
							first written to a dedicated redo log file on disk 
								before being applied to the actual data files. 
					This log serves as a record of changes, 
						allowing recovery in case of failures.

				Checkpointing: 
					InnoDB periodically performs checkpoints
						writing a snapshot of the current database state 
							(including in-memory data structures) 
								to a permanent location on disk. 
					This creates a consistent point-in-time reference for recovery. 
					After a restart
						the database can recover to the latest checkpoint and 
							then replay the redo log entries 
								since that checkpoint to reach the most recent state.

				InnoDB Redo Log Files:  
					MySQL maintains multiple redo log files for redundancy. 
					If one log file fails
						recovery can proceed using the remaining logs. 
					Additionally, redo logs are written sequentially
						ensuring an ordered record of changes.

				Double-write: 
					This optional technique 
						can be enabled (with innodb_doublewrite setting) 
							for enhanced durability. 
					During a write operation
						the changes are written not only to the redo log 
							but also to the data files in the InnoDB buffer pool. 
					This double-write approach 
						minimizes the risk of data loss 
							if a failure occurs after the redo log write 
								but before the data file update completes.

				Flush Operations: 
					InnoDB periodically flushes data from the buffer pool 
						to the actual data files on disk. 
					data modifications are not 
						solely present in memory 
							but are persisted to the storage device.

		Additional Considerations:

		Durability Guarantees: 
			While WAL and checkpointing ensure eventual consistency 
				(data will be consistent after recovery), 
				data loss might occur if a failure happens between a write to the redo log and the subsequent data file update. 
				For stricter guarantees, techniques like synchronous replication to secondary servers can be employed.
		Configuration Options: 
			MySQL provides various configuration options 
				to fine-tune durability behavior. 
			For instance, the innodb_flush_log_at_trx_commit setting controls 
				when data is flushed from the redo log to disk 
				(options include before commit, after commit, or a combination).
		Conclusion:

		MySQL's InnoDB engine offers a robust set of durability techniques 
			through 
				redo logging, 
				checkpointing etc. 
			By understanding these techniques and considering your specific data security needs, 
				you can configure MySQL to provide the desired level of data persistence and minimize the risk of data loss.	
		
		
Adv. of RDBMS
-------------
	1. No data redundancy
		no duplicates
	2. Data concurrency 
		lock such that abnormalities can be prevented
		
	3. Data searching 
		built-in search/query/index
	4. Data integrity 
		datatypes, constaints etc. 
			data cannot violate the rules 

Disadv
------
	1. Rigid schema 
	2. Very high cost 
	3. Horizontal scaling capability 

---------------------------------------------------------------------
	Comparison with other Database Models (Hierarchical, Network)
---------------------------------------------------------------------
RDBMS vs. Hierarchical and Network Database Models
While RDBMS (Relational Database Management System) 
	dominates the data storage landscape, 
	other database models like 
		hierarchical and 
		network models 
			existed before and still have niche applications. 
	Here's a comparison to understand their strengths and weaknesses:

RDBMS:

	Structure: 
		Data is organized in tables 
			rows and columns. 
			Relationships between tables are defined through keys 
			(primary and foreign).
	Strengths:
	Structured and organized data 
		simplifies retrieval and management.
	Relationships between tables 
		enable powerful 
			data analysis across different data points.
	ACID properties ensure data integrity and consistency.
	Standardized access through SQL 
		makes it versatile for various applications.
Weaknesses:
	Less flexible for highly specialized data structures 
		(e.g., semi-structured data).
	Complex queries involving multiple joins 
		can impact performance for very large datasets.
Hierarchical Model:

	Structure: 
		Data is organized in a tree-like structure
			single parent node at the top and 
				child nodes branching out below. 
		Each child node can have only one parent.
	Strengths:
		Efficient for representing one-to-many relationships 
			(e.g., file systems, organizational charts).
		Simple data navigation within the hierarchy.
Weaknesses:
	Limited flexibility for representing 
		complex relationships beyond 
			one-to-many hierarchies.
	Difficult to establish relationships 
		between data in different parts of the hierarchy.
	Not suitable for large datasets or complex queries.
Network Model:

	Structure: 
		Data is organized in a more flexible network structure, 
			allow records (entities) 
				to have multiple parent nodes. 
		Relationships are established using pointers between records.
	Strengths:
		More flexible than hierarchical models 
			for representing complex 
				many-to-many relationships.
		Efficient for specific use cases 
			like computer aided design (CAD) data.
Weaknesses:
	Data complexity can increase with 
		numerous relationships, 
		challenging to manage and navigate.
	Querying data across 
		different record types can be cumbersome.
	Not as widely used or supported compared to RDBMS.
Choosing the Right Model:

	The best database model depends on the specific needs of your application and data structure:

RDBMS: 
	Ideal for structured data 
		with complex relationships, 
		data integrity needs, and 
		requirement for standard SQL access.
Hierarchical Model: 
	Well-suited for 
		one-to-many relationships and 
		applications like file systems or organizational structures.
Network Model: 
	Useful for complex many-to-many relationships
	but less common due to its 
		complexity compared to RDBMS.
---------------------------------------------------------------------
Module 2	Module 2: Features of RDBMS
---------------------------------------------------------------------
---------------------------------------------------------------------
	ACID Properties
---------------------------------------------------------------------
already covered
---------------------------------------------------------------------
	Data Integrity
---------------------------------------------------------------------
already covered - but talk about nosql 
---------------------------------------------------------------------
	Data Consistency
---------------------------------------------------------------------
already covered - but talk about nosql 

---------------------------------------------------------------------
	Scalability
---------------------------------------------------------------------


Scalability in RDBMS (Relational Database Management Systems)
Scalability 
	crucial aspect of RDBMS
	
	ability to handle increasing amounts of 
		data, 
		user requests, and 
		overall workload.  
	Here's a breakdown of scalability in RDBMS:

Challenges to Scalability:

	As RDBMS usage grows, it can face challenges like:

	Performance Bottlenecks: 
		Processing large datasets 
	or 
		complex queries 
			can strain 
				hardware resources, 
				effect: slow response times.
	Storage Capacity Limits: 
		As data volume increases, 
			the existing storage might not be sufficient 
				to accommodate the growing needs.
	Concurrency Issues: 
		With more users accessing the database simultaneously, 
			managing concurrent transactions and 
			maintaining data integrity 
				becomes more complex.
Approaches to Scaling RDBMS:

RDBMS offers two main approaches to address scalability challenges:

Vertical Scaling (Scale-Up):
	Focuses on increasing the capacity of the existing hardware resources.
Methods include:
	Upgrade 
		CPU (Central Processing Unit).
	Add CPU count	
	Adding more RAM (Random Access Memory) 
		to improve processing speed.
	Upgrading storage capacity 
		with additional hard drives or 
		transition to Solid State Drives (SSDs) 
			for faster data access.
Horizontal Scaling (Scale-Out):
	add more servers (nodes) 
		to distribute the workload 
			across multiple machines.
	Common techniques include:
		Sharding: 
			Dividing the database into smaller, 
			self-contained units (shards) 
				based on a specific key (e.g., user ID, product category). 
			Each shard resides on a separate server, distributing the load.
		Replication: 
			Creating copies of the 
				entire database or 
				specific tables 
					on multiple servers. 
			Improves read performance by 
				allowing queries to be 
					executed on any replica server.
		Cache 
			frequently accessed data 
Choosing the Right Approach:

The choice between vertical and horizontal scaling depends on several factors:

Cost: 
	Vertical scaling 
		cheaper
		costs increase significantly as hardware reaches its limits. 
	Horizontal scaling 
		cost-effective scaling by adding commodity hardware.
	Complexity: 
		Vertical scaling 
			is simpler to implement
		horizontal scaling 
			manage multiple servers and 
			data distribution, 
			increased complexity.
	Data Model and Access Patterns: 
		Certain data models and access patterns might benefit more 
			from one approach over the other.

Additional Considerations for Scalability:

Database Optimization: 
	Techniques like 
		proper indexing, 
		query optimization, and 
		denormalization 
			can improve RDBMS performance and scalability.
Database Monitoring: 
	Continuously monitoring 
		performance metrics and 
		resource utilization 
			helps identify bottlenecks 
			and 
			plan scaling strategies proactively.
Choosing the Right RDBMS: 
	Some RDBMS solutions are inherently more scalable than others. 
	Consider database options (NoSQL) that offer built-in features for 
		sharding, 
		replication, and 
		other scaling 
			functionalities.

---------------------------------------------------------------------
	Security
---------------------------------------------------------------------
Security in RDBMS (Relational Database Management Systems)
Securing your RDBMS is critical 
	to 
		protect sensitive information and 
		maintain data integrity. 
Overview of essential security measures for RDBMS:

	Database is the most important component to secure.
	Network level security 
	Machine level security 
	Application level security 
	Database level security 

	1. Access Control:

		Authentication: 
			Verify the identity of users 
			involves 
				usernames, passwords, 
				or 
				other forms of credentials.
		Authorization: 
			Define user permissions based on their roles. 
			Grant access to 
				specific data, 
				functionalities 
					(read, write, update, delete), and 
				tables based on the user's needs 
					(principle of least privilege).
	2. User Account Management:

		Strong Password Policies: 
			Enforce complex password requirements, 
			regular password changes
			avoid password reuse.
		Account Lockouts: 
			Implement mechanisms to lock accounts 
				after a certain number of failed login attempts to 
				prevent brute-force attacks.
		Disable Unused Accounts: 
			Regularly review and disable unused accounts to 
			minimize potential attack surfaces.
	3. Data Encryption:

		Encrypt data at rest: 
			Store sensitive data 
				in an encrypted format 
					within the database to 
			protect it 
				even if unauthorized users 
					gain access to the storage.
			Layered security 		
		Encrypt data in transit: 
			Use secure protocols like TLS 
				(Transport Layer Security) 
				to 
					encrypt data transmission 
						between applications and the database server
						prevent interception during network traffic.
	4. Database Activity Monitoring:

		Track user activity: 
			Monitor database access logs 
				to identify 
					suspicious activity, 
					unauthorized access attempts, or 
					unusual queries.
		Alerting: 
			Set up alerts for 
				anomalies 
			or 
				potential security breaches 
					based on specific activity patterns.
		
	5. Database Hardening:

		Minimize attack surface: 
			Disable unnecessary services and functionalities 
				on the database server 
				to reduce potential vulnerabilities.
		Keep software updated: 
			Regularly apply security patches and updates 
				to the database software 
					to address known vulnerabilities.
		Network Security: 
			Implement firewalls and network segmentation to 
				restrict access to the database server 
					from unauthorized network segments.
	6. Backups and Disaster Recovery:

		Regular Backups: 
			Create periodic backups of your database 
				to a secure location. 
			This allows restoring data 
				in case of 
					accidental deletion, 
					corruption, or 
					security incidents.
		Disaster Recovery Plan: 
			Develop a plan for recovering from potential disasters like 
				hardware failures, 
				natural disasters, 
				or cyberattacks. 
			This plan should outline procedures for 
				restoring data from backups and 
				resuming database operations.

Additional Security Considerations:

	Database Encryption Keys: 
		Securely manage database encryption keys. 
		Consider hardware security modules (HSMs) for additional key protection.
	Data Masking and Anonymization: 
		For sensitive data that needs to be accessed for 
			development or testing purposes, 
		consider 
			data masking or 
			anonymization techniques 
				to minimize exposure of real data.
	Regular Security Audits: 
		Conduct periodic security audits to 
			identify vulnerabilities and 
			ensure the effectiveness of security measures.
			

---------------------------------------------------------------------
	Transactions management in database
---------------------------------------------------------------------
			
			
Transactions in a database 
	fundamental concept 
	ensure 
		data integrity and 
		consistency, 
			especially in environments with concurrent access.

Key Concepts

	Unit of Work: 
		A transaction 
			single logical unit of work that 
			comprises one or more operations 
				(e.g., read, write, update, delete) on the database.
	ACID Properties: 
		Transactions must adhere to the ACID properties:
	Atomicity: 
		All operations within a transaction 
			executed as a single, indivisible unit. 
		Either 
			all operations succeed, or 
			none of them do. 
			If any operation fails, 
				entire transaction is rolled back, 
				leaving the database in its original state.
	Consistency: 
		A transaction 
			must leave the database in a consistent state. 
			i.e. transaction 
				must not violate any 
					constraints or 
					rules 
						defined for the database (e.g., foreign key constraints).
	Isolation: 
		Concurrent transactions 
			should not interfere with each other. 
		The effects of one transaction should not be visible to other concurrent transactions 
			until the first transaction is committed. 
		This prevents anomalies like 
			dirty reads, 
			non-repeatable reads, and 
			phantom reads.
	Durability: 
		Once a transaction is committed, 
			its changes are permanently recorded in the database and 
			will survive system failures (e.g., power outages, crashes).
Transaction Management

	Concurrency Control: 
		Techniques used to 
			manage concurrent access to the database and 
			ensure data consistency.
	Locking: 
		A common method where database objects 
			(rows, tables) are locked 
				to prevent other transactions from 
					accessing 
					or 
					modifying them. 
		Different types of locks include 
			shared locks (for reading) and 
			exclusive locks (for writing).
	Timestamping: 
		Assigns timestamps to transactions and 
			uses them to determine the order of execution.
	Multi-version Concurrency Control (MVCC): 
		Creates multiple versions of data to allow concurrent transactions 
			to read without blocking each other.
Example

Imagine a bank transfer transaction:

	Read: 
		Read the current balance from the sender's account.
	Deduct: 
		Deduct the amount from the sender's account.
	Credit: 
		Credit the amount to the recipient's account.
	Update: 
		Update the balances in both accounts.
If any of these steps fail (e.g., insufficient funds, network error), the entire transaction should be rolled back, ensuring that the sender's account is not debited without the recipient's account being credited.

Benefits of Transactions

	Data Integrity: 
		Ensures data consistency and prevents data corruption.
	Concurrency Control: 
		Allows multiple users to access and modify the database concurrently without interfering with each other.
	Reliability: 
		Provides a mechanism for recovering from failures and ensuring that data is not lost.
	Atomicity: 
		Guarantees that all-or-nothing behavior of operations within a transaction.			
---------------------------------------------------------------------
	Codd's rules 
---------------------------------------------------------------------


Codd's rules / Codd's twelve rules
	set of thirteen guidelines 
		proposed by Edgar F. Codd in 1970 
			to define a true relational database management system (RDBMS). 
	
	foundation for ensuring 
		data integrity, 
		consistency, and 
		efficient data manipulation 
			within a relational database.

Here's a breakdown of the core principles outlined in Codd's rules:

	Data Representation and Access (Rules 1-3):

		1. The Information Rule: 
			All data in the database 
				must be represented logically 
					in tables with rows and columns. 
			structured and organized approach to data storage.
		
		2. The Guaranteed Access Rule: 
			Every individual data element (value) 
				within a table must be logically accessible 
					using a combination of 
						table name, 
						primary key (unique identifier for a row)
						attribute name (column name). 
				No other methods, like 
					pointers, should be used for data access.
		3. Systematic Treatment of NULL Values: 
			Null values represent missing or unknown data. 
			Codd's rules 
				emphasize a consistent way to handle null values 
					to avoid ambiguity and 
					ensure data integrity.
	Data Independence and Integrity (Rules 4-10):

		4. Active Online Catalog: 
			The structure of the entire database, 
				including 
					table definitions, 
					data types, 
					constraints, and 
					relationships, 
				must be stored in an online catalog 
					(data dictionary) accessible by authorized users. 
			efficient management and 
			reduces reliance on external documentation.

		5. The Comprehensive Data Sublanguage Rule: 
			There should be a single language 
				capable of defining 
					database schema (structure), 
					data manipulation (insertion, update, deletion), and 
					data retrieval (queries). 
			SQL (Structured Query Language) in most RDBMS, 
				promotes consistency and 
			simplifies data management.

		6. View Updating Rule: 
			If a view 
				(a virtual table based on a logical query of underlying tables) 
				can be retrieved from the database, 
				it should also be logically updatable 
					using the same language. 
			This ensures data consistency between the view and the base tables.

		7. High-level Insert, Update, and Delete: 
			Data manipulation 
				(insert, update, delete) 
				operations should be achievable using the 
					high-level data sublanguage (usually SQL) 
						without resorting to low-level implementation details. 
				This simplifies data management and reduces errors.

		8. Physical Data Independence:  
			The physical storage and access methods of the database 
				should be independent of the logical structure 
					(tables, relationships). 
			This allows for changes to the physical storage 
				layout without impacting the way users 
					interact with the data through the data sublanguage (SQL).

		9. Logical Data Independence:  
			Changes to the logical structure of the database 
				(e.g., adding/removing tables, modifying relationships) 
				should not require modifications to application programs 
					that interact with the data 
						using the data sublanguage (SQL). 
			This promotes data flexibility and simplifies application maintenance.

		10. Integrity Independence:  
			Data integrity constraints 
				(rules that govern data validity) 
				should be definable in the relational data model 
				and 
				stored in the data catalog, 
				independent of the application programs 
					that use the data. 
			This centralizes data integrity rules and simplifies enforcement.

Distribution and Subversion (Rules 11-13):

	11. Distribution Independence: 
		The ability to distribute the database physically 
			across multiple locations or servers 
				should be transparent to users and applications. 
		The data sublanguage (SQL) 
			should handle distribution without 
				requiring modifications.
	12. Non-Subversion Rule: 
		There should be no way to bypass the relational data sublanguage (SQL) 
			to access or manipulate the database. 
		This ensures all data manipulation adheres to the 
			defined rules and maintains data integrity.
While some argue there are thirteen rules, the core principles lie within the first twelve.  
These rules provide a framework for building robust and reliable relational database systems.  
Although not all RDBMS implementations strictly adhere to every rule, 
	understanding these principles gives you a strong foundation for working with relational databases and ensuring data 
		quality and consistency.


---------------------------------------------------------------------
	Database schema design 
---------------------------------------------------------------------

Use Case: Online Bookstore

Requirement: Design a database schema for an online bookstore that tracks books, authors, customers, and orders.

Entities and Attributes:

Book:

book_id (INT, PRIMARY KEY, AUTO_INCREMENT)
title (VARCHAR(255), NOT NULL) 
author_id (INT, NOT NULL)
isbn (VARCHAR(13), UNIQUE, NOT NULL)
price (DECIMAL(10, 2), NOT NULL)
publication_year (INT)
Author:

author_id (INT, PRIMARY KEY, AUTO_INCREMENT)
first_name (VARCHAR(50), NOT NULL)
last_name (VARCHAR(50), NOT NULL)
Customer:

customer_id (INT, PRIMARY KEY, AUTO_INCREMENT)
first_name (VARCHAR(50), NOT NULL)
last_name (VARCHAR(50), NOT NULL)
email (VARCHAR(255), UNIQUE, NOT NULL) 
address (TEXT)
Order:

order_id (INT, PRIMARY KEY, AUTO_INCREMENT)
customer_id (INT, NOT NULL)
order_date (DATE, NOT NULL) 
Order_Item:

order_id (INT, NOT NULL)
book_id (INT, NOT NULL)
quantity (INT, NOT NULL)
price (DECIMAL(10, 2), NOT NULL)
Note: This field can be redundant, as it can be derived from the Book table. However, storing it here can improve performance for order summaries.
Relationships:

One-to-Many: An author can write multiple books. 
One-to-Many: A customer can place multiple orders.
Many-to-Many: An order can contain multiple books. 
Database Schema (SQL Example):

SQL

CREATE TABLE Author (
    author_id INT AUTO_INCREMENT PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL
);

CREATE TABLE Book (
    book_id INT AUTO_INCREMENT PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    author_id INT NOT NULL,
    isbn VARCHAR(13) UNIQUE NOT NULL,
    price DECIMAL(10, 2) NOT NULL,
    publication_year INT,
    FOREIGN KEY (author_id) REFERENCES Author(author_id)
);

CREATE TABLE Customer (
    customer_id INT AUTO_INCREMENT PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    address TEXT
);

CREATE TABLE Order (
    order_id INT AUTO_INCREMENT PRIMARY KEY,
    customer_id INT NOT NULL,
    order_date DATE NOT NULL,
    FOREIGN KEY (customer_id) REFERENCES Customer(customer_id)
);

CREATE TABLE Order_Item (
    order_id INT,
    book_id INT,
    quantity INT NOT NULL,
    price DECIMAL(10, 2) NOT NULL,
    PRIMARY KEY (order_id, book_id), -- Composite primary key
    FOREIGN KEY (order_id) REFERENCES Order(order_id),
    FOREIGN KEY (book_id) REFERENCES Book(book_id)
);
Indexing:

Book:
INDEX idx_book_author (author_id): To efficiently find books by author.
INDEX idx_book_isbn (isbn): To efficiently find books by ISBN.
Order:
INDEX idx_order_customer (customer_id): To efficiently find orders placed by a specific customer.
Order_Item:
INDEX idx_order_item_order (order_id): To efficiently find order items for a specific order.
Note:

This is a simplified example, and the actual database design may need to be adjusted based on specific requirements and performance considerations.
This schema demonstrates the use of primary keys, foreign keys, unique constraints, and NOT NULL constraints to ensure data integrity.
Indexing is crucial for improving query performance, especially for frequently accessed data.

---------------------------------------------------------------------
Module 3	Module 3: Introduction to SQL
---------------------------------------------------------------------


Install mysql 
	Windows
		https://dev.mysql.com/doc/refman/8.0/en/windows-installation.html
	Mac
		https://dev.mysql.com/doc/refman/8.0/en/macos-installation.html
	Linux
		https://dev.mysql.com/doc/refman/8.0/en/linux-installation.html
		
		Yum linux
			https://dev.mysql.com/doc/refman/8.0/en/linux-installation-yum-repo.html
				https://dev.mysql.com/doc/refman/8.0/en/linux-installation.html
			sudo wget https://dev.mysql.com/get/mysql80-community-release-el9-1.noarch.rpm 
			sudo dnf install mysql80-community-release-el9-1.noarch.rpm -y
			sudo dnf install mysql-community-server -y
			dnf clean packages
		

		
		
	Get ubuntu 20.04 machine 		
		https://docs.rackspace.com/docs/install-mysql-server-on-the-ubuntu-operating-system
			UPDATE mysql.user SET authentication_string = PASSWORD('mypwd') WHERE User = 'root';

	For 22.04
			UPDATE mysql.user SET authentication_string = PASSWORD('mypwd') WHERE User = 'root'; did not work.
			Instead used ALTER USER 'root'@'localhost' IDENTIFIED BY 'new_password';
			

	sudo mysql --defaults-file=/etc/mysql/debian.cnf
	
INSERT INTO books (title, author_fname, author_lname, released_year, stock_quantity, pages)
VALUES
('The Namesake', 'Jhumpa', 'Lahiri', 2003, 32, 291),
('Norse Mythology', 'Neil', 'Gaiman',2016, 43, 304),
('American Gods', 'Neil', 'Gaiman', 2001, 12, 465),
('Interpreter of Maladies', 'Jhumpa', 'Lahiri', 1996, 97, 198),
('A Hologram for the King: A Novel', 'Dave', 'Eggers', 2012, 154, 352),
('The Circle', 'Dave', 'Eggers', 2013, 26, 504),
('The Amazing Adventures of Kavalier & Clay', 'Michael', 'Chabon', 2000, 68, 634),
('Just Kids', 'Patti', 'Smith', 2010, 55, 304),
('A Heartbreaking Work of Staggering Genius', 'Dave', 'Eggers', 2001, 104, 437),
('Coraline', 'Neil', 'Gaiman', 2003, 100, 208),
('What We Talk About When We Talk About Love: Stories', 'Raymond', 'Carver', 1981, 23, 176),
("Where I'm Calling From: Selected Stories", 'Raymond', 'Carver', 1989, 12, 526),
('White Noise', 'Don', 'DeLillo', 1985, 49, 320),
('Cannery Row', 'John', 'Steinbeck', 1945, 95, 181),
('Oblivion: Stories', 'David', 'Foster Wallace', 2004, 172, 329),
('Consider the Lobster', 'David', 'Foster Wallace', 2005, 92, 343);

What is SQL?

	(already covered above)
	SQL is a standardized language 
		designed to interact with relational databases.  
	Allows you to 
		create, 
		manipulate, and 
		retrieve data 
			stored in tables 
				within a relational database management system (RDBMS) like 
					MySQL, 
					Oracle Database, 
					Microsoft SQL Server, or 
					PostgreSQL.

Why is SQL important?

	Widely Used: 
		SQL 
			standard language for relational databases
				valuable skill for anyone working with data.
	Data Management and Analysis: 
		can 
			create, 
			insert, 
			update, 
				with lock
			delete, and 
			query 
				data within relational databases.
	Data Retrieval: 
		SQL provides powerful tools 
			for retrieving specific data based on various criteria, 
		can 
			generate reports, 
			analyze trends
			gain insights from your data.
	Integration with Applications: 
		Many applications interact with databases using SQL
		make SQL knowledge beneficial for developers and data analysts.
Basic SQL Commands:
-------------------
Here's a glimpse into some fundamental SQL commands to get you started:

	SELECT: 
		used to retrieve data from one or more tables. 
		can specify 
			columns (attributes) 
		filter the results using 
			WHERE clauses.
	INSERT: 
		insert new rows of data into a table. 
		specify table name 
			values for each column in the new row.
	UPDATE: 
		modify existing data in a table. 
		can update specific columns 
			based on a WHERE clause 
				to target the rows you want to modify.
	DELETE: 
		This command removes rows from a table. 
			use a WHERE clause to delete specific rows based on criteria.
	CREATE TABLE: 
		create new tables within the database
		defining the structure with column names and data types.

---------------------------------------------------------------------
	Definition and Purpose of SQL
---------------------------------------------------------------------

SQL: Definition and Purpose
	
Definition:

	Structured: 
		SQL queries follow a defined structure
			easy to understand and write.
			no change as you work with any rdbms
	Query Language: 
		can 
			retrieve (query) data, 
			manipulate data (insert, update, delete), and 
			manage the structure of a relational database.
Purpose:

	Data Manipulation: 
		SQL empowers you to 
			create, 
			insert, 
			update, and 
			delete 
				data within relational database tables.
	Data Retrieval: 
		tools for 
			querying data based on specific criteria. 
		Can 
			filter, 
			sort, 
			join tables, and 
			retrieve 
				the exact information you need.
	Database Management: 
		Can 
			create new tables, 
			define/modify their structure (columns and data types)
			modify the database schema as needed.
	Data Analysis: 
		By efficiently retrieving and manipulating data
			SQL enables you to 
				analyze trends, 
				generate reports, and 
				gain insights 
					from your data.
	Standardized Access: 
		SQL is a widely used and standardized language. 
		interact with 
			various relational databases seamlessly.



	Database Administrators (DBAs): 
		Manage and maintain the database infrastructure.
	Data Analysts: 
		Extracting and analyzing data for insights and decision making.
	Software Developers: 
		Building applications that interact with databases.
	Anyone Working with Data: 
		Gaining control over data retrieval and manipulation tasks.

---------------------------------------------------------------------
	Evolution and History of SQL
---------------------------------------------------------------------

Early Days (1970s):

	The Birth of the Relational Model: 
		In 1970, 
			Edgar F. Codd
				computer scientist at IBM, 
				published a paper titled 
					"A Relational Model of Data for Large Shared Banks." 
				This paper laid the foundation for the relational model
					outline the core principles 
						for organizing data in tables 
							with rows and columns.
	Development of System R: 
		Following Codd's relational model, 
			IBM researchers embarked on a project called System R. 
			This project aimed to 
				create a prototype relational database management system (RDBMS) 
					included the development of a query language 
						for interacting with the database.
Standardization and Rise of Commercial RDBMS (1970s - 1980s):

	SEQUEL (Structured English Query Language): 
		The early version of SQL, 
			known as SEQUEL, 
				emerged from the System R project. 
			Designed to be a user-friendly way to 
				interact with relational databases.
	Standardization Efforts: 
		use of relational databases grew
			need for a standardized query language became apparent. 
		In the 1980s, 
			organizations like 
				ANSI (American National Standards Institute) and 
				ISO (International Organization for Standardization) 
					played a key role in formalizing and standardizing SQL.
	Rise of Commercial RDBMS: 
		The 1970s and 1980s witnessed the birth of 
			several commercial RDBMS solutions like 
				Oracle, 
				DB2 (derived from System R), 
				Microsoft SQL Server, and 
				MySQL. 
			These systems adopted and implemented the evolving SQL standard.
Maturity and Continued Development (1990s - Present):

	Wide Adoption and Advancements: 
		With standardization and the increasing popularity of RDBMS, 
			SQL became the dominant language 
				for interacting with relational databases.
	Enhancements and Features: 
		Over time, 
			SQL has undergone various revisions and extensions 
				to incorporate new features like 
					stored procedures, 
					triggers, 
					functions, and 
					support for complex data types.
	Object-Relational Features: 
		Some RDBMS vendors 
			introduced object-relational features 
			that extended SQL syntax 
				to handle object-oriented concepts 
					within a relational framework.
	Integration with New Technologies: 
		Today, 
			SQL continues to evolve, 
				integrating with 
					cloud-based databases and 
					big data technologies.
The Future of SQL:

	While NoSQL databases 
		offer alternatives for specific use cases, 
			SQL remains a mainstay for relational databases. 
	Its 
		standardized nature, 
		versatility, and 
		strong foundation for 
			data management 
				ensure its continued relevance.
	Advancements in areas like 
		cloud integration, 
		security, and 
		integration with big data ecosystems 
			can further expand the reach and capabilities of SQL.

---------------------------------------------------------------------
	SQL Standards
---------------------------------------------------------------------
SQL standards 
	set of formally defined guidelines 
	govern the syntax and functionality of the Structured Query Language (SQL) 
	These standards ensure 
		consistency and 
		compatibility 
			across different relational database management systems (RDBMS) from various vendors.

Here's a breakdown of the key aspects of SQL standards:

Importance of Standards:

	Compatibility: 
		Standardized SQL 
			allows applications written for one RDBMS 
				to work (with some adjustments) 
				on another RDBMS 
					that adheres to the same standard. 
			promotes portability and 
			reduces development complexity.
	Vendor Independence: 
		Developers are not limited to specific RDBMS vendors 
			as they can leverage their SQL skills 
				across different compliant systems.
	Clear Communication: 
		Standardized SQL syntax 
			facilitates better communication and 
			collaboration between 
				database administrators, 
				developers, and 
				data analysts 
					working with different relational database environments.
Main Organizations Behind SQL Standards:

	American National Standards Institute (ANSI): 
		A US-based organization 
			played a crucial role in standardizing SQL in the 1980s. 
		The current standard is ANSI X3.135
			also known as ISO/IEC 9075.
	International Organization for Standardization (ISO): 
		An international body 
			collaborates with national standards institutions like 
				ANSI. 
			ISO/IEC 9075 
				is the international counterpart to the 
					ANSI SQL standard.
The SQL Standard (ISO/IEC 9075):

	This standard, 
		also referred to as SQL-92 
			(referring to the year of its initial publication), 
				defines the core functionalities of SQL, including:
	
	Data Definition Language (DDL) 
		commands for 
			creating, 
			altering, and 
			dropping 
				database objects like 
					tables, 
					views, and 
					indexes.
	Data Manipulation Language (DML) 
		commands for 
			inserting, 
			updating, 
			deleting, and 
			retrieving 
				data from tables.
	Data Control Language (DCL) 
		commands for managing 
			user access and 
			permissions 
				within the database.
	The standard has been revised and updated over time
		new versions like 
			SQL-99, 
			SQL-2003, and 
			SQL-2016 
				introducing additional features and functionalities.
Benefits of Using Standards-Compliant RDBMS:

	Reduced Development Time: 
		Leveraging standardized SQL 
			reduces the need to rewrite code 
				for different databases, 
			save development time and effort.
	Improved Interoperability: 
		Applications can interact with 
			various compliant databases 
				more easily, 
				fostering 
					data exchange and 
					integration.
	Enhanced Maintainability: 
		Standardized code 
			is easier to 
				understand, 
				maintain, and 
				modify 
					in the long run.
It's important to note that not all RDBMS implementations adhere strictly to every aspect of the SQL standard.  
Some vendors might include proprietary extensions to offer additional functionalities beyond the standard. 
However, understanding the core SQL standards provides a solid foundation for working with relational databases effectively.
---------------------------------------------------------------------
Module 4	Module 4 : Basic Terminologies
---------------------------------------------------------------------
---------------------------------------------------------------------
	Tables
---------------------------------------------------------------------
In SQL, 
	tables are the fundamental building blocks for 
		organizing data within a relational database. 
	They act like (excel) spreadsheets with rows and columns, 
		where 
			each row represents a 
				unique record
			each column represents a 
				specific attribute or characteristic of that record.

Here's a closer look at tables in SQL:

Structure:

	A table is defined by 
		its name and a 
		set of columns.
	Each column has a 
		name, 
		a data type 
			(e.g., integer, string, date), and 
		optional constraints 
			(rules governing the data in that column).
	The first column 
		often contains a unique identifier (primary key) for each row, 
		allow for 
			efficient 
				data retrieval and 
				manipulation.
Data Storage:

	Each row in a table represents a single data record.
	The values in each column 
		correspond to the specific attributes of that record.
	For example, 
		table named 
			"Customers" 
		columns for 
			"customer_id" (primary key), 
			"name," 
			"address," 
			"email," and 
			"phone_number."
Relationships Between Tables:

	Although tables store individual records
		they can be linked together using relationships.
	These relationships are established by referencing 
		columns between tables, 
		allow you to combine 
			data from multiple 
				tables for comprehensive analysis.
Key Concepts Related to Tables in SQL:

	(alredy covered)
	Data Definition Language (DDL): 
		These are SQL commands 
			used to 
				create, 
				alter, and 
				drop tables 
		The CREATE TABLE statement is fundamental for defining the structure of a table.
	Data Manipulation Language (DML): 
		These commands 
			interact with the data within the tables. 
		This includes 
			inserting new rows (INSERT), 
			updating existing data (UPDATE), 
			deleting rows (DELETE)
			retrieving data (SELECT) 
				based on specific criteria.
	Primary Key: A 
		column (or a combination of columns) 
			uniquely identifies each row in a table
			It enforces 
				data integrity and 
				prevents duplicate records.
	Foreign Key: 
		A column in one table that 
			references the primary key of another table. 
		This establishes a link between 
			related tables, 
			allow to join 
				data from different tables.
Benefits of Using Tables in SQL:

	Organized Data Storage: 
		Tables provide a 
			structured and 
			well-defined 
				way to 
					store and 
					manage data.
	Efficient Data Retrieval: 
		By leveraging 
			primary key and 
			indexing techniques
				SQL enables efficient retrieval of specific data records.
	Data Relationships: 
		Tables can be linked to represent 
			complex relationships between different data entities.
	Data Integrity: 
		Constraints and data types within tables 
			help maintain data accuracy and consistency.
---------------------------------------------------------------------
	Rows and Columns
---------------------------------------------------------------------
Within tables, the core data organization structure in SQL, reside two crucial elements: 
	rows and columns. 
	These elements work together to store and manage data efficiently in relational databases.

1. Rows (Records):

	Imagine rows like 
		rows in a spreadsheet 
	or 
		individual lines in a table.
	Each row represents a 
		single, 
		unique record 
			within the table.
	All rows in a table 
		have the same structure, 
		defined by the table's columns.
	Each row contains 
		data pertaining to a specific entity 
	or 
		concept that the table represents.
	For instance, in a 
		"Customers" table, 
			each row might represent a single customer.
2. Columns (Attributes/Fields):

	Columns 
		vertical categories in a spreadsheet 
		or 
		headings in a table
		or 
		attribute of an entity .
	All rows in a table 
		have corresponding values 
			for each column.
	The data type of a column 
		defines the kind of 
			data it can store 
			(e.g., numbers, text, dates).
	Columns provide a way to categorize and organize the data within each record.
Example:

Consider a table named "Products" that stores information about products in a store.

	product_id (PK)	product_name	price	stock
	1001		T-Shirt				25.99	50
	1002		Jeans				39.95	30
	1003		Coffee Mug			8.50	100
Here, 
	each row represents a single product. 
	The columns define specific attributes of a product, like its name, price, and current stock level.

Key Points to Remember:

	The number of rows (records) in a table can vary depending on the data you store.
	The number of columns (attributes) is fixed based on the table's structure defined during creation.
	The order of rows 
		is not inherently significant 
			in most cases. 
	However, the order of columns 
		defines the sequence of attributes associated with each record.
	The primary key (PK) constraint, often a unique identifier column, ensures no duplicate rows exist within a table.
	In conclusion, 
		rows and columns in SQL form the backbone of data organization within tables. 
	Understanding their roles is essential for effectively managing and interacting with data in relational databases.
---------------------------------------------------------------------
	Constraints
---------------------------------------------------------------------
In SQL, 
	constraints act like rules 
	govern the data stored within tables. 

	enforce 
		data integrity and 
		consistency, 
		accuracy and 
		reliability of 
			your information 
			in a relational database. 
Here's a breakdown of the different types of constraints commonly used in SQL:

1. Data Type Constraints:

	Define the kind of data a column can hold 
		(e.g., 
			integer (int), 
			string (varchar), 
			date).
	This prevents incompatible data types 
		from being entered, 
		reduce errors and 
		maintain data consistency.

	Example: 
		Specifying data_type: 
			INTEGER for a column named age 
				ensures only numerical values 
				representing ages are stored.
2. NOT NULL Constraints:
	
	Enforce that a column 
		cannot contain null values 
		(missing or unknown data).
	This ensures all rows have a valid value for that specific column.
	Example: 
		A NOT NULL constraint on the customer_name column 
			guarantees that every customer record has a name filled in.

3. Unique Constraints:

	Guarantee that specific values within a column, or 
		a combination of values 
			across multiple columns, are 
			unique throughout the table.
	Prevents 
		duplicate records and 
		ensures each row 
			has a distinct identity.
	Example: 
		A unique constraint on the email column in a "Customers" table 
			prohibits 
				storing the same email address for multiple customers.
4. Primary Key Constraints:

	A special type of unique constraint that 
		acts as the main identifier for each row in a table.
	A table can only have one primary key
		must not contain null values.
			fk allows one null value 
	The primary key 
		enforces data integrity 
		allows for efficient data retrieval 
			using indexing techniques.
	Example: 
		The product_id column in a "Products" table 
			can be designated as the 
				primary key, 
				uniquely identifying each product record.
5. Foreign Key Constraints:
	lab: 877
	
	Establish relationships between two tables 
		by referencing a primary key 
			from one table (parent table) 
				in another table (child table).
	This ensures 
		data consistency 
			across linked tables and 
			prevents orphaned records 
				(child records referencing non-existent parent records).
	Example: 
		A foreign key constraint 
			referencing the 
				customer_id (primary key) in the 
					"Orders" table 
				can link orders 
					to specific customers in the "Customers" table.
6. CHECK Constraints:

	Allow you to define a custom expression or condition 
		must be true for a row to be 
			inserted or updated.
	This provides more granular control 
		over the validity of data 
			beyond basic data types or uniqueness.
	Example: 
		A check constraint on a price column 
			can ensure it's always a positive value (e.g., price > 0).
Benefits of Using Constraints:

	Data Integrity: 
		Constraints help prevent 
			invalid or 
			inconsistent 
				data from entering the database, 
			safeguarding data accuracy.
	Improved Data Quality: 
		By enforcing data type 
			rules, 
			uniqueness, and 
			referential integrity, 
			constraints 
				promote reliable data for analysis and decision-making.
	Reduced Errors: 
		Constraints act as a safety net, 
		minimize data entry errors and 
		ensure data consistency across tables.
---------------------------------------------------------------------
	Indexes
---------------------------------------------------------------------

How databases store data
	row_id: mysql doesn't do this. but post-gre does.
	In row based storage model 
		rows are stored and read in logical pages.
	The database reads page(s) and not rows
	page has a size 
		(e.g. 8KB in postgres, 16KB in MySQL)
	Assume if a page holds 3 rows 
		then 500 rows would be stored in 
			500/3 = 167~ pages

	Column based storage and Row based storage 
		analyse 
			select * from emp where id=1
			select sum(salary) from emp
-------------------
Database Pages â€” A deep dive


-----------------------
lab: 
	create a db of 10 lakh rows 
	describe table 
		can you see indexes?
		explain analyze select id from emp where id =2
		
		explain analyze select * from emp where name = 'a'
		explain analyze select * from emp where name like  'a%'
	create index employee_name on employee(name);
		explain analyze select * from emp where name = 'a'
		explain analyze select * from emp where name like  'a%'
----------------------------		
Databases 
	use fixed-size pages to store data. 
	
	Tables, 
	collections, 
	rows, 
	columns, 
	indexes, 
	sequences, 
	documents 
		end up as bytes in a page. 
	
	This way 
		storage engine can be separated from the database frontend 
			responsible for data format and API. 
		Moreover, this makes it easier to read, write or cache data when everything is a page.

Here is an example of SQL Server page layout.
SQL Server page layout (link).



Databases read and write in pages. 
When you read a row from a table
	database finds the page 
		where the row lives 
		identifies the file and 
		offset where the page is located on disk. 
	The database then asks the OS to read from the file 
		on the particular offset for the length of the page. 
	The OS checks its filesystem cache and 
		if the required data isnâ€™t there
			OS issues the read and 
			pulls the page in memory for the database to consume.

The database allocates a pool of memory
	often called 
		shared or buffer pool. 
	Pages read from disk are placed in the buffer pool. 
	Once a page is in the buffer pool
		not only we get access to the requested row 
			but also other rows in the page too depending on how wide the rows are. 
		This makes reads efficient 
			especially those resulting from index range scans. 
		The smaller the rows
			more rows fit in a single page
				a single I/O should be able to get more data/rows.

The same goes for writes
	when a user updates a row
		database finds the page where the row lives
			pull the page in the buffer pool 
			update the row in memory 
			make a journal entry of the change (often called WAL) persisted to disk. 
		The page can remain in memory 
			so it may receive more writes 
			before it is finally flushed back to disk
				minimizing the number of I/Os. 
		Deletes and inserts work the same but implementation may vary.
Page Content

	What you store in pages is up to you. 
	Row-store databases 
		write rows and all their attributes 
			one after the other 
				packed in the page so that OLTP workloads are better especially write workload.

	Column-store databases 
		write the rows in pages column by column 
			such OLAP workloads that run a summary 
				fewer fields are more efficient. 
		A single page read will be packed with values from one column
			making aggregate functions like 
				SUM much more effective. 
		I made a video comparing row vs column based storage engines,

	Document based databases 
		compress documents and store them in page 
			just like row stores and graph based databases 
				persist the connectivity in pages 
					such that page read is efficient for traversing graphs, 
					this also can be tuned for depth vs breadth vs search.

	Whether you are storing 
		rows, 
		columns, 
		documents or 
		graphs,  
		
		goal is to pack your items in the page 
			such that a page read is effective. 
		The page should give you as much useful information as possible 
			to help with client side workload. 
		If you find yourself reading many pages to do tiny little work 
			consider rethinking your data modeling. 
		This is a whole different article, data modeling, underrated.
Small vs Large Pages

	Small pages are faster to read and write 
		especially if the page size is closer to the media block size
		however the overhead cost of the page header metadata 
			compare to useful data can get high
	On the other hand, 
		larger sizes can minimize metadata overhead and 
			page splits but at a cost of higher cold read and write.

	Of course this gets very complicated 
		the closer you get to the disk/SSD. 
	Great minds in storage industry are 
		working on technologies like 
			Zoned and 
			key value store namespaces in NVMe 
				to optimize read/writes between host and media. 
		
	Postgres Default page size is 8KB, 
	MySQL InnoDB is 16KB, 
	MongoDB WiredTiger is 32KB, 
	SQL Server is 8KB and 
	Oracle is also 8KB. 
	
	Database defaults work for most cases 
		DBA should 
			know these default 
			configure it for your use case.
How page are stored on Disk

	Many ways to 
		store  
		retrieve 
			pages to and from disk. 
		One way 
			make a 
				file per table 
				or 
				collection as an array of fixed-size pages. 
		Page 0 followed by 
			page 1 followed by 
				page 2. 
		To read something from disk 
			we need to inform
				the file name
				offset and 
				the length

	To read page x, 
		we know the file name from the table, 
			to get the offset it is X *Page_Size, and the 
				length in bytes are the page size.

	Example 
		reading table test
			assume a page size of 8KB, 
				to read pages 2 through 9
					we read the file where table test live, 
					with an offset 16484 (2*8192) for 
					8 pages, 65536 bytes (8*8192).

	N.B: every database implementation is differnet.
Postgres Page Layout

	Among the database, 
		I would like to explore how PostgreSQL store pages and provide my critisim of certain choices made. 
		In Postgres the default page size is 8KB, and here is how it looks like.
Postgres page layout (link)

The page must have metadata to describe what is in the page including the free space available. This is a 24 bytes fixed header.
ItemIds â€” 4 byte each

This is an array of item pointers (not the items or tuples themselves. Each itemId is a 4 byte offset:length pointer which points to the offset in the page of where the item is and how large is it.

It is the fact that this pointer exist allows the HOT optimization (Heap only tuple), when an update happens to a row in postgres, a new tuple is generated, if the tuple happened to fit in the same page as the old tuple, the HOT optiimization changes the old item id pointer to point to the new tuple. This way indexes and other data structures can still point to the old tuple id. Very powerful.

Although one criticism is the size the item pointers take, at 4 bytes each, if I can store 1000 items, half the page (4KB) is wasted on headers.

    I use items, tuples and rows but there a difference, the row is what the user see, the tuple is the physical instance of the row in the page, the item is the tuple. The same row can have 10 tuples, one active tuple and 7 left for older transactions (MVCC reasons) to read and 2 dead tuples that no one needs any more.

Items â€” variable length

This is where the items themselves live in the page one after the other.
Special â€” variable length

This section is only applicable to B+Tree index leaf pages where each page links to the previous and forward. Information about page pointers are stored here.

Here is an example of how tuples are referenced.
Summary

Data in databases end in pages, whether this is index, sequence, or a table rows. This makes it easier for the database to work with pages regardless what is in the page itself. The page it self has a header and data and is stored on disk in as part of a file. Each database has a different implementation of how the page looks like and how it is physically stored on disk but at the end, the concept is the same.


---------------------

indexes 
	special data structures 
	act like reference guides 
		within a relational database. 
	
	significantly improve performance 
		of data retrieval 
		especially those involving 
			filtering or 
			sorting 
				large datasets.


Here's how indexes work:

	Imagine a physical book 
		with an index at the back, 
			listing keywords and 
			their corresponding page numbers.
	SQL index 
		is a separate structure 
			maps specific values in a table column 
				to the actual 
					rows containing those values.
	When you execute a query 
		that 
			filters or 
			sorts data based on the 
				indexed column
		database engine 
			locate the relevant rows 
				using the index, 
				instead of scanning the entire table.
Benefits of Using Indexes:

	Faster Query Execution: 
		Indexes significantly speed up queries that involve 
			filtering or 
			sorting data 
				based on the indexed column(s). 
		This is because 
			database can quickly locate relevant rows 
				using the index 
				rather than examining every row in the table.
	Improved Read Performance: 
		Queries that retrieve data 
			based on indexed columns 
				benefit the most from indexing
		faster response times and 
		better overall database performance.
	Efficient WHERE Clauses: 
		Indexes are particularly helpful for queries with 
			WHERE clauses that filter data based on specific column values.
Types of Indexes:
	[Refer below section for detailed study]
	B-Tree Indexes: 
		The most common type of index 
		They organize data in a 
			tree-like structure 
				for efficient searching and retrieval.
	Hash Indexes: 
		Faster for exact value lookups 
			but don't support 
				efficient range-based queries 
					(e.g., finding all values between a specific range).
	Composite Indexes: 
		Indexes can be created on multiple columns together, 
			improve performance for queries 
				that involve 
					filtering or 
					sorting 
						based on combinations of these columns.
When to Use Indexes:

	Frequently Queried Columns: 
		Create indexes on 
			columns that are often used in WHERE clause 
				conditions, 
				joins, or 
				sorting 
					operations within queries.
	Selective Queries: 
		Indexes are most beneficial for queries that 
			filter or 
			sort data, 
			reduce the number of 
				rows the database engine needs to scan.
	Large Tables: 
		For tables with a significant number of rows
			indexes can make a substantial performance 
				difference in query execution speed.
Things to Consider with Indexes:

	Overhead: 
		Creating and maintaining indexes 
			consumes storage space 
		Requires additional processing 
			when data is 
				inserted, 
				updated, or 
				deleted.
	Index Selection: 
		Choosing the right columns to index is crucial. 
		Indexing every column might not always be necessary 
			can negatively impact performance in some cases.
	Monitoring and Maintenance: 
		Review and adjust indexes over time as query patterns and data volumes change.
In conclusion, 
	indexes are valuable tools for 
		optimizing the performance of data retrieval in SQL databases. 
	Understanding how they 
		work and when to use them effectively 
		can significantly enhance the 
		speed and efficiency of your database operations.
	

---------------------------------------------------------------------
	Types of Indexes
---------------------------------------------------------------------

General syntax: 
	CREATE [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name
		USING [BTREE | HASH | RTREE]
		ON table_name (column_name [(length)] [ASC | DESC],â€¦)

------------------------------------------------------
Types of Index.

Source https://dev.mysql.com/doc/refman/8.0/en/mysql-indexes.html
Most MySQL indexes 
	PRIMARY KEY, 
	UNIQUE, 
	INDEX, and 
	FULLTEXT
		are stored in B-trees. 
	
	Exceptions: 
		Indexes on spatial data types use R-trees; 
		MEMORY tables also support hash indexes; 
		InnoDB uses inverted lists for FULLTEXT indexes.



MySQL B-Tree index
Reference: 	https://severalnines.com/blog/overview-mysql-database-indexing/

Reference: 	
https://planetscale.com/learn/courses/mysql-for-developers/indexes/introduction-to-indexes
Indexing: the building blocks of database performance

	Characteristics of indexes
	Indexes 
		entirely separate data structure 
			maintain a copy of part of your data. 
			When you create an index
				it creates a second data structure
					different from your primary data structure (the table). 
			Each index 
				maintains a copy of part of your data
				multiple indexes
					multiple copies of different parts of your data.

		indexes have a pointer back to the row. 
		index should know 
			how to get back to the table 
				because it maintains a copy of part of your data.

	Rules for creating indexes
		improve read performance
		But following operations would slow 
			create
			update 
			delete

		Establishing balance (whether to create index or not)
			depends on 
				size of your database, 
				frequency of updates
					etc. 
			



B+ trees
Understanding B+ Trees: the data structure behind MySQL indexes
	If you've spent any amount of time working with MySQL, you've likely heard about B+ trees. This data structure is the underlying foundation for most indexes in the database management system. Despite being a complex concept, it's important for any application developer to understand how B+ trees work to efficiently manage large amounts of data.
	already covered in 
		Reference: 	https://severalnines.com/blog/overview-mysql-database-indexing/

	Why add indexes?
	So why add an index in the first place? Put simply, indexes speed up data retrieval. Without an index, MySQL would have to read through the entire table to find a specific value. That may be manageable for small tables, but as the number of rows increases, it becomes an impractical solution.

	Anatomy of a B+ tree
	This data structure resembles a tree structure with nodes, and it functions as a map for accessing specific data in a table. To demonstrate how B+ trees work, let's visualize an index on the people table. You can see that it looks like a tree, with the root node at the top and the leaf nodes at the bottom.

	The root node isn't very relevant to us right now, so we'll shift our focus to the leaf nodes. Each name in the table has its own corresponding node in the B+ tree. The leaf nodes contain the data that we have indexed, in this case, first names.

	Searching with a B+ tree
	Let's assume we want to search for the name "Suzanne." We'll pretend to be the database and follow our searching algorithm.

	Starting at the root node, we'll compare "Simon" to "Suzanne." As "Suzanne" is later in the alphabet than "Simon," we'll head down the right side of the tree. We'll then compare "Tyler" to "Suzanne." Since "Suzanne" comes before "Tyler," we'll head down the left side. We'll find the node with the name "Suzanne" and compare it to itself. Since the values match, we'll head down the right side of the tree and land where we want to be.

	This algorithm allows us to skip over many of the leaf nodes at the bottom and only look at a few nodes. Without an index, we'd have to read through every single row to find the name "Suzanne."

	How indexes make MySQL queries faster
	So how do B+ trees make things faster? They build up a secondary data structure that optimizes searching for specific values. By creating an index on the first name field, we've optimized this secondary data structure for searching first names.




3
Primary keys
	Understanding primary and secondary keys in MySQL
		
	A primary key 
		unique identifier 
			for each row in the table
			
	Creating a table and add a primary key

Try it 
	
	CREATE TABLE users (
	  ID BIGINT PRIMARY KEY
	);
	
		it will create a unique index for the column. 
		don't need to create a separate index for the primary key.

	show indexes from users;
	
	Understanding primary keys vs. secondary keys


	secondary key 
		any index that is not the primary key. 
		Improve performance 
			when searching for specific data in your table.

	primary key
		unique identifier for each row in the table. 
		Required for many relational database operations. 
		In MySQL
			primary key 
				determine the physical order 
					of the data in the table. 
			Choosing the right primary key 
				critical for optimizing performance.

	Why choosing the right primary key matters
		The primary key 
			determines how your data is stored on disk. 
		In MySQL's InnoDB engine
			primary key is a "clustered index," 
				data is physically ordered 
					based on the values in the primary key column. 
				So primary key lookups are incredibly fast
					but 
						when you insert a new row the 
							tree structure has to be updated.




Secondary keys
	Understanding secondary keys in MySQL
		secondary keys
			also known as indexes

	What are secondary keys in MySQL?
		Every MySQL table 
			has one primary key 
			can have multiple secondary keys. 
			
			primary and secondary keys 
				are still related to each other

	Understanding primary and secondary key relationship
	----------------------------------------------------
	For e.g. 
		people table 
			with the columns 
				id, 
				name, and 
				email.
	CREATE TABLE people (
	  id BIGINT PRIMARY KEY,
	  name varchar(500),
	  email varchar(500),
	  first_name varchar(500), 
	  last_name varchar(500), 
	  state varchar(200), 
	  birthday datetime
	);
	
	INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (1,'John Doe', 'john.doe@example.com', 'John', 'Doe', 'California', '1990-01-01');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (2,'Alice Brown', 'alice.brown@example.com', 'Alice', 'Brown', 'Texas', '1985-07-12');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (3,'David Miller', 'david.miller@example.com', 'David', 'Miller', 'New York', '1978-03-25');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (4,'Emily Garcia', 'emily.garcia@example.com', 'Emily', 'Garcia', 'Florida', '1992-11-08');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (5,'Michael Jones', 'michael.jones@example.com', 'Michael', 'Jones', 'California', '1980-05-17');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (6,'Sarah Hernandez', 'sarah.hernandez@example.com', 'Sarah', 'Hernandez', 'Arizona', '1995-09-03');
id, 
INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (7,'David Lee', 'david.lee@example.com', 'David', 'Lee', 'Washington', '1982-01-06');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (8,'Jessica Williams', 'jessica.williams@example.com', 'Jessica', 'Williams', 'Illinois', '1998-02-21');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (9, 'Matthew Johnson', 'matthew.johnson@example.com', 'Matthew', 'Johnson', 'Ohio', '1975-08-14');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (10,'Ashley Wilson', 'ashley.wilson@example.com', 'Ashley', 'Wilson', 'Georgia', '1990-04-28');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (11,'Richard Rodriguez', 'richard.rodriguez@example.com', 'Richard', 'Rodriguez', 'Texas', '1987-12-19');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (12,'Amanda Moore', 'amanda.moore@example.com', 'Amanda', 'Moore', 'California', '1993-10-05');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (13,'Joseph Taylor', 'joseph.taylor@example.com', 'Joseph', 'Taylor', 'Pennsylvania', '1981-06-30');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (14,'Elizabeth Garcia', 'elizabeth.garcia@example.com', 'Elizabeth', 'Garcia', 'Florida', '1996-01-16');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (15,'Christopher Allen', 'christopher.allen@example.com', 'Christopher', 'Allen', 'New York', '1977-09-02');

INSERT INTO people (id, name, email, first_name, last_name, state, birthday)
VALUES (16,'Jennifer Hernandez', 'jennifer.hernandez@example.com', 'Jennifer', 'Hernandez', 'Arizona', '1999-03-27');








	Creating a secondary id, key
		We can create a secondary key on name using the following MySQL query:id, 

		ALTER TABLE people ADD INDEX (name);
	
	Querying people table using secondary key
						representing the row with the matching name.

	name column is a secondary key
		it does not store all the data required for the query. 
		It only contains 
			indexed column name 
			and pointer to the row. 
		MySQL must 
			perform a second lookup 
				to retrieve the rest of the data related for that row.

	Relationship between secondary and primary keys
		Every secondary key 
			has the primary key appended to it
			each leaf node in the secondary key 
				contains a pointer back to the row. 
		When you perform a query with a secondary key
			MySQL first traverses the secondary index tree
			finds the corresponding primary key
				looks up that primary key in the primary index tree 
					to retrieve all the data.

	Importance of compact primary keys
	Crucial to choose 
		compact primary keys 
			that require a minimal amount of storage space.


Primary key data types
	
	The benefits of unsigned big integers
		Recommended practice 
			use unsigned big integers. 
				virtually infinite room to grow
					because 
						running out of primary key space 
							is a significant issue for databases. 
				allow auto-incrementing
					preserves a natural order for the records
					ensure 
						primary key B-tree isn't split unnecessarily.

	Choosing strings as primary keys
		Choosing a string data type
			e.g. 
				UUID or a 
				GUID
					as a primary key 
					
					can be tempting
					has potential pitfalls. 
			The problem with these types of data 
				their size
					indexes of the table grow enormously as a result of them. 
			B-tree may have to be rebalanced 
				very often 
					if insertions occur in the middle of the table.

	Still want to use a string as a primary key??
		use UUID or GUID that is time-sorted
		all new records go to the end of the table
		




Where to add indexes
	indexes can be used 
		equality, 
		like 
		unbounded and bounded range 
		sorting 
		grouping 


	Start with your queries
	queries should drive your indexes. 
		analyze the access patterns of your application 
			before deciding where to put indexes.

	ask 
		which queries are you running frequently 
		which tables are they accessing? 
		how often is the update operations?
		what operation is business critical
		By analyzing the access patterns of your queries
		
		not all queries 
			require indexes
			adding too many indexes 
				harm database's performance.

When to use indexes
	some general rules of thumb:
	DON'T's (Incorrect undersstanding)
		Anything that shows up in the where clause of a query should have an index. 
			Consider all queries being run and their respective access patterns.
		Create an index on every column. 
			This will slow down updates by functionally duplicating your table. 
			Won't help reads as much as you'd hope.
		create the perfect index for every query. 
			perfect index may vary at different times.
	DO's 	
		Consider the entire query 
			when deciding which columns to index. 
			This includes 
				sorting, 
				grouping, and 
				joining.
				
	Adding an index in MySQL
	------------------------
	Here's an example of how to add an index to a table in MySQL:

		alter table people add index birthday_index (birthday);
	
	
	Using indexes for specific queries
	

	Equality
		The most basic use of an index is for direct access
			query searches for a specific value in a particular column. 
		
		Here's an example:

		select * from people where birthday = '1989-02-14';
		explain select * from people where birthday = '1989-02-14';
		
	Ranges
		unbounded range 
			there is either no upper or lower limit 
			select * from people where birthday >= '2006-01-01';
			explain select * from people where birthday >= '2006-01-01';
		bounded range 
			both limit present 
				select * from people where birthday between '2006-01-01' and '2006-12-31';			
				explain select * from people where birthday between '2006-01-01' and '2006-12-31';
	Indexes can also be used for 
		unbounded and bounded ranges
		query requests all rows in a certain range. 
	
	
	

	Sorting
		Indexes can also be used to 
			select * from people order by birthday limit 10;
			explain select * from people order by birthday limit 10;
				mysql doesn't need to do a full table scan 
	
	Grouping
		Indexes can also be used to group rows 
			together for an aggregate function. 
			Here's an example:

		select birthday, count(*) from people group by birthday;
		explain select birthday, count(*) from people group by birthday;
		
		

Index selectivity
	Understanding index 
		cardinality and 
		selectivity in MySQL
		
	add indexes to 
		important columns t. 
The 
	process of selecting the best index for specific database columns can be a bit tricky. 
	
	Defining cardinality and selectivity
		Cardinality 
			number of distinct values 
				in a particular column 
					that an index covers. 
			SHOW INDEXES command in MySQL
				cardinality column in the output 
					shows 
						~ total number of unique values 
						in a given index column.

		Selectivity
			how unique the values in a column are. 
			measure of how selective an index can be in narrowing down results when queried. 
			The higher the selectivity of an index
				better it is for optimizing query performance.

	Practical implications of cardinality and selectivity
		For instance, 
			understanding selective value of a column 
				determine whether or not to put an index on it.
			

	Fr e.g. table with a type column that has two possible values
		user 
			most rows have this value 
	and 
		admin
			put an index on the type column 
				may not be the best approach 
					to find rows with type = "user" 
					
		The index is 
			not highly selective for user
			but 
			it is highly selective for admin!

	If confussed 
		check the selectivity of the index in question. 
		If the selectivity is low, 
			it is most likely that MySQL decided to read the table
			probably 
				index would be slower than a full table scan.


Prefix indexes
	Prefix indexing in MySQL
	For e.g. there is long string column
			for e.g. 
				URL or a 
				UID
					that's impossible to index entirely. 
		
			can index a part of the string 
				by creating prefix indexes.


	Creating prefix indexes
	To create a prefix index in MySQL, you have to specify the prefix length you want to index. Suppose we have a people table with a string column for the first name, and we want to index the first five characters only:

	ALTER TABLE people ADD INDEX (first_name(5));

	SHOW INDEXES FROM people;
	
	Advantages of using prefix indexing
		By indexing just a part of a string column
			can make index 
				smaller and 
				faster. 
		For example, 
			indexing few characters 
				reduces 
					size of the index 
					potentially maintaining the selectivity.

	Prefix 
		suitable for 
			long
			evenly distributed
			unique strings
				like 
					UUIDs and hashes.

	This query 
		displays all indexes 
			associated with the people table. 
			see the 
				first_name column
				sub_part column 
					specifies the index's prefix length.

	Determining prefix lengths
	How to find ideal prefix length of a column. 
		calculate the overall selectivity of the column 
		compare it to the selectivity of the prefix.

	For example 
		where we have a table with a column 
			first_name, 
			with 
				3009 unique first names 
				out of 500,000 people. 
				In this case, the selectivity of the column is:

	SELECT COUNT(DISTINCT first_name) / COUNT(*) as selectivity from people;
		selectivity of 0.0060.

	Now, we can experiment with different lengths in the LEFT() function to determine the smallest prefix length that provides close to full selectivity. Here's a sample of prefixes you can test:

	SELECT COUNT(DISTINCT LEFT(first_name, 4)) / COUNT(*) as left4 from people;
	SELECT COUNT(DISTINCT LEFT(first_name, 5)) / COUNT(*) as left5 from people;
	SELECT COUNT(DISTINCT LEFT(first_name, 6)) / COUNT(*) as left6 from people;
	SELECT COUNT(DISTINCT LEFT(first_name, 7)) / COUNT(*) as left7 from people;
	
	As you increase the prefix length of columns values
		we'll notice that the selectivity will also increase. 
		So ideal index for the column based on the minimum prefix length 
			that achieves close to full selectivity.

	Drawbacks of using prefix indexing
	----------------------------------
	prefix indexes 
		not suitable for 
			ordering or 
			grouping 
				as the index does not contain the full string value
			This index will not be considered (impossible) while 
				sorting
				grouping. 



Composite indexes
	also known as multi-column or composite indexes
	Composite indexes 
		cover multiple columns 
		 In this video, we'll discuss what composite indexes are, how to create them, and when they can and can't be used.

	Creating a composite index


	CREATE TABLE people1 (
	  id INT(11) NOT NULL AUTO_INCREMENT,
	  name varchar(100) not null,
	  first_name VARCHAR(50) NOT NULL,
	  last_name VARCHAR(50) NOT NULL,
	  birthday DATE NOT NULL,
	  PRIMARY KEY (id)
	);
	Suppose we want to create an index to retrieve records where the birth month is February. 
	We can create a function-based index using the month() function as shown below:

	INSERT INTO people1 (id, name, first_name, last_name, birthday)
VALUES (1,'John Doe', 'John', 'Doe', '1990-01-01');

INSERT INTO people1 (id, name, first_name, last_name, , birthday)
VALUES (2,'Alice Brown', 'Alice', 'Brown', '1985-07-12');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (3,'David Miller', 'David', 'Miller', '1978-03-25');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (4,'Emily Garcia', 'Emily', 'Garcia', '1992-11-08');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (5,'Michael Jones', 'Michael', 'Jones', '1980-05-17');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (6,'Sarah Hernandez', 'Sarah', 'Hernandez','1995-09-03');
id, 
INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (7,'David Lee', 'David', 'Lee', '1982-01-06');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (8,'Jessica Williams', 'Jessica', 'Williams', '1998-02-21');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (9, 'Matthew Johnson', 'Matthew', 'Johnson', '1975-08-14');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (10,'Ashley Wilson', 'Ashley', 'Wilson', '1990-04-28');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (11,'Richard Rodriguez', 'Richard', 'Rodriguez', '1987-12-19');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (12,'Amanda Moore', 'Amanda', 'Moore', '1993-10-05');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (13,'Joseph Taylor', 'Joseph', 'Taylor', '1981-06-30');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (14,'Elizabeth Garcia', 'Elizabeth', 'Garcia',  '1996-01-16');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (15,'Christopher Allen', 'Christopher', 'Allen', '1977-09-02');

INSERT INTO people1 (id, name,  first_name, last_name,  birthday)
VALUES (16,'Jennifer Hernandez', 'Jennifer', 'Hernandez', '1999-03-27');




	Single column index
		#ALTER TABLE people1 ADD INDEX first_name (first_name);
		#ALTER TABLE people1 ADD INDEX last_name (last_name);
		#ALTER TABLE people1 ADD INDEX birthday (birthday);
	
	composite index for same columns:
		#ALTER TABLE people1 ADD INDEX multi (first_name, last_name, birthday);
			max key length 3072 bytes
		ALTER TABLE people1 ADD INDEX multi (first_name, birthday);
	SHOW INDEXES FROM people;

	| Table  | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Index_type |
	|--------|------------|----------|--------------|-------------|-----------|-------------|------------|
	| people |          1 | multi    |            1 | first_name  | A         |        3107 | BTREE      |
	| people |          1 | multi    |            2 | last_name   | A         |      419540 | BTREE      |
	| people |          1 | multi    |            3 | birthday    | A         |      491583 | BTREE      |
	|--------|------------|----------|--------------|-------------|-----------|-------------|------------|
	
	Rules for composite indexes
	There are two main rules for using composite indexes:


		Left-to-right, no skipping: 
			MySQL can only access 
				index in order
			starting from the leftmost column and 
				moving to the right. 
				It can't skip columns in the index.
		Stops at the first range: 
			MySQL stops using the index after the first range condition encountered.

	Analyzing index usage
	
	
	EXPLAIN SELECT * FROM people1 WHERE first_name = 'Aaron' AND last_name = 'Francis';
	|----|-------------|--------|------|---------------|-------|---------|-------------|------|----------|
	| id | select_type | table  | type | possible_keys | key   | key_len | ref         | rows | filtered |
	|----|-------------|--------|------|---------------|-------|---------|-------------|------|----------|
	|  1 | SIMPLE      | people | ref  | multi         | multi | 404     | const,const |    1 |   100.00 |
	|----|-------------|--------|------|---------------|-------|---------|-------------|------|----------|
	
	The EXPLAIN output shows that the multi index is being used
		with a key length of 404 bytes. 
		This indicates that 
			MySQL is using both the first_name and last_name parts of the index.

	If we add birthday to the mix, the key_len jumps to 407.

	#EXPLAIN SELECT * FROM people1 WHERE first_name = 'Aaron' AND last_name = 'Francis' and birthday = '1989-02-14';
	EXPLAIN SELECT * FROM people1 WHERE first_name = 'Aaron' and birthday = '1989-02-14';

	|----|-------------|--------|------|---------------|-------|---------|-------------------|------|----------|
	| id | select_type | table  | type | possible_keys | key   | key_len | ref               | rows | filtered |
	|----|-------------|--------|------|---------------|-------|---------|-------------------|------|----------|
	|  1 | SIMPLE      | people | ref  | multi         | multi | 407     | const,const,const |    1 |   100.00 |
	|----|-------------|--------|------|---------------|-------|---------|-------------------|------|----------|



	EXPLAIN SELECT * FROM people1 WHERE first_name = 'Aaron' AND last_name < 'Francis' and birthday = '1989-02-14';

	|----|-------------|--------|-------|---------------|-------|---------|-----|------|----------|
	| id | select_type | table  | type  | possible_keys | key   | key_len | ref | rows | filtered |
	|----|-------------|--------|-------|---------------|-------|---------|-----|------|----------|
	|  1 | SIMPLE      | people | range | multi         | multi | 404     |     |   55 |    10.00 |
	|----|-------------|--------|-------|---------------|-------|---------|-----|------|----------|


	The key length remains 404 bytes
		MySQL stops using the index 
			at the first range condition 
			(last_name in this case) and 
			doesn't use the birthday part of the index.

	Tips for defining composite indexes
	Choosing the right order for columns 
		in a composite index 
		depends on the access patterns of your application. 
		Consider the following tips when defining composite indexes:

	Equality conditions that are commonly used 
		would be good candidates 
			for being first in a composite index.
	Range conditions or less frequently used columns 
		would be better candidates 
			for ordering later in the composite index.


Covering indexes
	
	What are covering indexes?
	A covering index 
		regular index 
			that provides all the data required 
				for a query without having to access the actual table. 
		When a query is executed
			database 
				looks for the required data in the index tree
				retrieves it
				returns the result. 
		This eliminates the need for the engine to access the actual table
			saving a secondary traversal to gather the rest of the data.

	For an index to be considered a covering index, 
		it must have all the data needed for a particular query. 
		This includes the columns being selected
			columns being filtered on
			columns being used for sorting. 
		If an index satisfies all of these requirements, 
			it is said to be a "covering index" for that query.

	How do covering indexes work?
	In MySQL
		primary key is a clustered index
		data is physically sorted 
			in the order of the primary key.

		A secondary index
			separate data structure 
				maintains a copy of part of the data. 
		When you create a composite index
			MySQL 
				creates a B-tree 
					on the specified columns to index them. 
				This B-tree 
					contains a copy of the indexed columns 
					a pointer to the corresponding row in the clustered index.

	When a query is executed
		engine traverses the B-tree 
		to find the required rows and 
		then follows the pointer to fetch the corresponding rows 
			from the clustered index. 
		For covering index
			engine can retrieve all the data 
				it needs from the secondary index itself
				without having to access the clustered index.

	This is possible because a covering index includes all the columns required for the query and does not need to access the clustered index to retrieve additional data.

	When to use covering indexes
	Covering indexes 
		significantly improve query performance. 
		Not suitable for all situations.

	Because a covering index must include all of the columns required for a query, 
		it can be challenging to find a covering index in the real world. 
		This is particularly true for queries that require multiple columns or involve complex filtering or sorting.

	When designing indexes, it is essential to strike a balance between creating indexes that optimize query performance and maintaining the overall performance of the database system. Indexes can have a significant impact on database performance, and an excessive number of indexes can lead to slower performance. It may or may not make sense to try to design for a covering index.

	In general, it is best to create indexes that meet the specific needs of your queries while minimizing the number of indexes required. This approach will help ensure optimal query performance while maintaining the overall performance of your database system.


Functional indexes
	
	What are function-based indexes?
	Function-based indexes 
		where you need to create an index 
			on a function 
			rather than a column. 
		For example
			to retrieve all the records 
				where the birth month is February. 
			
			
			For e.g. 
				birthday column in your table
					apply a function month() to it 
						will not allow you to use an index. 
				Function-based indexes come in handy.

	A function-based index 
		created by applying a function 
			to one or more columns of a table
			create an index on the results of that function. 
		allows 
			to search for records 
			based on the results of that function
				still taking advantage of the speed and efficiency benefits of an index.

	Creating a function-based index
	To better understand how to create a function-based index in MySQL, let's take a look at a sample table called people.
	ALTER TABLE people1 ADD INDEX idx_month_birth ((MONTH(birthday)));
		The double parentheses are used to denote that we are creating a function-based index.



select MONTH(birthday) from people1;
	select * from people1 where MONTH(birthday) = 2;
	explain select * from people1 where MONTH(birthday) = 2;
	
	
	
Indexing JSON columns
	How to index JSON in MySQL: 
		generated columns and function-based indexes
	MySQL provides two viable methods to index specific keys out of a JSON blob: 
		generating a column and creating a function-based index. 
		In this video, we'll take a look at both methods.

	The problem with JSON blobs
	The problem? 
		MySQL doesn't support indexing JSON blobs. 
		Other blobs do.

	Method 1: generating a column
	The first method we will cover is generating a column. 
	This method works in MySQL 5.7 and 8. 
		Here's how it works:

		Alter the table to add a new generated column. We'll call it email and make it VARCHAR(255).
		Index the new email column.
		Let's break these steps down further.

		Step 1: Alter the table
			ALTER TABLE json_data ADD COLUMN email VARCHAR(255) GENERATED ALWAYS AS (`json` ->> '$.email');

		Step 2: Index the new column
			ALTER TABLE json_data ADD INDEX (email);
		
	Method 2: Function-based index
	The second method is to create a function-based index. 
	This method only works in MySQL 8. 
		
	We can use CAST function 
		to turn the JSON extract into a CHAR(255) value that can be indexed and setting the collation to utf8mb4_bin.

		ALTER TABLE json_data ADD INDEX ((
			CAST(`json`->>'$.email') AS CHAR(255) COLLATE utf8mb4_bin)
		));




Indexing for wildcard searches
	
	Let's say we want to search for all rows 
		where the email column starts with the name "aaron." 
	We can use the LIKE operator and the % wildcard character to find all matches:

		SELECT * FROM people WHERE email LIKE 'aaron%';
	
		return all rows where the email column starts with "aaron" 
			We don't care what follows
	
	Adding an index
	
	ALTER TABLE people ADD INDEX (email);
		SELECT * FROM people WHERE email LIKE 'aaron%';
		explain SELECT * FROM people WHERE email LIKE 'aaron%';
		
	EXPLAIN SELECT * FROM people WHERE email LIKE 'aaron%';
	| --- | ----------- | ------ | ----- | ------------- | ----- | ------- | --- | ---- | -------- |
	| id  | select_type | table  | type  | possible_keys | key   | key_len | ref | rows | filtered |
	| --- | ----------- | ------ | ----- | ------------- | ----- | ------- | --- | ---- | -------- |
	| 1   | SIMPLE      | people | range | email         | email | 1022    |     | 180  | 100.00   |
	| --- | ----------- | ------ | ----- | ------------- | ----- | ------- | --- | ---- | -------- |
	
	Querying for specific words or phrases
	N.B: 
		MySQL can only use index until it reaches a wildcard character
			such as %. 

	For example, let's say we want to find all rows where the email column contains the word "aaron" 
		anywhere within the column:

	SELECT * FROM people WHERE email LIKE '%aaron%';
	
	Searching by domain
	If we want to search for rows based on the domain of the email address, 
		we may be tempted to put the wildcard at the beginning of the string. 
		However, as we mentioned earlier, 
			MySQL cannot use an index when a wildcard character is at the beginning of a search string.

	Workaround 
		create a generated column 
			that stores only the domain of the email address, 
			create an index on that column. 
			This allows us to perform a strict equality check
				rather than a wildcard search.

	For example
		we can create a email_domain column that extracts the domain from the email address:

		ALTER TABLE people ADD COLUMN email_domain VARCHAR(255) AS (SUBSTRING_INDEX(email, '@', -1));
	
		Then, we can add an index on this column and perform a search like so:

		ALTER TABLE people ADD INDEX (email_domain);

		SELECT * FROM people WHERE email_domain = 'example.com';
		explain SELECT * FROM people WHERE email_domain = 'example.com';
		
Full text indexes
	While B-tree indexes work well for wildcard searches at the end of a search string, 
		they may not be sufficient for more complex text searches. 
		 In these cases, we can use a full text index.
	Full text indexes allow us to search for specific words or phrases within a larger text column with much greater efficiency than using a simple wildcard search. We'll look at how to use full text indexes in MySQL in the next video.

Full-text searching in MySQL
	Searching for data in a database is a crucial aspect of creating effective applications. While simple string searches can help find basic results, what happens when you're searching across multiple columns or trying to find specific words within a block of text? That's where full-text indexing and full-text searching come in handy in databases like MySQL.

	In this video, we'll go over how to use full-text indexing in MySQL, including adding indexes and implementing full-text searches with natural language and boolean modes.

	Adding full-text indexes
	To add a full-text index to a table in MySQL, you can use an ALTER TABLE statement. In this example, we'll be adding a full-text index across the first_name, last_name, and bio columns in our people table.

	ALTER TABLE people ADD FULLTEXT INDEX `fulltext`(first_name, last_name, bio);
		ALTER TABLE people ADD FULLTEXT INDEX `fulltext`(first_name, last_name);
	Note the use of the FULLTEXT keyword to create a full-text index instead of a regular B-tree index.

	Creating a full-text index can take some time, so be prepared for it to take longer than a regular index. You can check that the index was created successfully by running SHOW INDEXES FROM people; which should display the new full-text index.

	Implementing full-text searches
	With the full-text index in place, we can start implementing full-text searches.

	Natural language mode
	By default, full-text searches in MySQL are done in natural language mode. Natural language mode matches the search query against the indexed columns and returns the most relevant results.

	For example, to search for all people with the first name "Aaron," we can use the following query:

	SELECT * FROM people WHERE MATCH(first_name, last_name, bio) AGAINST('Aaron');
	SELECT * FROM people WHERE MATCH(first_name, last_name) AGAINST('Jennifer');
	explain SELECT * FROM people WHERE MATCH(first_name, last_name) AGAINST('Jennifer');
	This query will search across all three indexed columns and display all rows where "Aaron" appeared.

	Boolean mode
	For more advanced full-text searches, you can switch to boolean mode. Boolean mode allows you to use modifiers, like +, -, >, <, and parentheses in your search query.

	Here's an example of a boolean search query:

	SELECT * FROM people
	  WHERE MATCH(first_name, last_name, bio) AGAINST('+Aaron -Francis' IN BOOLEAN MODE);
	This query will search for all rows where "Aaron" appears and exclude any rows where "Francis" appears. The + indicates that "Aaron" is a required search term, and the - indicates that "Francis" is excluded.

	In boolean mode, you can also add quotation marks to search for an exact phrase or use the NEAR operator to search for words within a certain distance of each other.

	Sorting results by relevancy
	When using natural language mode, MySQL automatically orders the results by their relevancy score, with the most relevant result at the top. However, in Boolean mode, you need to manually sort the results.

	Luckily, MySQL returns the relevancy score as part of the search query results. You can use this score to sort the results using an ORDER BY statement.

	Conclusion
	Full-text indexing and searching in MySQL can be a powerful tool for searching across multiple columns and finding specific words or phrases within text blocks. While it may not be as robust as a standalone search engine, it's a great option if you're already using MySQL as your database and want to avoid adding another tool to your infrastructure.



Invisible indexes
Making indexes invisible in MySQL
	Sometimes you might need to drop an index in MySQL, but you're not entirely sure of the ramifications. You might feel nervous and want to make sure you've looked at every query that uses this index. This is where making an index invisible in MySQL comes in handy. In this video, we'll look at how to make indexes invisible and why you might want to do so.

	Why make an index invisible?
	There are times when you need to drop an index due to its inefficiency or because it's no longer used by your queries. However, before you do so, you get nervous and start thinking, "What if I've missed something?" Making an index invisible allows you to monitor how your queries perform without the index without having to rebuild it. If everything goes well, you can drop the index. But if something goes wrong, you can quickly turn the index back on without any hassle. Thus, making an index invisible reduces the risks and potential complications of dropping an index.

	How to make an index invisible
	Making an index invisible is a straightforward process in MySQL:

	ALTER TABLE people ALTER INDEX email_idx INVISIBLE;
	This command makes the email_idx index invisible so that it is no longer used in any queries.

	To verify that the index is invisible, you can use the show indexes from [table] command again. This time, you'll notice that the email_idx index still exists, but its "Visible" column is set to "NO."

	| Table  | Non_unique | Key_name  | Column_name | Collation | Cardinality | Index_type | Visible |
	| ------ | ---------- | --------- | ----------- | --------- | ----------- | ---------- | ------- |
	| people | 1          | email_idx | email       | A         | 468192      | BTREE      | NO      |
	If you want to use the index again, you can revert it back to its visible state using the "alter index [index_name] visible" command.

	ALTER TABLE people ALTER INDEX email_idx VISIBLE;
	This command makes the index visible again so that MySQL uses it in queries.

	Benefits of making an index invisible
	Making an index invisible enables you to test your queries without risking data loss or any adverse impact on system performance. Once you make an index invisible, MySQL will stop using it in any queries, but it will still maintain this index's integrity. If something goes wrong during testing, it's easy to revert back to the original index state.

	You can monitor your system's performance with minimal to no effect on your workflow. Once you are confident of your system's stability with the invisible index, you can fully drop the index.



Duplicate indexes
	Understanding duplicate indexes in MySQL
	As a MySQL user, you likely understand the importance of creating indexes on your database tables to make queries run more efficiently. However, not all indexes are created equal, and some may end up being redundant. In this video, we'll take a deep dive into duplicate indexes in MySQL and how they can impact the performance of your queries.

	Identifying duplicate indexes
	To start, let's take a look at an example of duplicate indexes. Suppose we have a table called people, and we create two indexes:

	ALTER TABLE people ADD INDEX first_name (first_name);
	ALTER TABLE people ADD INDEX full_name (first_name, last_name, birthday);
	If we run SHOW INDEXES FROM people, we'll see that we now have two indexes, first_name and full_name.

	Based on what we've learned so far about composite indexes, we know that the full_name index covers the first_name index, since the first key part of the full_name index is first_name. Therefore, the first_name index is redundant and can be safely removed.

	However, it's important to note that when we add an index on a column in InnoDB, we're really adding an index on column_name and id. Similarly, when we add an index on multiple columns, we're actually adding an index on column_1_name, column_2_name, column_3_name, and id. This is because InnoDB always appends the primary key to the leaf nodes of each index.

	Handling redundant indexes
	To remove a redundant index, we can use the following code:

	ALTER TABLE people ALTER INDEX first_name INVISIBLE;
	This makes the index invisible to the MySQL query planner, effectively removing it without deleting it. Now, when we run a query, MySQL will happily use the full_name index and won't even consider the first_name index.

	However, it's important to note that removing a redundant index can have unintended consequences, especially if you depend on the ordering of the rows in that index. For example, if you run a query like this:

	SELECT * FROM people WHERE first_name = 'Aaron' ORDER BY id DESC;
	MySQL will use the full_name index for the access pattern, but it will have to manually sort the rows because the id column is all the way at the end of the index. This can have a negative impact on performance, especially for large tables.

	Preventing duplicate indexes
	To prevent duplicate indexes from occurring in the first place, it's important to keep an eye out for indexes that have overlapping leftmost prefixes. In our example above, we had a single column index on first_name and a composite index on first_name, last_name, and birthday. Since the first_name column is the leftmost part of both indexes, they were redundant.

	If you find that you have multiple indexes with overlapping prefixes, consider removing the redundant ones to improve the performance of your queries.


Foreign keys
Foreign keys in MySQL
	In this video, we will explore the concept of foreign keys and how they can be used to build and maintain data relationships within relational databases.

	Foreign keys vs. foreign key constraints
	To start, it's important to understand the difference between foreign keys and foreign key constraints. 
	A foreign key is a column or set of columns in a table that references the primary key of another table. 
	This enables related data to be linked together in separate tables.

	On the other hand, a foreign key constraint is a condition that ensures the referential integrity of the 
		data by enforcing a relationship between the foreign key and the referenced primary key. 
		This means that the constraint will guarantee that all data references are valid and consistent, preventing data from being added, updated, or deleted in a way that would break the relationships between tables.

	It's worth noting that foreign keys can exist without constraints, but constraints are helpful to maintain referential integrity. Constraints also require additional computation to maintain, so at a certain scale, you may need to consider dropping some constraints if they become too costly in terms of performance.

	Creating tables with foreign keys
	Let's take a look at a simple example of creating two tables with a foreign key constraint. We'll start with a parent table and a child table. Here's the code to create the parent table:

	CREATE TABLE parent (
	  ID BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY
	);
	This creates a table parent with a single column id as a primary key. Now let's create the child table with a foreign key constraint that references the parent table:

	CREATE TABLE child (
	  ID BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
	  parent_id BIGINT UNSIGNED,

	  FOREIGN KEY (parent_id) REFERENCES parent(ID)
	);
	This creates a table child with two columns, id and parent_id. The parent_id column references the primary key of the parent table using a foreign key constraint. This constraint enforces the referential integrity of the data, ensuring that any data added to the child table is consistent with the data in the parent table.

	However, it's important to note that when creating a foreign key constraint, the referenced column must be of the same data type as the referencing column. For instance, if the id column in the parent table is unsigned, the parent_id column in the child table must also be unsigned. Additionally, the length and character set of string columns used for referencing each other should match for optimal performance.

	Modifying data with foreign keys
	Now that we have our tables set up with a foreign key constraint, let's take a look at how data can be modified with this constraint in place.

	First, let's insert some data into the child table:

	INSERT INTO child (parent_id) VALUES (1);
	This code attempts to insert a record into the child table with a parent_id value of 1. However, since the parent table is currently empty, this will fail because there is no data to reference.

	Let's insert a record into the parent table to correct that:

	INSERT INTO parent (ID) VALUES (1);
	Now that we have a record in the parent table, we can successfully insert a record into the child table:

	INSERT INTO child (parent_id) VALUES (1);
	The foreign key constraint ensures that this data is consistent and reflects a valid relationship between the two tables.

	If we try to delete the record from the parent table, we would encounter another issue:

	DELETE FROM parent WHERE ID = 1;
	This code attempts to delete the record from the parent table with an ID of 1. Since there is still a record in the child table that references this ID, the foreign key constraint will prevent this deletion from taking place.

	To enable cascading deletes or nullifying deleted references, additional options can be set on the foreign key constraint. It is important to consider the scale and impact of these options to avoid unintended consequences such as excessive data deletion or corruption.


------------------------------------------------------
primary key index
	covered
spatial index
	CREATE SPATIAL INDEX location_idx ON locations (geom);
	it's R-Tree index in MySQL, 
		specialized data structure 
			used to optimize queries 
				involving geospatial data. 
		Acts like a roadmap 
			for efficiently finding locations within your database.
unique index
	covered
column indexes
	covered
multiple-column indexes
	covered
invisible indexes
	------------------------------
Reference: 		
	Reference: https://www.pythian.com/blog/technical-track/mysql-8-0-invisible-indexes (but corrections made)
	CREATE TABLE test (a INT NOT NULL,b INT NOT NULL,UNIQUE b_idx (b)) ENGINE = InnoDB;
		The definition shows there is no explicit primary key defined here, 
			but the UNIQUE index on NOT NULL column b places the same constraint on rows as a primary key
			hence it cannot be made invisible:
	ALTER TABLE test ALTER INDEX b_idx INVISIBLE;
		fails ERROR 3522 (HY000): A primary key index cannot be invisible.
	ALTER TABLE test ADD PRIMARY KEY (a);
		unique index on b no longer acts as an implicit primary key, so it can be made invisible now.
	ALTER TABLE test ALTER INDEX a_idx INVISIBLE;
		ERROR 3522 (HY000): A primary key index cannot be invisible.
	SHOW INDEXES FROM test;
		b_idx is visible 
	ALTER TABLE test ALTER INDEX b_idx INVISIBLE;
		Query OK, 0 rows affected (0.01 sec)
		Records: 0  Duplicates: 0  Warnings: 0
	SHOW INDEXES FROM test;
		b_idx is now invisible 
		
	------------------------------
	B-Tree 
		 CREATE INDEX emp_no ON employees(employee_id);
Hash Indexes
	N.B: HASH indexes are only for in-memory tables (or NDB) but not myISAM or InnoDB 
	---------------------------
	CREATE UNIQUE INDEX employee_id_idx USING HASH ON employees (employee_id );

SHOW INDEXES FROM employees;

---------------------------------------------------------------------
	How to decide which index to use?
---------------------------------------------------------------------

Here's how indexes work:
	
	SQL index is a separate structure 
		maps specific values in a 
			table column to the actual rows containing those values.
	When you execute a query that 
		filters or 
		sorts data 
			based on the indexed column
	database engine 
		locate the relevant rows using the index
		instead of scanning the entire table.

	For detailed step by step evaluation refer: 
		https://dev.mysql.com/doc/refman/8.0/en/mysql-indexes.html
	


Benefits of Using Indexes:

	Faster Query Execution: 
		Indexes significantly speed up queries 
			that involve 
				filtering or 
				sorting data 
					based on the indexed column(s). 
		because the database can quickly locate relevant rows 
			using the index 
				rather than examining every row in the table.
	Improved Read Performance: 
		Queries that retrieve data 
			based on indexed columns 
				benefit the most from 
					indexing, 
				leading to faster response times and 
				better overall database performance.
	Efficient WHERE Clauses: 
		Indexes are particularly helpful for queries with 
			WHERE clauses that filter data based on specific column values.

Types of Indexes:

	B-Tree Indexes: 
		The most common type of index in SQL. 
		They organize data in a tree-like structure 
			for efficient searching and retrieval.
	Hash Indexes: 
		Internal only for memory db.
		Faster for exact value lookups 
			but don't support efficient range-based queries 
			(e.g., finding all values between a specific range).
	Composite Indexes: 
		Indexes can be created on multiple columns together, 
			improving performance for queries 
				that involve 
					filtering 
					or 
					sorting 
						based on combinations of these columns.

When to Use Indexes:

	Frequently Queried Columns: 
		Create indexes on columns that are often used in 
			WHERE clause 
				conditions, 
				joins, or 
				sorting operations within queries.
	Selective Queries: 
		Indexes are most beneficial for 
			queries that filter or sort data, 
			reducing the number of rows the database engine needs to scan.
	Large Tables: 
		For tables with a significant number of rows
			indexes can make a substantial performance difference in query execution speed.

		


Things to Consider with Indexes:
	Overhead: 
		Creating and maintaining indexes 
			consumes storage space 
			requires additional processing 
				when data is 
					inserted, 
					updated, or 
					deleted.
	Index Selection: 
		Choosing the right columns to index is crucial. 
		Indexing every column 
			not necessary and 
			can negatively impact performance 
				in some cases.
	Monitoring and Maintenance: 
		Review and adjust indexes over time 
			as 
				query patterns and 
				data volumes change.

---------------------------------------------------------------------
Module 5	Module 5: PLSQL Data Types
---------------------------------------------------------------------
---------------------------------------------------------------------
	Numeric Data Types
---------------------------------------------------------------------

1. Integer Types:

Used to store whole numbers (no decimal points).
Examples: TINYINT, SMALLINT, INTEGER, BIGINT.
These data types differ in the range of whole numbers they can represent.
	INTEGER: 
		default integer type 
			in many SQL implementations, 
			offer a wider range 	
				(e.g., -2,147,483,648 to 2,147,483,647).
	BIGINT: 
		Can accommodate very large integer values, 
		ranging from 
			-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.
	TINYINT: 
		used for small integer values
			ranging from -128 to 127.
	SMALLINT: 
		Stores values within a broader range
			from -32,768 to 32,767.

2. Decimal or NUMERIC Types:

	Designed to store numbers with decimal points
		allow for precise representation of fractional values.
	These data types typically specify two properties:
		Precision: 
			Total number of digits the data type can hold 
				(including both integer and decimal parts).
		Scale: 
			The number of digits allowed to the right of the decimal point.
	Example: 
		DECIMAL(p, s) - 
			'p': 
				precision and 
			's' 
				scale. 
		DECIMAL(5,2) 
			can store values from -999.99 to 999.99.
3. Floating-Point Types:

	Represent real numbers using a scientific notation format.
	Examples: 
		FLOAT, 
		DOUBLE PRECISION (DOUBLE).
	wider range 
		compared to 
			integer or 
			decimal 
	has inherent limitations in precision 
		due 
			the way they store numbers internally.
			
			--------------------------------
			For example
				number 1.2345:

				Significand: 12345
				Exponent: -3 (decimal point is shifted 3 places to the left)

			Limitations:

				Precision: 
					Since computers have a fixed number of bits to store the significand, 
						they can only represent certain real numbers exactly. 
						Numbers with non-repeating decimal representations 
							might be imprecise 
							or 
							rounded when stored in floating-point format.
				Rounding Errors: 
					Arithmetic operations like 
						addition, 
						subtraction, 
						multiplication, and 
						division 
							can introduce rounding errors 
							due to 
								inherent limitations of precision 
									in floating-point representation. 
							These errors can accumulate over a series of calculations.
		--------------------------------

			
	FLOAT 
		less precision 
			than DOUBLE PRECISION.

---------------------------------------------------------------------
	Character Data Types
---------------------------------------------------------------------

Character data types in SQL 
	store textual information 
	define 
		format, 
		size, and 
		character set limitations 
			for the text you can store. 
	Selecting the appropriate character data type is crucial for 
		efficient storage, 
		retrieval, and 
		manipulation of 
			textual data. 
	Here's a breakdown of some common character data types in SQL:

CHAR:
	Fixed-length: 
		Allocates a predetermined number of characters for each value
			regardless of the actual data length.
	Padding: 
		If the data entered is less than the defined length
			spaces are added to 
				pad the remaining characters 
			ensure consistency in table structure.
	Example: 
		CHAR(20) 
			can store text up to 20 characters. 
			If you enter "Hello", 
				store "Hello<15 spaces>" (with 15 spaces).
VARCHAR:
		Variable-length: 
			Allocates storage space 
				based on the actual length of the data entered
				make it more efficient for storing text of varying lengths.
	Maximum length: 
		There's a defined maximum limit 
			for the number of characters a VARCHAR column can hold.
	Example: 
		VARCHAR(50) can store text up to 50 characters. 
		"Hello" would be stored as "Hello" (without any padding).
TEXT and CLOB:
	Large text objects: 
		Designed to store significantly larger amounts of text data, 
			often used for 
				long descriptions, 
				articles, or 
				unstructured textual content.
		Storage limitations: 
			The exact storage capacity 
				can vary depending on the specific DBMS implementation.
NCHAR and NVARCHAR:
	Unicode support: 
		data types for storing Unicode characters
			represent text in various languages 
				with special characters or symbols.
	Similar to CHAR and VARCHAR: 
		They function similarly to their non-Unicode counterparts 
			(CHAR and VARCHAR) 
		but use more storage space per character 
			to accommodate Unicode representation.

Choosing the Right Character Data Type:

	Consider the expected text length: 
		If you know the text will always be a fixed length, 
			CHAR might be suitable. 
		For varying lengths, 
			VARCHAR is more efficient.
	Storage requirements: 
		For very large text content, 
			TEXT or CLOB might be appropriate.
	Global or multilingual needs: 
		If you need to store text in multiple languages 
			with special characters, 
			NCHAR or NVARCHAR would be necessary.

Additional Considerations:

	Character Sets: 
		The character set defines the supported range of characters 
			(e.g., ASCII, UTF-8). 
		Choose a character set that accommodates the languages and symbols you expect to use.
	Collation: 
		Collation dictates how characters are sorted and compared 
			within the database. 
		Consider collation rules if sorting or searching text is a frequent operation.
	By understanding these character data types and their properties
		you can effectively manage textual data 
			in your SQL databases. 
	Select the right data type 
		efficient storage utilization
		avoids unnecessary padding
		work with data in a way that 
			aligns with your language and storage requirements.

---------------------------------------------------------------------
	Date Data Types
---------------------------------------------------------------------
MySQL 
	several data types 
	specifically designed 
		for storing dates and times:

    DATE: 
		Stores just the 
			year
			month, and 
			day information. 
		Follows 
			YYYY-MM-DD format 
			supports a range of dates from 
			'1000-01-01' to '9999-12-31'.

    DATETIME: 
		Stores both  
			date and time
			including 
				year, 
				month, 
				day, 
				hours, 
				minutes, and 
				seconds. 
		The format is 
			YYYY-MM-DD hh:mm:ss 
			supported range is 
				'1000-01-01 00:00:00' to '9999-12-31 23:59:59'.

    TIMESTAMP: 
		Similar to 
			DATETIME, TIMESTAMP 
			stores date and time. 
		But have additional characteristics like 
			time zone 
			or 
			fractional seconds. 
		The range for TIMESTAMP is generally 
			'1970-01-01 00:00:01' UTC to '2038-01-19 03:14:07' UTC 
			due to limitations in its internal representation.

    YEAR: 
		Stores just the year information in 
			YYYY 
			or 
			YY format.

When choosing a date data type in MySQL, 
	consider these factors:

    Data requirements: 
		Do you need to store just the date or both date and time?
    Precision: 
		Does your application require full seconds 
		or 
		do milliseconds or microseconds matter?
    Range: 
		What is the expected range of dates your application will handle?

For e.g., 
	for birthdays (date only)
		then DATE is sufficient. 
	order timestamps (including time), 
		DATETIME would be the better choice.
---------------------------------------------------------------------
	Boolean Data Type
---------------------------------------------------------------------

Some SQL implementations might offer a dedicated Boolean data type
	Not universally present across all SQL database systems. 
	
	Boolean values are typically handled in SQL as one among the following ways:

1. Using BIT Data Type:

	Most common approach 
		for representing Boolean values in SQL.
	BIT data type 
		store a single binary bit (0 or 1).
	By convention, 
		0 is considered FALSE and 
		1 is considered TRUE.
	Example: 
		A table column named 
			is_active can be defined with a BIT data type 
				to indicate whether a record is active (1) or inactive (0).
2. Using Integer Data Types (Less Common):

	Sometimes, 
		integer data types 
		(like TINYINT) 
			is used for Boolean values.
	Similar to the BIT approach, 
		0 represents FALSE and 
		a non-zero value (often 1) represents TRUE.
	less common 
	can be found in older database systems or specific implementations.
3. Advantages of BIT Data Type:

	Storage Efficiency: 
		A single bit is the most space-efficient way 
			to store a Boolean value 
				compared to larger data types.
	Standard Interpretation: 
		The convention of 0 for FALSE and 1 for TRUE is widely adopted, 
			making code portability 
				easier between different SQL environments 
				(assuming they also use BIT for Booleans).
4. Limitations to Consider:

	Not Self-Documenting: 
		Looking at the data type alone (BIT) 
			doesn't explicitly tell you 
				it represents Boolean values. 
		Use meaningful column names 
			and comments to improve code readability.
	Database-Specific Behavior: 
		BIT is a common approach
		some database systems 
			might have their own way 
				of handling Boolean values. 
		Be mindful of potential differences 
			when working across various SQL platforms.

---------------------------------------------------------------------
Module 6	Module 6: Primary Key vs Foreign Key
---------------------------------------------------------------------


Primary keys and foreign keys 
	crucial concepts to 
		ensure data integrity 
		establish relationships 
			between tables. 

Key differences and functionalities:

Primary Key:

	Uniquely identifies each row: 
		A primary key is a column 
		(or a combination of columns) 
			within a table 
				that uniquely identifies each record. 
		No two rows in the table can have the same value 
			for the primary key.
	Enforces data integrity: 
		ensure uniqueness
			maintains data consistency within the table.

	Optimizes data retrieval: 
		Primary keys are often used to create indexes
			which significantly speed up data retrieval queries, 
				especially when searching for specific records.
	One table can only have one primary key: 
		There can't be multiple primary keys within a single table.
Foreign Key:

	Establishes links between tables: 
		foreign key 
			column (or a combination of columns) 
				in one table 
					references the primary key of another table. 
		Creates a link between 
			related data points across different tables.
	Maintains referential integrity: 
		Foreign keys 
			help prevent orphaned records
				which occur 
					when a record in one table 
						references a non-existent record in another table.
	Enforces data consistency: 
		Foreign key values 
			reference valid primary key values 
				in the referenced table
		foreign keys promote 
			data consistency 
				across relational databases.
	A table can have multiple foreign keys: 
		A single table 
			can have multiple foreign keys 
				referencing primary keys 
					from different tables, 
					enable the creation of 
						complex relationships between data sets.
Here's an analogy to illustrate the difference:

	Imagine a library with two tables: 
		one for Books (with a primary key of ISBN) and 
		another for Borrowers (with a primary key of Borrower ID).
	The Borrowers table might have a foreign key named "ISBN" 
		that references the ISBN (primary key) in the Books table. 
	This establishes a connection between borrowers and the specific books they borrow.
In essence:

	Primary keys act like unique identification tags within a single table.
	Foreign keys act like reference links that connect data across different tables.
	By understanding these concepts, you can effectively design and manage relational databases that maintain data accuracy, consistency, and facilitate efficient retrieval of interconnected information.

---------------------------------------------------------------------
	Definition and Purpose
---------------------------------------------------------------------


In relational databases, primary keys and foreign keys are crucial for maintaining data integrity and establishing relationships between tables. Here's a breakdown of their definitions and purposes:

Primary Key:

    Definition: A primary key is a column (or a set of columns) within a table that uniquely identifies each row. No two rows can have the same value for the primary key. This enforces uniqueness and prevents duplicate data within the table.
    Purpose:
        Ensures data integrity 
			by guaranteeing every row has a distinct identifier.
        Simplifies data retrieval as 
			you can efficiently locate specific records 
				using the primary key.
        Acts as a reference point for establishing relationships 
			with other tables through foreign keys.

Foreign Key:

    Definition: 
		A foreign key is a column (or set of columns) in one table that references the primary key of another table. It creates a link between the two tables, establishing a parent-child relationship.
    Purpose:
        Enforces referential integrity 
			by ensuring data referenced in the child table 
				exists in the parent table. 
			This prevents orphaned or dangling references.
        Enables efficient joining of data 
			from multiple tables based on 
				the established relationships.

Here's an analogy to illustrate:

    Imagine a library with two tables: 
		one for Books 
			(with a primary key of ISBN) and 
		another for Borrowers 
			(with a primary key of Borrower ID). 
	The Borrowers table might have a foreign key referencing the ISBN column in the Books table. 
	This ensures a borrower has borrowed an actual book in the library (referential integrity) and 
		allows you to connect borrowing details with specific books (joining data).

---------------------------------------------------------------------
	Examples and Use Cases
---------------------------------------------------------------------
---------------------------------------------------------------------
Module 7	Module 7: The Unique Constraint
---------------------------------------------------------------------
The unique constraint in SQL is a database rule 
	enforces uniqueness within a (combination of) column in a table . 
	It ensures that specific values, 
		either in a single column 
	or 
		a combination of columns
			cannot appear more than once throughout the table. 
	
	How unique constraints work and their benefits:

How Unique Constraints Work:

	You define a unique constraint during 
		table creation 
	or 
		by altering an existing table.
	You specify which column(s) the constraint applies to. 
	This can be a single column or a combination of multiple columns.
	When you try to 
		insert a new row into the table, 
		the database engine checks the values in the designated column(s) 
			against all existing rows.
	If the database encounters a duplicate value 
		(violating the uniqueness), 
			it will 
				raise an error 
				prevent the row from being inserted/updated 
				maintaining data integrity.
Benefits of Using Unique Constraints:

	Prevents Duplicate Data: 
		Primary function of unique constraints. 
		Ensure each record has a distinct value 
			for the constrained column(s), 
			eliminate redundancy and 
			maintaining data consistency.
	Improved Data Quality: 
		By preventing duplicates
			unique constraints 
				promote data accuracy and 
				reliability 
					within your database.
	Enforces Business Rules: 
		You can use 
			unique constraints 
			to implement specific business rules. 
		For instance, 
			you might have a 
				unique constraint on a "customer_email" column to 
					ensure no duplicate email addresses exist in a customer table.
	Indexes and Performance: 
		In some cases, 
			unique constraints can lead to the creation of indexes 
				on the constrained column(s). 
			These indexes can improve performance of queries 
				that involve filtering or searching based on the unique column(s).
Comparison with Primary Keys:

	Unique constraints 
		similar to primary keys
		both enforcing uniqueness. 
		
		However, there are key differences:
			A table can only have one primary key
				but it can have multiple unique constraints.
			Primary keys 
				often come with additional constraints
				such as NOT NULL
					which is not mandatory 
						for unique constraints.
When to Use Unique Constraints:

	You should use unique constraints 
		when you need to ensure 
			uniqueness for a column 
			or 
			a combination of columns, 
			but these columns 
				don't necessarily qualify 
					as the primary identifier 
						for the entire table 
						(the role of the primary key).
	Common use cases include:
		Unique email addresses in a customer table.
		Unique identification numbers for products.
		Combinations of columns 
			that uniquely identify entities 
				(e.g., 
					"customer_id" and 
					"order_number
						" together for a unique order record).
In conclusion, 
	the unique constraint 
		is a valuable tool for 
			maintain data integrity and 
			prevent duplicate entries within your SQL tables. 
	Understanding 
		how they work 
		when to use them 
			lead to a 
				cleaner, 
				more reliable 
					relational database.
---------------------------------------------------------------------
	Definition and Purpose
---------------------------------------------------------------------


A unique constraint, in the realm of relational databases, is a rule that enforces uniqueness for a specific column or group of columns within a table. It guarantees that no two rows will have identical values in the constrained column(s).

Here's a breakdown of its definition and purpose:

    Definition: A unique constraint is a database object that restricts a column (or set of columns) from containing duplicate values. This ensures data integrity by preventing redundancy within the table.
    Purpose:
        Data Integrity: 
			Unique constraints 
				prevent duplicate entries
				maintain clean and accurate data. 
			Very important for columns that should be distinct
				like 
					email 
					order ID 
						in an orders table.
        Improved Search Performance: 
			By ensuring uniqueness
				you can efficiently 
					search and 
					retrieve data 
						using the unique column(s). 
				This is because the unique values act as a quick identifier for each row.

While similar to a primary key, there are key differences:

    Primary Key vs. Unique Constraint:
        Uniqueness: 
			Both enforce uniqueness
			table can 
				only have one primary key
				can have multiple unique constraints on different columns or combinations of columns.
        NOT NULL: 
			Primary keys by definition 
				cannot contain null values
				unique constraints can allow null values.

In essence, a unique constraint offers an extra layer of data validation beyond what a primary key can provide. It allows you to define uniqueness requirements for specific columns that might not be suitable as the primary key for the entire table.

---------------------------------------------------------------------
	Differences from Primary and Foreign Key Constraints
---------------------------------------------------------------------


Primary keys and foreign keys are both crucial for maintaining data integrity in relational databases, but they serve distinct purposes:

Uniqueness:

    Primary Key: Enforces strict uniqueness for all its columns combined. No two rows can have identical values for the primary key. A table can only have one primary key.
    Foreign Key: Doesn't necessarily enforce uniqueness within itself. It references the primary key of another table, ensuring the referenced data exists. A table can have multiple foreign keys referencing different tables or even the same table.

Values:

    Primary Key: By definition, cannot contain null values. Every row must have a distinct primary key value.
    Foreign Key: Can allow null values. This allows for scenarios where a record in the child table might not have a corresponding entry in the parent table yet (unmatched reference).

Purpose:

    Primary Key:
        Uniquely identifies each row in a table.
        Acts as a reference point for establishing relationships with other tables through foreign keys.
    Foreign Key:
        Enforces referential integrity by ensuring data referenced in one table (child) has a corresponding entry in another table (parent).
        Creates relationships between tables, enabling data retrieval and manipulation across them.

Here's an analogy:

    Imagine a school library with two tables: Students (with a primary key of Student ID) and BorrowedBooks (with a StudentID column acting as a foreign key referencing the Students table). The Student ID uniquely identifies each student (primary key), and the StudentID in BorrowedBooks ensures a book is borrowed by a valid student (foreign key).

In essence:

    Primary keys are the unique identifiers within a table, ensuring no data duplication.
    Foreign keys establish relationships between tables, maintaining data consistency by referencing existing data in another table.

---------------------------------------------------------------------
Module 8	Module 8: DDL Statements
---------------------------------------------------------------------
---------------------------------------------------------------------
	CREATE Statement
---------------------------------------------------------------------

The CREATE statement 
	fundamental Data 
		Definition Language (DDL) statement 
			used in relational databases 
				to create various database objects. 
		It's the primary tool for 
			defining 
				structure and 
				schema 
					of your database.

Here's a breakdown of the CREATE statement and its core components:

Syntax:
SQL

CREATE [schema_name.]object_type [object_name] ([column1 data_type [constraint1, constraint2, ...]], [...]);

Use code with caution.

    schema_name (optional): 
		Specifies the schema where the object will be created. In some database systems, schemas are used to logically group database objects.
    object_type: 
		Defines the type of database object you're creating. 
		most common type: TABLE 
			for creating tables
		CREATE can also be used for 
			users, 
			views, 
			stored procedures
			etc.
    object_name: 
		name you assign to 
			the database object being created (e.g., MyTable).
    column definitions: 
		Within parentheses
			columns (fields) that will 
				make up the table. 
		Each column definition includes:
        column_name: 
			The name you assign to the column.
        data_type: 
			Defines the 
				type of data the column can hold (e.g., INTEGER, VARCHAR, DATE).
        constraints (optional): These are optional rules that further define how data can be stored in the column. Common constraints include:
            NOT NULL: Ensures the column cannot contain null values.
            UNIQUE: Enforces uniqueness for the values in that column (no duplicates).
            DEFAULT: Specifies a default value to be inserted if no value is provided during data insertion.

Example:
SQL

CREATE TABLE Customers (
    CustomerID INT PRIMARY KEY NOT NULL,
    CustomerName VARCHAR(50) NOT NULL,
    Email VARCHAR(100) UNIQUE,
    Phone CHAR(12)
);

Use code with caution.

This example creates a table named Customers with four columns:

    CustomerID (integer, primary key, not null): Unique identifier for each customer.
    CustomerName (varchar(50), not null): Customer's name.
    Email (varchar(100), unique): Customer's email address (ensures uniqueness).
    Phone (char(12)): Customer's phone number.

Remember: The specific syntax for the CREATE statement might vary slightly depending on the database system you're using (e.g., MySQL, PostgreSQL, SQL Server).

---------------------------------------------------------------------
	ALTER Statement
---------------------------------------------------------------------

lab: Try it 
---------------------------
		CREATE TABLE Customers (
		  CustomerID INT PRIMARY KEY AUTO_INCREMENT,
		  CustomerName VARCHAR(255) NOT NULL,
		  Email VARCHAR(255)
		);


		ALTER TABLE Customers
			ADD Phone VARCHAR(20);

		ALTER TABLE Customers
			MODIFY Email VARCHAR(500);

			or
				ALTER TABLE Customers
				MODIFY Email VARCHAR(255) CHARACTER SET utf8mb4;

--------------------

CHARACTER SET utf8mb4 in MySQL is a specific way to store text data that allows for a very wide range of characters, including:

Most languages: English, Spanish, Chinese, Japanese, etc.
Emojis and symbols: â‚¬ âˆž and many more
Supplementary characters: Characters outside the most common set (Basic Multilingual Plane)
Here's a breakdown of what utf8mb4 means:

utf8: This refers to the encoding standard, UTF-8 (Unicode Transformation Format - 8 bit), which is a popular way to represent Unicode characters in computers.
mb4: This stands for "multi-byte 4." It indicates that utf8mb4 can use up to 4 bytes to store a single character. This allows it to represent a much larger number of characters than earlier versions like utf8mb3 (which is limited to 3 bytes).
Why use utf8mb4?

Future-proof: utf8mb4 is the recommended character set for MySQL 8.0 and later because it can handle pretty much any character you're likely to encounter.
Wider compatibility: With utf8mb4, you can store and work with text data in a variety of languages and symbol sets without worrying about corruption or missing characters.
Things to consider:

Storage space: utf8mb4 may take slightly more storage space compared to character sets that support fewer characters. However, for most use cases, this difference is negligible.
Collation: utf8mb4 works in conjunction with a collation, which defines how characters are sorted and compared. Choose the appropriate collation based on your specific language needs.
--------------------

		ALTER TABLE Customers
			DROP COLUMN Email;


		ALTER TABLE Customers
			RENAME COLUMN CustomerName TO FullName;
---------------------------


The ALTER statement, another cornerstone of Data Definition Language (DDL) in relational databases, allows you to modify the structure of existing database objects. It's essential for making changes to tables, views, indexes, and other elements after they've been created.

Here's a closer look at the ALTER statement and its functionalities:

Common Use Cases:

    Adding Columns: 
		You can use ALTER 
			to add new columns 
				to an existing table 
					to accommodate evolving data needs.
    Modifying Columns: 
		Need to change a data type, 
			allow null values, 
			or 
			set a default value 
				for an existing column? 
				ALTER facilitates these modifications.
    Dropping Columns: 
		If a column is no longer needed
			ALTER enables you to remove it 
				from the table structure.
    Renaming Columns: 
		ALTER can help you rename columns for 
			better clarity or consistency within your database schema.



Syntax:

The general syntax for ALTER statements follows this pattern:
SQL

ALTER [schema_name.]object_type object_name ALTER_COMMAND;

Use code with caution.

    schema_name (optional): Similar to the CREATE statement, you can specify the schema where the object resides.
    object_type: Defines the type of database object you're modifying (e.g., TABLE, VIEW).
    object_name: The name of the existing database object you want to alter.
    ALTER_COMMAND: This specifies the specific modification you want to perform. Here are some common examples:
        ADD COLUMN column_name data_type [constraints]: Adds a new column with the specified data type and constraints.
        MODIFY COLUMN column_name data_type [constraints]: Modifies an existing column by changing its data type or adding constraints.
        DROP COLUMN column_name: Removes a column from the table.
        RENAME COLUMN old_name TO new_name: Renames an existing column.

Example:
SQL

ALTER TABLE Customers
    ADD COLUMN Age INT NOT NULL DEFAULT 25,
    MODIFY COLUMN Phone VARCHAR(15);

Use code with caution.

In this example, we're modifying the Customers table:

    A new column named Age (integer, not null) with a default value of 25 is added.
    The existing Phone column's data type is changed from CHAR(12) to VARCHAR(15) to allow for more flexibility in phone number formats.

Important Considerations:

    While ALTER is powerful, it's crucial to use it cautiously, especially in production environments. Consider potential impacts on existing data and applications before making structural changes.
    Data integrity checks and potential downtime during modifications are essential factors to keep in mind when using ALTER.

Remember, the specific syntax and available ALTER commands might vary depending on your database system.



continue from here 
---------------------------------------------------------------------
	DROP Statement
---------------------------------------------------------------------

lab: Try it 
--------------------------
Type of drop's 
--------------
	DROP TABLE table_name;
	DROP INDEX index_name ON table_name;
	DROP VIEW view_name;
	DROP PROCEDURE procedure_name;
	DROP TRIGGER trigger_name ON table_name;
	
	

------------
source: media 
    CREATE TABLE cars (
        make varchar(100),
        model varchar(100),
        year int,
        value decimal(10, 2)
    );

INSERT INTO cars
VALUES
('Porsche', '911 GT3', 2020, 169700),
('Porsche', 'Cayman GT4', 2018, 118000),
('Porsche', 'Panamera', 2022, 113200),
('Porsche', 'Macan', 2019, 27400),
('Porsche', '718 Boxster', 2017, 48880),
('Ferrari', '488 GTB', 2015, 254750),
('Ferrari', 'F8 Tributo', 2019, 375000),
('Ferrari', 'SF90 Stradale', 2020, 627000),
('Ferrari', '812 Superfast', 2017, 335300),
('Ferrari', 'GTC4Lusso', 2016, 268000);

DELIMITER //
CREATE PROCEDURE get_all_cars()
BEGIN
    SELECT * FROM cars ORDER BY make, value DESC;
END //
DELIMITER ;

GRANT CREATE ROUTINE, ALTER ROUTINE, EXECUTE on *.* TO 'sammy'@'localhost';
FLUSH PRIVILEGES;

CALL get_all_cars;


DELIMITER //
CREATE PROCEDURE get_cars_by_year(
    IN year_filter int
)
BEGIN
    SELECT * FROM cars WHERE year = year_filter ORDER BY make, value DESC;
END //
DELIMITER ;

CALL get_cars_by_year;
CALL get_cars_by_year(2020);




DELIMITER //
CREATE PROCEDURE get_car_stats_by_year(
    IN year_filter int,
    OUT cars_number int,
    OUT min_value decimal(10, 2),
    OUT avg_value decimal(10, 2),
    OUT max_value decimal(10, 2)
)
BEGIN
    SELECT COUNT(*), MIN(value), AVG(value), MAX(value)
    INTO cars_number, min_value, avg_value, max_value
    FROM cars
    WHERE year = year_filter ORDER BY make, value DESC;
END //
DELIMITER ;


CALL get_car_stats_by_year(2017, @number, @min, @avg, @max);
SELECT @number, @min, @avg, @max;
DROP PROCEDURE get_car_stats_by_year;


Done
------------
Reference: 	
	https://www.mysqltutorial.org/mysql-triggers/
CREATE TABLE items (
    id INT PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    price DECIMAL(10, 2) NOT NULL
);

INSERT INTO items(id, name, price) 
VALUES (1, 'Item', 50.00);

CREATE TABLE item_changes (
    change_id INT PRIMARY KEY AUTO_INCREMENT,
    item_id INT,
    change_type VARCHAR(10),
    change_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (item_id) REFERENCES items(id)
);

DELIMITER //
CREATE TRIGGER update_items_trigger
AFTER UPDATE
ON items
FOR EACH ROW
BEGIN
    INSERT INTO item_changes (item_id, change_type)
    VALUES (NEW.id, 'UPDATE');
END;
//

DELIMITER ;


UPDATE items
SET price = 60.00 
WHERE id = 1;

SELECT * FROM item_changes;

SHOW TRIGGERS;

DROP TRIGGER update_items_trigger;
More examples refer: https://www.mysqltutorial.org/mysql-triggers/
Done
------------
	DROP TRIGGER UpdateCustomerOrderTotal;
	DROP PROCEDURE UpdateCustomerEmail;
	DROP VIEW ActiveCustomers;
	DROP INDEX idx_customer_name ON Customers;
	DROP TABLE Books;




The DROP statement
	within the realm of Data Definition Language (DDL) in relational databases, serves the purpose of permanently deleting database objects. It's a powerful tool for removing tables, views, indexes, and other schema elements that are no longer needed.

Important to Remember:

    Permanence: Dropped objects are erased from the database and cannot be recovered using typical methods. Ensure you completely understand the implications before using DROP, especially in production environments.
    Cascading Effects: Dropping a table with foreign key relationships might also impact referencing tables. The database might cascade the deletion or require manual handling of these relationships.

Syntax:

The basic structure of the DROP statement is as follows:
SQL

DROP [schema_name.]object_type object_name;

Use code with caution.

    schema_name (optional): If your database system uses schemas, you can specify the schema where the object resides.
    object_type: Defines the type of database object you're deleting (e.g., TABLE, VIEW, INDEX).
    object_name: The name of the existing database object you want to remove.

Common Use Cases:

    Cleaning Up Unused Objects: As your database evolves, you might have tables or views that are no longer relevant. DROP helps remove these to declutter your schema and potentially improve performance.
    Restructuring Schema: Sometimes, a complete schema redesign might be necessary. DROP allows you to remove existing objects before creating the new structure.

Example:
SQL

DROP TABLE OldCustomers;

Use code with caution.

This statement permanently deletes a table named OldCustomers from the database.

Additional Considerations:

    Permissions: Depending on the database system and user privileges, you might require specific permissions to execute DROP statements.
    Data Backups: It's highly recommended to have a recent data backup before using DROP, especially for critical tables. In case of accidental deletion, you can restore the data from the backup.

Alternatives to DROP (in some cases):

    Truncate: 
		If you only want to remove the data from a table but keep the table structure, consider using TRUNCATE instead of DROP. However, TRUNCATE cannot be rolled back like DELETE.
    Disabling Objects: 
		Some database systems allow temporarily disabling tables or views instead of permanent deletion. This can be useful if you're unsure about dropping an object but want to prevent its use for a time.

Remember, exercising caution with DROP is crucial to avoid data loss or disrupting your database applications.


	

---------------------------------------------------------------------
Module 9	Module 9: DML Statements
---------------------------------------------------------------------
---------------------------------------------------------------------
	INSERT Statement
---------------------------------------------------------------------

lab: Try it
--------------
CREATE TABLE Customers (
		  CustomerID INT PRIMARY KEY AUTO_INCREMENT,
		  CustomerName VARCHAR(255) NOT NULL,
		  Email VARCHAR(255)
		);


	CREATE TABLE Products (
	  ProductID INT PRIMARY KEY AUTO_INCREMENT,
	  ProductName VARCHAR(255) NOT NULL,
	  Description TEXT,
	  Price DECIMAL(10,2) NOT NULL,
	  Stock INT NOT NULL
	);

	INSERT INTO Products (ProductName, Description, Price, Stock)
	VALUES ('T-Shirt', 'Comfortable cotton T-shirt', 19.99, 50);


	CREATE TABLE Orders (
	  OrderID INT PRIMARY KEY AUTO_INCREMENT,
	  CustomerID INT NOT NULL,
	  OrderDate DATE,
	  Amount DECIMAL(10,2) NOT NULL,
	  FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID) 
	);

	CREATE TABLE OrderSummary (
	  SummaryID INT PRIMARY KEY AUTO_INCREMENT,
	  TotalOrders INT,
	  TotalAmount DECIMAL(10,2),
	  AverageOrderAmount DECIMAL(10,2)
	);

	(continue from here - customers doesn't have data )
	INSERT INTO Orders (CustomerID, OrderDate, Amount)
	VALUES (1, '2024-05-05', 50.99),
		   (2, '2024-05-07', 25.50),
		   (1, '2024-05-08', 78.25);


	b) Inserting specific columns with values:
		INSERT INTO Products (ProductName, Price, Stock)
			VALUES ('Coffee Mug', 8.50, 25);

	c) Inserting multiple rows with a single statement:
		INSERT INTO Products (ProductName, Description, Price, Stock)
		VALUES ('Laptop', 'Powerful laptop for work and play', 799.99, 10),
			   ('Headphones', 'Wireless headphones with noise cancellation', 149.95, 30);

	d) insert into table select ...
	CREATE TABLE ActiveOrders (
	  OrderID INT PRIMARY KEY,
	  CustomerID INT,
	  OrderDate DATE,
	  TotalAmount DECIMAL(10,2),
	  FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)
	);

	INSERT INTO ActiveOrders (OrderID, CustomerID, OrderDate, TotalAmount)
		SELECT Orders.OrderID, Orders.CustomerID, Orders.OrderDate, Orders.TotalAmount
		FROM Orders
		INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID
		WHERE Customers.Active = 1;



The INSERT statement, a fundamental part of Data Manipulation Language (DML) in relational databases, is used to add new rows of data to existing tables. It's the primary tool for populating your database with the information you need.

Core Functionality:

    Inserting new records: You can insert single or multiple rows of data into a table using INSERT.
    Specifying data values: The statement allows you to define the values for each column in the new row(s).

Syntax:

There are two main ways to construct INSERT statements:

    Specifying Column Names:

SQL

INSERT INTO table_name (column1, column2, ..., columnN)
VALUES (value1, value2, ..., valueN);

Use code with caution.

    table_name: The name of the table you're inserting data into.
    column1, column2, ..., columnN: A comma-separated list of column names in the table. The order should match the order of the values you're providing.
    value1, value2, ..., valueN: A comma-separated list of values you're inserting for each corresponding column. The data types of these values must be compatible with the data types of the columns.

    Without Specifying Column Names (order matters):

SQL

INSERT INTO table_name
VALUES (value1, value2, ..., valueN);

Use code with caution.

    table_name: The name of the table you're inserting data into.
    value1, value2, ..., valueN: A comma-separated list of values in the same order as the columns in the table.

Example 1 (Specifying Column Names):
SQL

INSERT INTO Customers (CustomerID, CustomerName, Email, Phone)
VALUES (1001, 'John Doe', 'johndoe@email.com', '555-123-4567');

Use code with caution.

This example inserts a new row into the Customers table with specific values for each column.

Example 2 (Without Specifying Column Names):
SQL

Assuming the Customers table has columns in the order CustomerID, CustomerName, Email, Phone:

INSERT INTO Customers
VALUES (1002, 'Jane Smith', 'janesmith@email.com', '444-567-8910');

Use code with caution.

This example inserts another row into the Customers table, but the order of the values is crucial as it corresponds to the column order in the table.

Additional Considerations:

    Data Type Compatibility: Ensure the data types of the values you're inserting match the data types of the columns in the table.
    NULL Values: You can insert NULL values for columns that allow them.
    Inserting Multiple Rows: The VALUES clause can be used multiple times within a single INSERT statement to insert multiple rows at once.

Remember, understanding data types and potential constraints on columns is essential before using INSERT to ensure successful data population in your relational database.




---------------------------------------------------------------------
	UPDATE Statement
---------------------------------------------------------------------

lab: Try it
-----------
	CREATE TABLE Library (
	  book_id INT PRIMARY KEY AUTO_INCREMENT,
	  title VARCHAR(255) NOT NULL,
	  author VARCHAR(255) NOT NULL,
	  genre VARCHAR(50),
	  year_published INT
	);


	INSERT INTO Library (title, author, genre, year_published)
	VALUES ('The Hitchhiker\'s Guide to the Galaxy', 'Douglas Adams', 'Science Fiction', 1979),
		   ('Pride and Prejudice', 'Jane Austen', 'Romance', 1813),
		   ('To Kill a Mockingbird', 'Harper Lee', 'Fiction', 1960);

	
	UPDATE Library
		SET year_published = 2023
		WHERE title = 'Pride and Prejudice';




--------------------------------------------------







The UPDATE statement, a cornerstone of Data Manipulation Language (DML) in relational databases, allows you to modify existing data within a table. It's a powerful tool for making corrections, updating values, or applying changes to specific rows or groups of rows based on certain criteria.

Core Functionality:

    Updating data in existing rows: You can use UPDATE to modify specific values in one or more columns within a table.
    Targeting rows for update: The WHERE clause allows you to specify which rows in the table should be affected by the update.

Syntax:

The basic structure of the UPDATE statement is as follows:
SQL

UPDATE table_name
SET column1 = new_value1, column2 = new_value2, ..., columnN = new_valueN
WHERE condition;

Use code with caution.

    table_name: The name of the table you're modifying data in.
    SET clause: This clause defines which columns will be updated and their new values.
        column1, column2, ..., columnN: The columns you want to modify.
        new_value1, new_value2, ..., new_valueN: The new values to be assigned to the corresponding columns.
    WHERE clause (optional): This clause filters the rows that will be affected by the update. You can specify a condition that needs to be true for a row to be updated. If omitted, all rows in the table will be modified.

Example 1 (Updating a Single Column):
SQL

UPDATE Customers
SET Email = 'updated_email@email.com'
WHERE CustomerID = 1001;

Use code with caution.

This example updates the Email address for the customer with CustomerID 1001.

Example 2 (Updating Multiple Columns with a WHERE Clause):
SQL

UPDATE Orders
SET OrderStatus = 'Shipped', ShippedDate = '2024-05-01'
WHERE OrderID = 1234 AND OrderStatus = 'Pending';

Use code with caution.

This example updates the OrderStatus and ShippedDate for OrderID 1234, but only if the current OrderStatus is 'Pending'.

Additional Considerations:

    Data Type Compatibility: Similar to INSERT, ensure the new values you're setting for columns match their data types.
    Updating Multiple Rows: The WHERE clause allows you to target specific rows or groups of rows for the update.
    Conditional Updates: The WHERE clause enables you to perform conditional updates, modifying data only when certain conditions are met.
    CAREFUL! Using UPDATE without a WHERE clause can modify all rows in the table, so be cautious and ensure your update logic is accurate.

Remember, UPDATE is a powerful tool, but use it thoughtfully to avoid unintended modifications to your database. It's often recommended to test your UPDATE statements on a copy of your data before applying them to your main database.


---------------------------------------------------------------------
	DELETE Statement
---------------------------------------------------------------------


lab: Try it
-----------
---------------------------------------
	CREATE TABLE Library (
	  book_id INT PRIMARY KEY AUTO_INCREMENT,
	  title VARCHAR(255) NOT NULL,
	  author VARCHAR(255) NOT NULL,
	  genre VARCHAR(50),
	  year_published INT
	);


	INSERT INTO Library (title, author, genre, year_published)
	VALUES ('The Hitchhiker\'s Guide to the Galaxy', 'Douglas Adams', 'Science Fiction', 1979),
		   ('Pride and Prejudice', 'Jane Austen', 'Romance', 1813),
		   ('To Kill a Mockingbird', 'Harper Lee', 'Fiction', 1960);

	
---------------------------------------	
	
	
	

The DELETE statement, another essential part of Data Manipulation Language (DML) in relational databases, is used to remove existing rows from tables. It allows you to permanently erase data that is no longer needed or that meets specific criteria.

Core Functionality:

    Deleting rows from tables: You can use DELETE to target and remove unwanted rows from a table.
    Specifying rows for deletion: The WHERE clause helps you filter the rows that will be deleted, ensuring you target the intended data.

Syntax:

The DELETE statement follows this basic structure:
SQL

DELETE FROM table_name
WHERE condition;

Use code with caution.

    table_name: The name of the table from which you want to delete rows.
    WHERE clause (optional): This clause filters the rows that will be affected by the deletion. You can specify a condition that needs to be true for a row to be deleted. If omitted, all rows in the table will be deleted (use with caution!).

Example 1 (Deleting All Rows):
SQL

DELETE FROM OldData;

Use code with caution.

This example removes all rows from the table named OldData. Be mindful of this because it permanently deletes all data!

Example 2 (Deleting Rows with a WHERE Clause):
SQL

DELETE FROM Customers
WHERE Email = 'invalid_email@domain.com';

Use code with caution.

This example deletes rows from the Customers table where the Email address is 'invalid_email@domain.com'.

Additional Considerations:

    Permanence: Remember that DELETE permanently removes data. There is no built-in undo functionality, so use it cautiously, especially with critical data.
    Targeting Rows: The WHERE clause is crucial for ensuring you only delete the intended rows.
    Empty Table: If you delete all rows from a table, the table itself remains, but it will be empty.
    Cascading Deletes: If the table you're deleting from has foreign key relationships with other tables, deleting rows might trigger cascading deletes in the referenced tables. Be aware of these relationships to avoid unintended data loss.

Alternatives to DELETE (in some cases):

    Truncate: If you only want to remove the data from a table but keep the table structure, consider using TRUNCATE instead of DELETE. However, TRUNCATE cannot be rolled back like DELETE.

Remember, DELETE is a powerful tool for data removal, but use it with caution and proper understanding of your database schema to avoid unintended consequences.

TRUNCATE Vs	DELETE
------------------
------------------------------------------------------------------------------------------------------------
Feature			TRUNCATE									DELETE
------------------------------------------------------------------------------------------------------------
Function		Removes all rows from a table				Deletes specific rows based on a WHERE clause
Locking			Short lock on the table						Locks rows as it processes the WHERE clause
Transaction		Operates outside the current transaction	Operates within the current transaction
Rollback		Not possible								Possible if within a transaction
Performance		Faster										Can be slower depending on the WHERE clause
Data Recovery	Not possible								Possible if within a transaction
Logging			Minimal logging								May involve detailed logging of deleted rows
Use Cases		Quickly clearing all data					Deleting specific rows with criteria
------------------------------------------------------------------------------------------------------------



---------------------------------------------------------------------
Module 10	Module 10: Retrieving Records
---------------------------------------------------------------------
---------------------------------------------------------------------
	SELECT Statement
---------------------------------------------------------------------

lab: Try it
-----------
---------------------------------------
---------------------------------------


The SELECT statement in MySQL is the fundamental tool for retrieving data from your database tables. It allows you to extract specific columns or rows based on various criteria, forming the foundation for data retrieval and analysis.

Core Functionality:

    Extracting Data: You can use SELECT to retrieve data from one or more tables.
    Specifying Columns: The SELECT clause allows you to choose the specific columns you want to include in the result set.
    Filtering Data: The WHERE clause enables you to filter the data based on specific conditions, ensuring you retrieve only relevant rows.
    Sorting Data: The ORDER BY clause helps you sort the results in ascending or descending order based on one or more columns.
    Grouping Data: The GROUP BY clause allows you to group rows together based on shared values in a column, often used for aggregation with functions like SUM, AVG, COUNT.
    Combining Results: You can use JOINs to combine data from multiple tables based on relationships between them.

Basic Syntax:
SQL

SELECT [DISTINCT] select_expression1, select_expression2, ..., select_expressionN
FROM table_name1 [, table_name2 ...]
[WHERE condition]
[GROUP BY column_name1 [, column_name2 ...]]
[HAVING condition]
[ORDER BY column_name1 [ASC | DESC], column_name2 [ASC | DESC] ...]
[LIMIT offset, rows];

Use code with caution.

Clause Breakdown:

    SELECT: Introduces the statement and specifies the columns to be retrieved.
        DISTINCT (optional): Eliminates duplicate rows from the result set.
        select_expression1, select_expression2, ...: These can be column names, aliases for columns using AS, or expressions involving columns and operators.
    FROM: Specifies the table(s) from which data will be retrieved. You can include multiple tables separated by commas for joins.
    WHERE (optional): Filters rows based on a condition. You can use comparison operators, logical operators (AND, OR, NOT), and functions to create complex filtering criteria.
    GROUP BY (optional): Groups rows together based on shared values in one or more columns.
    HAVING (optional): Filters groups created by GROUP BY based on a condition applied to aggregate functions (e.g., only show groups with a total order amount greater than $100).
    ORDER BY (optional): Sorts the result set based on one or more columns in ascending (ASC) or descending (DESC) order.
    LIMIT (optional): Defines a limit on the number of rows returned and optionally an offset (starting position) for retrieving a specific subset of results.

Example 1 (Selecting Specific Columns):
SQL

SELECT CustomerName, Email FROM Customers;

Use code with caution.

This retrieves the CustomerName and Email columns from the Customers table.

Example 2 (Filtering with WHERE):
SQL

SELECT * FROM Orders WHERE OrderStatus = 'Shipped' AND OrderDate > '2024-04-01';

Use code with caution.

This selects all columns (indicated by *) from the Orders table where the OrderStatus is 'Shipped' and the OrderDate is after April 1st, 2024.

Remember: The SELECT statement is versatile and can be tailored to your specific data retrieval needs in MySQL. By understanding its clauses and functionalities, you can effectively extract and manipulate data from your database tables.
	
---------------------------------------------------------------------
	WHERE Clause
---------------------------------------------------------------------


The WHERE clause in MySQL acts as a filter within the SELECT statement, allowing you to specify conditions that rows must meet to be included in the final result set. It's a powerful tool for retrieving specific data based on various criteria.

Core Functionality:

    Filtering rows: The WHERE clause restricts the rows returned by a SELECT statement based on a condition you define.
    Complex filtering logic: You can create comprehensive filtering criteria using comparison operators, logical operators (AND, OR, NOT), and functions.

Clause Structure:

The WHERE clause follows this general syntax:
SQL

WHERE condition;

Use code with caution.

    condition: This is an expression that evaluates to true or false for each row in the table. Rows where the condition evaluates to true will be included in the result set.

Common Operators:

    Comparison Operators: These compare values between columns, constants, or expressions (e.g., =, >, <, !=, LIKE).
    Logical Operators: They combine multiple conditions (e.g., AND, OR, NOT) to create more complex filtering logic.
    Set Operators: In some cases, you can use set operators (e.g., IN, NOT IN) to check if a value exists within a set of values.

Examples:

    Filtering by a Specific Value:

SQL

SELECT * FROM Orders WHERE OrderStatus = 'Processing';

Use code with caution.

This retrieves all rows from the Orders table where the OrderStatus column is exactly 'Processing'.

    Filtering by a Range of Values:

SQL

SELECT * FROM Products WHERE Price BETWEEN 100 AND 200;

Use code with caution.

This selects all products where the Price is greater than or equal to 100 and less than or equal to 200.

    Complex Filtering with Logical Operators:

SQL

SELECT * FROM Customers WHERE (City = 'New York' OR City = 'Los Angeles') AND Country = 'USA';

Use code with caution.

This retrieves customers who are either in New York or Los Angeles, and also reside in the USA (combining conditions with OR and AND).

    LIKE Operator for Pattern Matching:

SQL

SELECT * FROM Customers WHERE Email LIKE '%@gmail.com';

Use code with caution.

This selects customers whose email addresses end with '@gmail.com' (using the LIKE operator with wildcards).

Additional Considerations:

    Nesting Conditions: You can nest WHERE clauses within parentheses to create more intricate filtering logic.
    Combining with Other Clauses: WHERE often works alongside other clauses like ORDER BY and LIMIT to refine your data retrieval process in MySQL.

By effectively using the WHERE clause, you can ensure your SELECT statements target the precise data you need from your MySQL database.

---------------------------------------------------------------------
	ORDER BY Clause
---------------------------------------------------------------------


lab: Try it
-----------
---------------------------------------
---------------------------------------
















The ORDER BY clause in MySQL is an essential component of the SELECT statement, empowering you to sort the retrieved data based on one or more columns. It dictates the order in which rows appear in the result set.

Core Functionality:

    Sorting Results: The ORDER BY clause allows you to arrange the retrieved data in ascending (default) or descending order based on the specified columns.

Syntax:

The ORDER BY clause follows this basic structure:
SQL

ORDER BY column_name1 [ASC | DESC], column_name2 [ASC | DESC], ...

Use code with caution.

    column_name1, column_name2, ...: These represent the columns you want to sort the data by. You can specify multiple columns for multi-level sorting.
    ASC (optional): Sorts the data in ascending order (lowest to highest values). This is the default behavior if no order is specified.
    DESC (optional): Sorts the data in descending order (highest to lowest values).

Sorting Examples:

    Sorting by a Single Column (Ascending):

SQL

SELECT * FROM Customers ORDER BY CustomerName;

Use code with caution.

This sorts the customers alphabetically by their CustomerName in ascending order (A to Z).

    Sorting by Multiple Columns:

SQL

SELECT * FROM Orders ORDER BY OrderDate DESC, OrderID ASC;

Use code with caution.

This sorts orders first by OrderDate in descending order (most recent first), and then within each date, sorts by OrderID in ascending order.

    Sorting with Numbers or Dates:

SQL

SELECT * FROM Products ORDER BY Price DESC;

Use code with caution.

This sorts products by their Price in descending order, ensuring the most expensive products appear first.

Additional Considerations:

    Sorting on NULL Values: By default, NULL values are usually treated as the highest values when sorting in ascending order and the lowest values when sorting in descending order. You can use the ISNULL() function to handle NULL values explicitly in your sorting criteria.
    Case Sensitivity: The sorting behavior might be case-sensitive depending on your database collation. You can use the BINARY keyword with the column name to perform a case-sensitive sort.

Combining ORDER BY with Other Clauses:

The ORDER BY clause is often used in conjunction with WHERE and LIMIT clauses to refine your data retrieval and presentation in MySQL.

By understanding the ORDER BY clause, you can effectively organize your SELECT statement results, making them easier to analyze and interpret.

---------------------------------------------------------------------
	LIMIT Clause
---------------------------------------------------------------------

lab: Try it
-----------
---------------------------------------
---------------------------------------

The LIMIT clause in MySQL is a versatile tool within the SELECT statement that grants you control over the number of rows returned in your results. It's beneficial for various purposes, including:

    Retrieving a Specific Number of Rows: This is helpful for initial data exploration or pagination in web applications.
    Optimizing Performance: Limiting the number of rows retrieved can improve query performance, especially when dealing with large tables.

Syntax:

The LIMIT clause has two main options for specifying the number of rows to return:

    Limiting the Number of Rows:

SQL

LIMIT rowCount;

Use code with caution.

    rowCount: This is a positive integer that defines the maximum number of rows you want to retrieve.

    Limiting with Offset:

SQL

LIMIT offset, rowCount;

Use code with caution.

    offset: This is a non-negative integer that specifies the number of rows to skip before starting to return results.
    rowCount: This is a positive integer that defines the maximum number of rows to be returned after skipping the offset rows.

Examples:

    Retrieving the First 10 Rows:

SQL

SELECT * FROM Products LIMIT 10;

Use code with caution.

This retrieves only the first 10 rows from the Products table.

    Skipping the First 5 Rows and Retrieving the Next 3:

SQL

SELECT * FROM Orders LIMIT 5, 3;

Use code with caution.

This skips the first 5 rows in the Orders table and then retrieves the next 3 rows.

Additional Considerations:

    LIMIT 0: While not typical for data retrieval, LIMIT 0 can be used to check the validity of a SELECT statement's syntax without fetching any data.
    Combining with ORDER BY: LIMIT often works effectively with the ORDER BY clause to control the order and number of rows returned.

LIMIT for Pagination:

When working with large datasets, LIMIT is useful for implementing pagination. You can set the rowCount to the desired number of rows per page and use the offset value to retrieve different page sets progressively.

By mastering the LIMIT clause, you can optimize your MySQL queries to retrieve the specific data you need while improving performance, especially for extensive datasets.


---------------------------------------------------------------------
Module 11	Module 11: Normalization
---------------------------------------------------------------------

Normalization in DBMS refers to a process of organizing data in your relational database tables to minimize redundancy and improve data integrity. By following normalization rules, you can create a more efficient, reliable, and maintainable database structure.

Here's a breakdown of key concepts in normalization:

Data Redundancy:

    Redundancy occurs when the same data is stored in multiple places within a database. This can lead to inconsistencies if the data needs to be updated in one place but not everywhere else.

Normalization Forms:

Normalization involves progressively eliminating redundancy by decomposing tables into smaller, more focused tables with defined relationships between them. There are several normalization forms, each with a specific set of rules:

    First Normal Form (1NF):
        Eliminates 
			repeating groups of data 
				within a table 
					by using separate columns for each attribute.
        Ensures all atomic values 
			(indivisible data units) 
				in each column.
				
			Ensures 
				each cell in a table 
					contains a single value 
					(no repeating groups of data within a cell).
				Eliminates tables with multiple 
					unrelated data groups mixed together.
			Imagine a table storing student information 
				with columns for 
					Name, 
					Age, and 
					Courses (multiple courses listed in one cell). 
				This wouldn't be in 1NF. 
				A better approach would be to have 
					separate Course names 
						in separate cells 
						or a separate table for 
							Courses linked to the student table.	
							

    Second Normal Form (2NF):
        Complies with 1NF rules.
        Eliminates partial dependencies. 
		all non-key attributes 
			(columns that don't uniquely identify a row) 
				must depend on the entire primary key 
				(the column(s) that uniquely identify a row), 
					not just a part of it.
					
			Builds on 1NF by eliminating partial dependencies.
			Ensures all non-key attributes 
				(columns that aren't part of the primary key) 
					depend entirely on the
				whole primary key, 
					not just a part of it.

			For e.g. table storing Order details 
				with columns like 
					OrderID, 
					CustomerID (primary key), 
					Product Name, and 
					Price. 
				This might not be in 2NF. 
				Why? 
					Because "Product Name" and "Price" depend on the specific OrderID, 
						not the entire primary key (which could include CustomerID as well). 
						A separate table for 
							Products linked to the Orders table would be a 2NF solution.		
					

    Third Normal Form (3NF):
        Complies with 2NF rules.
        Eliminates transitive dependencies. 
		This means 
			no non-key attribute should 
				depend on another non-key attribute; 
				all non-key attributes must rely solely on the primary key.
		Builds on 2NF by 
			eliminating transitive dependencies.
		Ensures all 
			non-key attributes 
				depend only on the primary key,
				and not on any other non-key attributes.

		E.g. 
			Order table with 
				OrderID (primary key), 
				CustomerID, 
				ProductID (foreign key referencing Products table), and 
				Discount (based on a customer loyalty program). 
				This could violate 3NF. 
				Why? 
					Because "Discount" 
						might depend on the CustomerID, 
							which is not the primary key of the Orders table. 
					To achieve 3NF, 
						you might need another table for Customer Loyalty details linked to the Customers table.



Benefits of Normalization:

    Reduced Data Redundancy: 
		Minimizes the risk of inconsistencies and wasted storage space.
    Improved Data Integrity: 
		Ensures data accuracy and consistency by centralizing updates in a single location.
    Enhanced Data Manipulation: 
		Simplifies processes like inserting, updating, and deleting data.
    Efficient Query Performance: 
		Normalized tables can lead to faster query execution due to reduced data repetition.
    Simplified Database Maintenance: 
		Makes the database easier to understand, modify, and scale over time.

Drawbacks of Normalization:

    Increased Join Complexity: 
		Normalized databases often involve more tables and joins between them, which can make queries slightly more complex to write.
    Potential Performance Overhead: 
		In some specific scenarios, excessive normalization might introduce a slight overhead due to the additional joins required.

Finding the Balance:

In practice, database design often involves striking a balance between achieving a high level of normalization and maintaining query performance. The appropriate level of normalization depends on the specific needs and usage patterns of your database application.


---------------------------------------------------------------------
	First Normal Form (1NF)
---------------------------------------------------------------------

First Normal Form (1NF) is the fundamental building block for a well-structured relational database. It establishes the foundation for further normalization processes by ensuring a basic level of data organization and eliminating data redundancy within a single table.

Here's a closer look at the core principles of 1NF:

    Atomic Values: This is the cornerstone of 1NF. It dictates that each cell (intersection of a row and column) in a table should contain a single, indivisible value. This eliminates the need for storing complex data structures like lists or nested tables within a single column.
    No Repeating Groups: 1NF prohibits the storage of repetitive data groupings within a single column. If you find yourself needing to store multiple related values for a single entity, it's typically an indication that you should separate those values into distinct columns.

Example of a Violation:

Imagine a table named Customers with the following structure:
CustomerID	CustomerName	PhoneNumbers
1001	John Doe	555-123-4567, 444-567-8910

This table violates 1NF because the PhoneNumbers column stores multiple values (phone numbers) separated by a comma. This is a repeating group, and it's not ideal for a relational database structure.

How to Normalize (in this example):

To achieve 1NF compliance, you would need to create a separate table to store phone numbers, possibly named PhoneNumbers, with the following structure:
CustomerID	PhoneNumber
1001	555-123-4567
1001	444-567-8910

Then, the original Customers table would only have the CustomerID and CustomerName columns. A foreign key relationship (linking the CustomerID in both tables) can be established to connect a customer to their phone numbers.

Benefits of 1NF:

Enforcing 1NF principles offers several advantages:

    Reduced Redundancy: Eliminates the storage of duplicate data, minimizing storage requirements and the risk of inconsistencies.
    Simplified Data Manipulation: Makes inserting, updating, and deleting data more straightforward as you're working with atomic values in each cell.
    Improved Data Integrity: Ensures data accuracy and consistency by centralizing each data point.

Remember:  1NF is the essential first step towards a well-structured relational database. By adhering to its principles, you lay the groundwork for a more reliable, maintainable, and efficient database.
---------------------------------------------------------------------
	Second Normal Form (2NF)
---------------------------------------------------------------------

The Second Normal Form (2NF) builds upon the foundation established by First Normal Form (1NF). It goes beyond eliminating repeating groups of data within a table and tackles a specific type of data redundancy known as partial dependencies.

Understanding Partial Dependencies:

A partial dependency exists when a non-key attribute (a column that isn't part of the primary key) relies on only a portion of the primary key for its value. This can lead to data integrity issues if the dependent attribute needs to be updated but the entire primary key isn't available for reference.

2NF Rules:

A table complies with 2NF if it adheres to all the rules of 1NF (atomic values, no repeating groups) and also satisfies the following additional condition:

    Elimination of Partial Dependencies: Every non-key attribute in the table must be fully dependent on the entire primary key, not just a part of it.

Example of a Violation:

Consider a table named Orders with this structure:
OrderID	CustomerID	OrderDate	ProductID	ProductName

This table violates 2NF because the attribute ProductName depends only on the ProductID, which is a part of the primary key (likely OrderID and ProductID together). There's no direct relationship between ProductName and CustomerID. If you needed to update ProductName without knowing the complete order details (OrderID), it would be problematic.

How to Normalize (in this example):

To achieve 2NF, you can decompose the Orders table into two separate tables:

    Orders Table:

| OrderID | CustomerID | OrderDate | ProductID |

    Products Table:

| ProductID | ProductName | (other product attributes) |

A foreign key relationship (linking ProductID between the tables) can be established to connect orders to their corresponding products.

Benefits of 2NF:

Enforcing 2NF offers further advantages on top of the benefits of 1NF:

    Reduced Data Redundancy: Eliminates partial dependencies, minimizing the chance of redundant data and improving data integrity.
    Improved Update Anomalies: Protects against situations where updates to a non-key attribute might lead to inconsistencies if the full primary key isn't available.

Remember:  2NF is an essential step in database normalization. By ensuring your tables comply with 2NF, you create a more robust and reliable data structure.

---------------------------------------------------------------------
	Third Normal Form (3NF)
---------------------------------------------------------------------
Third Normal Form (3NF) is a level of database normalization that refines the structure created in Second Normal Form (2NF) by eliminating a specific type of data dependency known as transitive dependency. 3NF builds upon the principles of 1NF (atomic values, no repeating groups) and 2NF (no partial dependencies) to further reduce data redundancy and enhance data integrity.

Understanding Transitive Dependencies:

A transitive dependency occurs when a non-key attribute in a table is dependent on another non-key attribute, which in turn depends on the entire primary key. This chain of dependencies can lead to data integrity issues during updates and insertions.

3NF Rules:

A table adheres to 3NF if it satisfies all the rules of 1NF and 2NF, and also meets the following additional condition:

    Elimination of Transitive Dependencies: All non-key attributes in the table must be directly dependent on the entire primary key, and not on any other non-key attribute.

Example of a Violation:

Imagine a table named Customers with this structure:
CustomerID	CustomerName	City	Country	OrderID

This table violates 3NF because the attribute City depends on the Country, and Country in turn depends on the CustomerID (assuming CustomerID is the primary key). There's a transitive dependency between City and the primary key.

How to Normalize (in this example):

To achieve 3NF, you can decompose the Customers table into two tables:

    Customers Table:

| CustomerID | CustomerName |

    CustomerLocations Table:

| CustomerID | City | Country | (other location attributes) |

A foreign key relationship (linking CustomerID between the tables) can be established to connect customers to their locations.

Benefits of 3NF:

Enforcing 3NF offers additional benefits:

    Reduced Data Redundancy: Eliminates transitive dependencies, minimizing data duplication and improving data integrity.
    Improved Update/Insert Anomalies: Protects against situations where updates or insertions to a non-key attribute might lead to inconsistencies due to transitive dependencies.
    Simplified Database Maintenance: Creates a more streamlined database structure that's easier to understand, modify, and scale.

Important Note:

While 3NF is considered a desirable level of normalization for many relational databases, it's not always strictly necessary. There might be situations where a slightly less normalized structure (2NF) can offer better query performance for specific use cases. The decision of how far to normalize a database often involves a trade-off between data integrity and query efficiency.

---------------------------------------------------------------------
	Denormalization
---------------------------------------------------------------------

Denormalization is a database design technique that intentionally deviates from the strictest levels of normalization (like Third Normal Form or 3NF) to optimize query performance. It involves strategically introducing some controlled redundancy in your database schema.

Why Denormalize?

While normalization principles are essential for data integrity and reducing redundancy, there are scenarios where strictly following them can lead to inefficiencies:

    Frequent Joins: Complex queries involving many joins between normalized tables can become slow, especially for large datasets.
    Read-Heavy Workloads: If your application primarily involves reading data and retrieving specific information, normalized structures might require multiple joins, impacting performance.

How Denormalization Works:

Denormalization involves duplicating a small amount of data from one table to another related table. This creates some redundancy, but it can significantly improve read query performance by reducing the need for joins.

Example:

Imagine an Orders table with a foreign key referencing a Customers table. To retrieve order details along with customer names in a normalized approach, you'd likely need a JOIN between the tables.

With denormalization, you could add a CustomerName column to the Orders table. This introduces redundancy (customer name exists in both tables), but it allows retrieving order details and customer names in a single query without a JOIN, potentially improving performance.

Benefits of Denormalization:

    Improved Read Query Performance: By reducing or eliminating JOINs in queries, denormalization can significantly speed up data retrieval, especially for frequently accessed data.
    Simplified Queries: Denormalized tables can sometimes lead to simpler queries that are easier to write and understand.

Drawbacks of Denormalization:

    Increased Data Redundancy: Denormalization introduces some duplication of data, which can waste storage space and lead to inconsistencies if not managed carefully.
    Increased Update Complexity: Updates to the duplicated data need to be reflected in all relevant tables, adding complexity to data maintenance.
    Potential for Data Integrity Issues: If updates aren't handled properly, denormalization can lead to data inconsistencies across tables.

When to Denormalize:

Denormalization is a trade-off. It's most suitable when:

    You have a read-heavy workload with frequent queries that involve joins.
    The performance gain from faster reads outweighs the overhead of maintaining some data redundancy.

Important Considerations:

    Controlled Denormalization: Only denormalize specific data that impacts query performance significantly.
    Careful Planning: Thoroughly analyze your queries and data access patterns before denormalizing.
    Effective Maintenance: Implement mechanisms to ensure data consistency across denormalized tables during updates.

Remember, denormalization is a powerful technique, but use it judiciously and with a clear understanding of its implications to optimize your database for your specific needs.


---------------------------------------------------------------------
Module 12	Module 12: Functions in SQL
---------------------------------------------------------------------
lab: try it 
-------------
CREATE TABLE Customers (
  CustomerID INT PRIMARY KEY AUTO_INCREMENT,
  CustomerName VARCHAR(255),
  Email VARCHAR(255)
);

INSERT INTO Customers (CustomerName, Email)
VALUES ('John Doe', 'john.doe@example.com'),
       ('Jane Smith', 'jane.smith@example.com'),
       ('Alice Johnson', 'alice.johnson@example.com');

	INSERT INTO Customers (Email)
		VALUES ('nobody@somebody.com');

		CREATE OR REPLACE FUNCTION GetCustomerFullName (customer_id IN NUMBER)
		RETURN VARCHAR2
		IS
		  customer_name VARCHAR2(255);
		BEGIN
		  SELECT Name INTO customer_name
		  FROM Customers
		  WHERE CustomerID = customer_id;

		  IF customer_name IS NULL THEN
			RETURN 'Customer Not Found';
		  ELSE
			RETURN customer_name;
		  END IF;
		END;
		/

In SQL, functions are reusable blocks of code that perform specific calculations, manipulations, or data transformations on your data. They are essential tools for enhancing your queries by making them more concise, readable, and powerful.

Types of SQL Functions:

There are various categories of SQL functions, each serving a distinct purpose:

    Aggregate Functions: These functions operate on a group of values (often from a single column) and return a single summarized result. Examples include COUNT (number of rows), SUM (total of values), AVG (average), MIN (smallest value), and MAX (largest value).
    String Functions: These functions manipulate text data. Common examples include UPPER (convert to uppercase), LOWER (convert to lowercase), TRIM (remove leading/trailing spaces), SUBSTRING (extract a portion of a string), and CONCAT (concatenate strings).
    Date and Time Functions: These functions work with date and time data. Examples include NOW() (get the current date and time), YEAR(date), MONTH(date), DAY(date), and various functions for formatting, adding/subtracting time intervals.
    Mathematical Functions: These functions perform mathematical calculations. Examples include ABS (absolute value), SQRT (square root), POWER(x raised to the power of y), CEIL (round up to the nearest integer), and FLOOR (round down to the nearest integer).
    Conditional Functions: These functions evaluate conditions and return different values based on the outcome. A common example is CASE, which allows you to define logic for returning specific values based on conditions.
    User-Defined Functions (UDFs): In some database systems, you can create custom functions to encapsulate complex logic or reuse calculations across your queries.

Using Functions in SQL Statements:

Functions are typically incorporated into SQL statements like SELECT, INSERT, UPDATE, and DELETE. Here's a general structure:
SQL

SELECT column1, function(column2), ...
FROM table_name;

Use code with caution.

In this example, function(column2) demonstrates applying a function to a specific column. You can also use functions within other functions for more complex manipulations.

Benefits of Using Functions:

    Code Reusability: Functions allow you to write reusable code blocks, reducing redundancy and improving code maintainability.
    Improved Readability: Functions make your queries more concise and easier to understand by encapsulating complex logic.
    Enhanced Data Manipulation: Functions provide a powerful way to transform, clean, and analyze your data within your queries.
    Standardized Operations: Functions ensure consistent calculations and manipulations across your queries.

Examples of Common Function Usage:

    Calculating Order Totals: Use the SUM function in the SELECT clause to calculate the total amount for each order.
    Formatting Dates: Use date functions to display dates in a specific format (e.g., YYYY-MM-DD).
    Extracting Substrings: Use the SUBSTRING function to extract specific parts of text data from a column.
    Validating Data: Use functions like ISNULL to check for null values before performing calculations.

By effectively using functions in your SQL statements, you can streamline your data retrieval and manipulation tasks, making your database operations more efficient and productive.

---------------------------------------------------------------------
	Aggregate Functions
---------------------------------------------------------------------
lab: try it
-------------


	CREATE TABLE Orders (
	  OrderID INT PRIMARY KEY AUTO_INCREMENT,
	  CustomerID INT NOT NULL,
	  OrderDate DATE,
	  Amount DECIMAL(10,2) NOT NULL,
	  FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID) -- Assuming a Customers table exists
	);

	CREATE TABLE OrderSummary (
	  SummaryID INT PRIMARY KEY AUTO_INCREMENT,
	  TotalOrders INT,
	  TotalAmount DECIMAL(10,2),
	  AverageOrderAmount DECIMAL(10,2)
	);

	INSERT INTO Orders (CustomerID, OrderDate, Amount)
	VALUES (1, '2024-05-05', 50.99),
		   (2, '2024-05-07', 25.50),
		   (1, '2024-05-08', 78.25);


	CREATE OR REPLACE FUNCTION CalculateOrderSummary()
	IS
	  total_orders INT;
	  total_amount DECIMAL(10,2);
	  avg_amount DECIMAL(10,2);
	BEGIN
	  -- Calculate aggregate values
	  SELECT COUNT(*), SUM(Amount), AVG(Amount)
	  INTO total_orders, total_amount, avg_amount
	  FROM Orders;

	  -- Insert data into OrderSummary table
	  INSERT INTO OrderSummary (TotalOrders, TotalAmount, AverageOrderAmount)
	  VALUES (total_orders, total_amount, avg_amount);
	END;
	/


Now improve this code to include min and max too.

--------------------

Aggregate functions in SQL are a powerful category of functions that work on a collection of values (often from a single column) and return a single summarized result. They are essential for data analysis and summarization within your relational database.

Common Aggregate Functions:

    COUNT: This function calculates the total number of rows in a table or the number of rows that meet a specific condition within the WHERE clause. It can also be used with the DISTINCT keyword to count the number of unique values in a column.

    SUM: This function calculates the total sum of all values in a numeric column. You can use it to find the total sales amount, overall quantity of items, or any other numerical summation.

    AVG: This function calculates the average of all values in a numeric column. It's useful for understanding the central tendency of a dataset.

    MIN: This function returns the smallest value from a numeric column. It can be helpful for identifying minimum values like the lowest product price or the earliest order date.

    MAX: This function returns the largest value from a numeric column. It can be used to find the highest product price, the latest order date, or any other maximum value.

Using Aggregate Functions:

Aggregate functions are typically used within the SELECT clause of your SQL statement, often accompanied by the GROUP BY clause to group rows together before performing the aggregation. Here's a general structure:
SQL

SELECT
    aggregate_function(column_name) AS alias,
    ...
FROM table_name
GROUP BY group_by_column;

Use code with caution.

    aggregate_function: This represents the specific function you want to use (COUNT, SUM, AVG, MIN, MAX).
    column_name: This is the column containing the values you want to aggregate.
    alias (optional): You can assign an alias to the aggregated result for better readability.
    group_by_column: This is the column used to group rows before applying the aggregate function.

Examples of Aggregate Function Usage:

    Counting Total Customers:

SQL

SELECT COUNT(*) AS total_customers
FROM Customers;

Use code with caution.

This query calculates the total number of rows (customers) in the Customers table.

    Calculating Total Sales:

SQL

SELECT SUM(amount) AS total_sales
FROM Orders;

Use code with caution.

This query calculates the sum of all amount values (assuming a numeric column for order amounts) in the Orders table, providing the total sales amount.

    Finding Average Order Value:

SQL

SELECT AVG(amount) AS average_order_value
FROM Orders;

Use code with caution.

This query calculates the average value (AVG) of the amount column in the Orders table, representing the average order value.

Additional Considerations:

    Grouping with Aggregate Functions: The GROUP BY clause is often used in conjunction with aggregate functions to group rows based on a shared value before performing the aggregation.
    Aggregate Functions on Strings: While uncommon, some aggregate functions like CONCAT can be used with string data for specific concatenations.
    Handling NULL Values: By default, some aggregate functions ignore NULL values. You can use functions like COUNT(*) or ISNULL to include or exclude NULL values as needed.

By mastering aggregate functions, you can effectively summarize and analyze your data in SQL, extracting valuable insights from your relational databases.

---------------------------------------------------------------------
	Scalar Functions
---------------------------------------------------------------------


lab: try it
-------------
---------------------------------------
---------------------------------------

In SQL, scalar functions are a type of function that, unlike aggregate functions, return a single value for each input they receive. They operate on a single value or a small number of values and perform calculations, manipulations, or transformations on that data. Scalar functions are fundamental building blocks for working with data in your SQL queries.

Key Characteristics:

    Single Output: The defining characteristic of scalar functions is that they return a single value as the result, even if they take multiple input values.
    Diverse Functionality: Scalar functions encompass a wide range of operations, including:
        String manipulation (e.g., UPPER, LOWER, SUBSTRING)
        Mathematical calculations (e.g., ABS, SQRT, POWER)
        Date and time operations (e.g., YEAR, MONTH, DAY, NOW)
        Conditional logic (e.g., CASE)
        Data type conversions (e.g., CAST, CONVERT)

Using Scalar Functions in SQL Statements:

Scalar functions are frequently incorporated into various SQL statements, including:

    SELECT: Used to retrieve data from tables. Scalar functions can be applied to columns within the SELECT clause to transform or manipulate data before retrieval.
    WHERE: Used to filter data based on conditions. Scalar functions can be used within the WHERE clause to create more intricate filtering criteria.
    UPDATE: Used to modify existing data in tables. Scalar functions can be used within the SET clause of an UPDATE statement to perform calculations or transformations on data before updating the table.
    INSERT: Used to insert new data into tables. Scalar functions can be used within the VALUES clause of an INSERT statement to prepare or modify data before insertion.

Syntax:

The general syntax for using scalar functions in SQL statements varies depending on the specific function. However, it commonly follows this pattern:
SQL

function_name(argument1, argument2, ...)

Use code with caution.

    function_name: This represents the name of the scalar function you want to use.
    argument1, argument2, ...: These are the input values or expressions that the function will process. The number of arguments can vary depending on the function.

Examples of Scalar Function Usage:

    Extracting Initials from Names:

SQL

SELECT customer_id, UPPER(LEFT(customer_name, 1)) AS initial
FROM Customers;

Use code with caution.

This query uses the UPPER function to convert the first letter of the customer_name (achieved with LEFT(customer_name, 1)) to uppercase, effectively extracting the initial for each customer.

    Calculating Order Discounts:

SQL

SELECT order_id, product_price * (1 - discount_rate) AS discounted_price
FROM Orders;

Use code with caution.

This query multiplies the product_price by (1 minus the discount_rate) to calculate the discounted price for each order using mathematical operations within the SELECT clause.

    Filtering Dates:

SQL

SELECT * FROM Orders
WHERE order_date >= DATE_SUB(CURDATE(), INTERVAL 30 DAY);

Use code with caution.

This query uses the DATE_SUB function to get the date 30 days before the current date (CURDATE()) and filters the Orders table to include only orders placed after that date.

By understanding and effectively using scalar functions, you can significantly enhance the power and flexibility of your SQL queries, allowing you to manipulate, transform, and analyze your data in diverse ways.

---------------------------------------------------------------------
	Date and Time Functions
---------------------------------------------------------------------
lab: try it
-------------
---------------------------------------
---------------------------------------

Date and time functions are essential tools in SQL for working with temporal data. They enable you to extract, manipulate, format, and perform calculations on date and time values within your relational database.

Common Date and Time Functions:

    CURDATE() or CURRENT_DATE: 
		Retrieves the current date as a YYYY-MM-DD value.
    CURTIME() or CURRENT_TIME: 
		Retrieves the current time as an HH:MM:SS value (format might vary depending on the database system).
    NOW() or CURRENT_TIMESTAMP: 
		Retrieves the current date and time as a YYYY-MM-DD HH:MI:SS value.
    YEAR(date_value): 
		Extracts the year (YYYY) from a date value.
    MONTH(date_value): 
		Extracts the month (1-12) from a date value.
    DAY(date_value): 
		Extracts the day of the month (1-31) from a date value.
    HOUR(time_value): 
		Extracts the hour (0-23) from a time value.
    MINUTE(time_value): 
		Extracts the minute (0-59) from a time value.
    SECOND(time_value): 
		Extracts the second (0-59) from a time value.
    DATE_ADD(date_value, interval): 
		Adds a specified time interval to a date value. The interval can be days, weeks, months, or years.
    DATE_SUB(date_value, interval): 
		Subtracts a specified time interval from a date value.
    DATEDIFF(date_value1, date_value2): 
		Calculates the difference between two date values in days.
    DATE_FORMAT(date_value, format_string): 
		Formats a date value according to a specified format string. Common format specifiers include %Y (year), %m (month), %d (day).

Using Date and Time Functions in SQL Statements:

Date and time functions can be incorporated into various SQL statements, including:

    SELECT: 
		Used to retrieve data from tables. Date and time functions can be used to extract specific parts of dates, calculate differences, or format dates for better readability.
    WHERE: 
		Used to filter data based on conditions. Date and time functions can be used to filter data based on date ranges or specific time criteria.
    UPDATE: 
		Used to modify existing data in tables. Date and time functions can be used to update dates or timestamps within the SET clause of an UPDATE statement.

Examples of Date and Time Function Usage:

    Filtering Orders by Date Range:

SQL

SELECT * FROM Orders
WHERE order_date BETWEEN '2024-04-01' AND '2024-04-30';

Use code with caution.

This query filters the Orders table to include only orders placed between April 1st, 2024, and April 30th, 2024.

    Calculating Age from Birthdate:

SQL

SELECT customer_id, YEAR(CURDATE()) - YEAR(birth_date) AS age
FROM Customers;

Use code with caution.

This query calculates the age of each customer by subtracting the year of birth (YEAR(birth_date)) from the current year (YEAR(CURDATE())).

    Formatting Dates for Display:

SQL

SELECT order_id, DATE_FORMAT(order_date, '%d/%m/%Y') AS formatted_date
FROM Orders;

Use code with caution.

This query uses the DATE_FORMAT function to display the order date in a user-friendly format (DD/MM/YYYY) within the result set.

By mastering date and time functions, you can effectively manage temporal data in your SQL queries, enabling you to analyze trends, filter data based on specific timeframes, and perform various date and time related operations within your database.
---------------------------------------------------------------------
Module 13	Module 13: Handling Null Values
---------------------------------------------------------------------
lab: try it
-------------
---------------------------------------
lab in following section 

IS NULL and IS NOT NULL
COALESCE
CASE
IFNULL


---------------------------------------
Null values in SQL represent the absence of a known value for a particular attribute (column) in a table record. They are distinct from empty strings or zeros and require special handling within your SQL queries to avoid errors and ensure data integrity.

Why Null Values Exist:

There are several reasons why null values might appear in a database:

    Missing Information: Sometimes, data might be genuinely unavailable during data entry due to unknown values or incomplete information.
    Not Applicable: For certain attributes, a specific value might not be relevant for a particular record (e.g., a customer's middle name might be optional).
    Data Cleaning: Null values can be used as placeholders during data cleaning processes to identify and address missing or invalid data.

Potential Issues with Null Values:

    Unexpected Results: Comparisons involving null values can lead to unexpected results in queries if not handled correctly.
    Data Integrity Concerns: Inconsistent handling of null values can compromise data integrity and lead to inaccurate results.
    Performance Implications: Certain database operations might be less efficient when dealing with null values.

Approaches to Handling Null Values:

There are several techniques for dealing with null values in SQL, each with its strengths and considerations:

    Filtering with IS NULL and IS NOT NULL:  These operators allow you to explicitly identify and filter rows based on whether a specific column is null or not null.

    Coalesce Function:  The COALESCE function returns the first non-null value from a list of arguments. It's useful for providing a default value if the primary value is null.

    CASE Statement:  The CASE statement offers more conditional control. You can define conditions to return different values based on whether a column is null or has specific values.

    Database-Specific Functions:  Some database systems might offer additional functions for handling null values, such as IFNULL (similar to COALESCE) or NVL (function name might vary).

Choosing the Right Approach:

The best approach for handling null values depends on the specific scenario and your desired outcome. Consider these factors when making a decision:

    Expected Behavior: Determine how you want null values to be treated in queries (filtered out, replaced with defaults, or handled differently).
    Data Integrity: Ensure your chosen method maintains data consistency and avoids introducing errors.
    Readability and Maintainability: Opt for solutions that make your queries clear and easy to understand for future maintenance.

By effectively handling null values in your SQL queries, you can ensure accurate data retrieval, maintain data integrity, and write more robust and reliable database code.

---------------------------------------------------------------------
	IS NULL
---------------------------------------------------------------------
lab: try it 
-------------
---------------------------------------
---------------------------------------

In SQL, the IS NULL operator is a fundamental tool for checking if a specific column in a table record contains a null value. It's essential for handling null values effectively within your queries and avoiding unexpected results.

Understanding Null Values:

    Null values represent the absence of a known value for a particular attribute (column) in a database table.
    They are distinct from empty strings or zeros and require special considerations in your SQL statements.

Using the IS NULL Operator:

The IS NULL operator is a unary operator, meaning it takes a single operand (column name or expression) and evaluates to true (1) if the operand is null, and false (0) otherwise.

Syntax:
SQL

column_name IS NULL

Use code with caution.

    column_name: This represents the name of the column you want to check for null values.

Common Use Cases:

    Filtering Rows: The IS NULL operator is frequently used within the WHERE clause of a SELECT statement to filter and retrieve only rows where a specific column contains null values.

SQL

SELECT * FROM Customers
WHERE email IS NULL;

Use code with caution.

This query selects all records from the Customers table where the email column is null, effectively identifying customers who haven't provided their email addresses.

    Combining with Other Operators: IS NULL can be combined with other comparison operators (e.g., =, >, <) to create more intricate filtering criteria.

SQL

SELECT * FROM Orders
WHERE order_date IS NULL OR shipped_date IS NULL;

Use code with caution.

This query selects orders where either the order_date or the shipped_date (or both) is null, potentially indicating incomplete order information.

Alternatives to IS NULL:

    IS NOT NULL: This operator is the logical opposite of IS NULL and returns true if the operand is not null.

SQL

SELECT * FROM Products
WHERE product_name IS NOT NULL AND price > 0;

Use code with caution.

This query selects products that have both a name (not null) and a positive price (> 0).

    Coalesce Function: The COALESCE function provides a default value if the primary value is null.

SQL

SELECT customer_id, COALESCE(email, 'No email provided') AS email_address
FROM Customers;

Use code with caution.

This query retrieves customer information, replacing null values in the email column with the string "No email provided" for better readability.

Remember:

    IS NULL is a versatile tool for null value handling in SQL.
    Use it effectively to filter rows, create complex filtering logic, and ensure your queries produce accurate results.
    Consider combining IS NULL with other operators and functions for more comprehensive control over null values in your database operations.

---------------------------------------------------------------------
	COALESCE
---------------------------------------------------------------------
lab: try it
-------------
---------------------------------------
---------------------------------------
In SQL, the COALESCE function is a powerful tool for handling null values. It allows you to specify a list of arguments, and the function returns the first non-null value it encounters in that list. This makes it extremely useful for providing default values in queries where columns might contain nulls.

Understanding Null Values:

    Null values represent the absence of a known value for a particular attribute (column) in a database table.
    They are distinct from empty strings or zeros and require special considerations within your SQL queries.

Using the COALESCE Function:

The COALESCE function takes a variable number of arguments, typically consisting of column names, expressions, or literal values. It evaluates the arguments in the order they are provided and returns the first value that is not null.

Syntax:
SQL

COALESCE(argument1, argument2, ..., argumentN)

Use code with caution.

    argument1, argument2, ..., argumentN: This represents the list of values (columns, expressions, or literals) that the function will evaluate. You can specify multiple arguments to create a chain of fallback options.

Common Use Cases:

    Replacing Null Values with Defaults: COALESCE is frequently used to replace null values in queries with a more meaningful default value.

SQL

SELECT customer_id, COALESCE(first_name, 'Unknown') AS customer_name
FROM Customers;

Use code with caution.

This query retrieves customer information. If the first_name is null, COALESCE replaces it with the string "Unknown" for a more user-friendly presentation.

    Chained Defaults: You can specify multiple arguments in the COALESCE function to create a chain of fallback options.

SQL

SELECT product_id, COALESCE(product_name, product_description, 'Product Details Not Available') AS product_details
FROM Products;

Use code with caution.

This query retrieves product details. If product_name is null, it attempts to use product_description. If both are null, it displays "Product Details Not Available".

Benefits of Using COALESCE:

    Improved Readability: By replacing null values with defaults, COALESCE enhances the readability of your query results.
    Avoiding Errors: Null values can sometimes lead to errors in calculations or comparisons. COALESCE helps prevent these errors by providing a substitute value.
    Flexibility: The ability to specify multiple arguments allows for creating customized fallback options based on your specific needs.

Alternatives to COALESCE:

    IS NULL: This operator checks if a value is null. It can be used for filtering purposes but doesn't provide replacement values.
    CASE Statement: The CASE statement offers more conditional control over handling null values. However, it can be more complex to write compared to COALESCE.

When to Choose COALESCE:

    When you want a simple and efficient way to replace null values with a default value in your queries.
    When you have a clear hierarchy of fallback options for handling nulls.

By effectively using COALESCE in your SQL queries, you can enhance data readability, prevent errors caused by null values, and create more robust and user-friendly database interactions.

---------------------------------------------------------------------
	NULLIF
---------------------------------------------------------------------
lab: try it
-------------
---------------------------------------
---------------------------------------
In SQL, the NULLIF function is used for conditional null value handling. It takes two arguments and compares them. If the arguments are equal, NULLIF returns null. Otherwise, it returns the first argument.

Understanding NULLIF:

    It's useful for scenarios where you want to exclude rows based on specific criteria or enforce data integrity rules.
    Remember, NULLIF operates on the values themselves, not whether they are null or not.

Syntax:
SQL

NULLIF(expression1, expression2)

Use code with caution.

    expression1: This is the first value or expression to be compared.
    expression2: This is the second value or expression to be compared.

How it Works:

    NULLIF evaluates both expression1 and expression2.
    If expression1 is equal to expression2, NULLIF returns null.
    If expression1 is not equal to expression2, NULLIF returns the value of expression1.

Common Use Cases:

    Enforcing Data Integrity: NULLIF can be used to ensure specific columns never contain certain values.

SQL

SELECT * FROM Orders
WHERE NULLIF(order_status, 'Cancelled') IS NOT NULL;

Use code with caution.

This query selects orders where the order_status is not equal to 'Cancelled'. NULLIF effectively removes rows with 'Cancelled' status from the results.

    Conditional Filtering: You can use NULLIF in combination with filtering conditions.

SQL

SELECT * FROM Customers
WHERE city = 'New York' AND NULLIF(country, 'USA') IS NOT NULL;

Use code with caution.

This query selects customers from New York City (based on city) who are not from the USA (based on country). NULLIF returns null if country is 'USA', filtering those rows out.

Alternatives to NULLIF:

    WHERE Clause Comparisons: You can achieve similar results using comparisons directly in the WHERE clause. However, NULLIF can sometimes offer a more concise way to express the logic.
    CASE Statement: The CASE statement provides more flexibility for handling null values and defining various conditions. However, it can be more verbose compared to NULLIF for simple comparisons.

When to Choose NULLIF:

    When you want a straightforward way to exclude rows based on a specific value comparison.
    When you need to enforce data integrity rules by ensuring a column never contains a particular value.
    When conciseness is preferred over more complex CASE statement logic for simple comparisons.

Important Considerations:

    Be cautious when using NULLIF in calculations or comparisons, as introducing null values can lead to unexpected results.
    Consider alternative approaches if you need more intricate conditional logic for handling null values.

By understanding NULLIF and its use cases, you can effectively manage specific data filtering and comparison scenarios within your SQL queries.


---------------------------------------------------------------------
Module 14	Module 14: SQL Joins
---------------------------------------------------------------------
lab: try it
-------------
---------------------------------------
Inner Join
Left Join
Right Join
Full Join
---------------------------------------

SQL joins are a fundamental concept in relational databases. They allow you to combine data from multiple tables based on a shared relationship, enabling you to retrieve and analyze information from various perspectives.

Why Use Joins?

Database tables are often designed to store related data efficiently. For instance, an Orders table might have a foreign key referencing a Customers table. To retrieve order details along with customer names, you'd typically need to join these tables.

Types of Joins:

There are several types of SQL joins, each serving a distinct purpose:

    **Inner Join (JOIN or INNER JOIN): This is the most common type of join. It returns only rows where there's a match in both tables based on the join condition.

    **Left Join (LEFT OUTER JOIN): This join includes all rows from the left table (the table specified before the LEFT keyword) and matching rows from the right table. Rows from the right table with no match in the left table will have null values in the corresponding join columns.

    **Right Join (RIGHT OUTER JOIN): Similar to a left join, but it includes all rows from the right table and matching rows from the left table. Rows from the left table with no match in the right table will have null values in the corresponding join columns.

    **Full Join (FULL OUTER JOIN): This join includes all rows from both tables, regardless of whether there's a match in the other table. Unmatched rows will have null values in the corresponding join columns.

Specifying a Join Condition:

The ON clause is used within the join syntax to specify the condition that determines which rows from each table will be included in the result set. This condition typically involves equating columns from both tables that represent the relationship between them (e.g., customers.customer_id = orders.customer_id).

Example (Inner Join):

Imagine you have Customers and Orders tables. To retrieve customer names along with their order details, you could use an inner join:
SQL

SELECT c.customer_name, o.order_id, o.order_date
FROM Customers AS c
INNER JOIN Orders AS o ON c.customer_id = o.customer_id;

Use code with caution.

This query will return rows only for customers who have placed orders (matching rows based on customer_id).

Choosing the Right Join:

The type of join you use depends on the specific data you want to retrieve and the relationships between your tables.

    Inner Join: Use this for retrieving data where rows exist in both tables.
    Left Join: Use this to retrieve all data from the left table and matching data from the right table (including rows with null values for unmatched records in the right table).
    Right Join: Similar to left join, but prioritizes the right table.
    Full Join: Use this to get all data from both tables, including unmatched rows with null values.

Benefits of Using Joins:

    Efficient Data Retrieval: Joins allow you to retrieve related data from multiple tables in a single query, improving efficiency compared to querying each table separately.
    Data Analysis: By combining data from different tables, joins enable you to perform richer data analysis and generate more comprehensive reports.
    Data Integrity: Proper use of joins helps maintain data integrity by ensuring data consistency across related tables.

Remember:

    Joins are essential for working with relational databases and retrieving data from multiple tables.
    Understand the different join types and choose the one that best suits your data retrieval needs.
    Clearly define the join condition using the ON clause to ensure accurate results.

By mastering SQL joins, you can unlock the full potential of relational databases for data retrieval, analysis, and generating meaningful insights.

---------------------------------------------------------------------
	INNER JOIN
---------------------------------------------------------------------

lab: Try it
-------------
---------------------------------------
---------------------------------------

Inner join, also referred to as simply JOIN in some database systems, is the most fundamental and frequently used type of join in SQL. It's designed to retrieve data from multiple tables based on a matching relationship between them.

Understanding Inner Joins:

    Inner joins combine rows from two or more tables based on a shared condition.
    They only return rows where there's a match in both tables according to the join criteria.
    Rows that don't have a match in both tables are excluded from the result set.

Core Functionalities:

    Specifying Tables: You provide the names of the tables you want to join in the FROM clause of your SQL query.
    Join Condition: The ON clause is crucial. It defines the matching criteria that determine which rows from each table will be included in the results. This typically involves equating columns from both tables that represent the relationship between them.

Syntax:
SQL

SELECT column1, column2, ...
FROM table1 AS alias1
INNER JOIN table2 AS alias2
ON alias1.column_name = alias2.column_name;

Use code with caution.

    column1, column2, ...: These represent the columns you want to retrieve from the tables.
    table1 AS alias1: This specifies the first table name and assigns it an optional alias (e.g., Customers AS c).
    table2 AS alias2: This specifies the second table name and assigns it an optional alias.
    ON alias1.column_name = alias2.column_name: This defines the join condition using qualified column names from both tables.

Example: Combining Customers and Orders:

Imagine you have Customers and Orders tables. To retrieve customer names along with their order details, you can use an inner join:
SQL

SELECT c.customer_name, o.order_id, o.order_date
FROM Customers AS c
INNER JOIN Orders AS o ON c.customer_id = o.customer_id;

Use code with caution.

This query will join the Customers and Orders tables based on the matching customer_id values. The result set will include only customers who have placed orders, and for each customer, it will display their name, order ID, and order date.

Benefits of Using Inner Joins:

    Efficient Data Retrieval: Inner joins allow you to fetch related data from multiple tables in a single query, streamlining data retrieval compared to querying each table separately.
    Data Integrity: By ensuring you only retrieve data with matches in both tables, inner joins help maintain data consistency.

When to Use Inner Joins:

    Inner joins are ideal when you want to retrieve data that has a clear relationship and only where rows exist in both tables involved in the join.
    For instance, to find information about customers who have placed orders or products that belong to specific categories.

By understanding and effectively using inner joins, you can write efficient SQL queries to retrieve relevant data from relational databases where rows in multiple tables share a meaningful connection.
---------------------------------------------------------------------
	LEFT JOIN
---------------------------------------------------------------------

lab: Try it
-------------
---------------------------------------
---------------------------------------

Left joins, formally called Left Outer Joins in SQL, are a type of join that retrieves all rows from the left table (the table specified before the LEFT keyword) and matching rows from the right table. They are essential for including data from the left table even if there are no corresponding matches in the right table.

Understanding Left Joins:

    Left joins ensure all rows from the left table are included in the result set, regardless of whether there's a match in the right table.
    For rows in the left table that don't have a match in the right table, the corresponding columns from the right table will contain null values.
    This allows you to see all data from the left table and identify missing information from the right table.

Core Functionalities:

    Specifying Tables: Similar to other joins, you provide the table names in the FROM clause.
    Join Condition: The ON clause defines the matching criteria between the two tables, determining which rows from the right table will be included for rows in the left table.

Syntax:
SQL

SELECT column1, column2, ...
FROM table1 AS alias1
LEFT JOIN table2 AS alias2
ON alias1.column_name = alias2.column_name;

Use code with caution.

    column1, column2, ...: These represent the columns you want to retrieve from the tables.
    table1 AS alias1: This specifies the left table name and assigns it an optional alias.
    table2 AS alias2: This specifies the right table name and assigns it an optional alias.
    ON alias1.column_name = alias2.column_name: This defines the join condition using qualified column names from both tables.

Example: Customers with or without Orders:

Imagine you have Customers and Orders tables. To retrieve all customer information along with their order details (if any), you can use a left join:
SQL

SELECT c.customer_name, o.order_id, o.order_date
FROM Customers AS c
LEFT JOIN Orders AS o ON c.customer_id = o.customer_id;

Use code with caution.

This query will include all customers from the Customers table (left table). For customers with orders, their order details (order ID and date) from the Orders table (right table) will be included. For customers without orders, the corresponding columns from Orders (order_id and order_date) will be null.

Benefits of Using Left Joins:

    Complete Information from Left Table: Left joins guarantee that all data from the left table is included, providing a comprehensive view of your data.
    Identifying Missing Information: By showing null values for unmatched rows in the right table, left joins help you identify missing data or potential discrepancies.

When to Use Left Joins:

    Left joins are ideal when you want to retrieve all data from the left table, even if there's no corresponding data in the right table.
    For instance, to get a list of all customers, including those who haven't placed any orders, or to find all products, even if some might be out of stock (with null values for quantity in a stock table).

By mastering left joins, you can effectively retrieve and analyze data from relational databases, ensuring you have a complete picture of the information stored in the left table, even when there might be missing data in the right table.
---------------------------------------------------------------------
	RIGHT JOIN
---------------------------------------------------------------------


lab: Try it
-------------
---------------------------------------
---------------------------------------
Right joins, formally called Right Outer Joins in SQL, are a type of join that prioritizes including all rows from the right table (the table specified after the RIGHT keyword) in the result set. It then matches rows from the left table based on a join condition.

Understanding Right Joins:

    Right joins ensure all rows from the right table are included in the result set, regardless of whether there's a match in the left table.
    For rows in the right table that don't have a match in the left table, the corresponding columns from the left table will contain null values.
    This is useful for scenarios where you want to analyze data primarily from the right table and identify missing information in the left table.

Core Functionalities:

    Specifying Tables: Similar to other joins, you provide the table names in the FROM clause.
    Join Condition: The ON clause defines the matching criteria between the two tables, determining which rows from the left table will be included for rows in the right table.

Syntax:
SQL

SELECT column1, column2, ...
FROM table1 AS alias1
RIGHT JOIN table2 AS alias2
ON alias1.column_name = alias2.column_name;

Use code with caution.

    column1, column2, ...: These represent the columns you want to retrieve from the tables.
    table1 AS alias1: This specifies the left table name and assigns it an optional alias.
    table2 AS alias2: This specifies the right table name and assigns it an optional alias.
    ON alias1.column_name = alias2.column_name: This defines the join condition using qualified column names from both tables.

Example: Orders with or without Customers:

Imagine you have Customers and Orders tables. To retrieve all orders along with customer information (if available), you can use a right join:
SQL

SELECT c.customer_name, o.order_id, o.order_date
FROM Customers AS c
RIGHT JOIN Orders AS o ON c.customer_id = o.customer_id;

Use code with caution.

This query will include all orders from the Orders table (right table). For orders with a matching customer in the Customers table (left table), the customer name will be included. For orders without a corresponding customer, the customer_name column will be null.

Benefits of Using Right Joins:

    Complete Information from Right Table: Right joins ensure all data from the right table is included, providing a focus on your right table's data.
    Identifying Missing Information: By showing null values for unmatched rows in the left table, right joins help you identify potential missing customer information for specific orders.

When to Use Right Joins:

    Right joins are ideal when you want to prioritize retrieving all data from the right table, even if there's no corresponding data in the left table.
    For instance, to analyze all orders placed, including those from customers who might not be registered in the Customers table yet (with null values for customer_name).

Choosing Between Left Join and Right Join:

The decision between using a left join or a right join depends on your primary focus for data retrieval.

    Use a left join if you want to ensure all data from the left table is included, even for unmatched rows in the right table.
    Use a right join if you want to prioritize including all data from the right table, even for unmatched rows in the left table.

By understanding right joins and their applications, you can effectively retrieve and analyze data from relational databases, focusing on the right table's information while identifying potential missing data connections from the left table.

---------------------------------------------------------------------
	FULL OUTER JOIN
---------------------------------------------------------------------


lab: Try it
-------------
---------------------------------------
---------------------------------------
In SQL, a FULL OUTER JOIN, also sometimes called a full join, is a comprehensive join operation that combines data from two tables and includes all rows from both tables in the result set, regardless of whether there's a matching row in the other table.

Understanding Full Outer Joins:

    Full outer joins ensure that every row from both participating tables is included in the final results.
    For rows that don't have a match in the other table, the corresponding columns will be filled with null values.
    This approach provides a complete picture of the data in both tables, even when there might not be a direct relationship between all rows.

Core Functionalities:

    Specifying Tables: Similar to other joins, you provide the table names in the FROM clause.
    Join Condition (Optional): The ON clause is optional for full outer joins. If specified, it defines the matching criteria between the two tables, influencing which rows are matched and how null values are positioned in the results. However, even without an ON clause, all rows from both tables will be included.

Syntax:
SQL

SELECT column1, column2, ...
FROM table1 AS alias1
FULL OUTER JOIN table2 AS alias2
ON alias1.column_name = alias2.column_name;

Use code with caution.

    column1, column2, ...: These represent the columns you want to retrieve from the tables.
    table1 AS alias1: This specifies the first table name and assigns it an optional alias.
    table2 AS alias2: This specifies the second table name and assigns it an optional alias.
    ON alias1.column_name = alias2.column_name (Optional): This defines the join condition using qualified column names from both tables.

Example: Combining Customers and Orders (with or without Matches):

Imagine you have Customers and Orders tables. To retrieve all customer information and order details (including those without matches), you can use a full outer join:
SQL

SELECT c.customer_name, o.order_id, o.order_date
FROM Customers AS c
FULL OUTER JOIN Orders AS o ON c.customer_id = o.customer_id;

Use code with caution.

This query will include all customers from Customers and all orders from Orders. For customers with orders, their order details will be populated. For customers without orders, the order_id and order_date columns will be null. Similarly, for orders without a matching customer, the customer_name column will be null.

Benefits of Using Full Outer Joins:

    Comprehensive Data View: Full outer joins provide a complete picture of data from both tables, including unmatched rows, which can be valuable for data analysis.
    Identifying Missing Relationships: By showing null values, full outer joins help you identify potential missing relationships or data gaps between the two tables.

When to Use Full Outer Joins:

    Full outer joins are suitable when you need to see all data from both tables, even if there are no corresponding matches in the other table.
    For instance, to analyze all customers (including those who haven't ordered yet) along with all orders (including those from unregistered customers).

Important Considerations:

    Full outer joins can result in larger result sets compared to other join types.
    Carefully consider whether you need null values in the results to avoid misinterpretations.
    In some cases, a combination of left join and right join might achieve the same outcome as a full outer join, depending on the specific scenario.

By understanding full outer joins and their applications, you can effectively retrieve and analyze data from relational databases, ensuring a complete view of information from both tables, even when there might not be perfect one-to-one matches between all rows.

---------------------------------------------------------------------
	CROSS JOIN
---------------------------------------------------------------------

lab: Try it
-------------
---------------------------------------
---------------------------------------
In SQL, a CROSS JOIN, also known as a Cartesian product, is a join operation that combines rows from two tables and returns every single possible combination of rows between them. It differs from other join types (inner join, left join, etc.) in its approach to data retrieval.

Understanding Cross Joins:

    A cross join multiplies (or creates a Cartesian product of) the rows from two tables, resulting in a large dataset.
    Every row from the first table is paired with every row from the second table, regardless of any specific matching criteria.
    This can be useful in specific scenarios but can also lead to a significant increase in the size of the result set, especially for large tables.

Core Functionalities:

    Specifying Tables: Similar to other joins, you provide the table names in the FROM clause.
    No Join Condition: Unlike most joins that use the ON clause, cross joins typically don't require a join condition. The core operation is the pairing of all rows from one table with all rows from the other.

Syntax:

There are two common ways to perform a cross join:

    Using the CROSS JOIN keyword:

SQL

SELECT column1, column2, ...
FROM table1 AS alias1
CROSS JOIN table2 AS alias2;

Use code with caution.

    Using a comma (,) between table names (supported by some databases):

SQL

SELECT column1, column2, ...
FROM table1 AS alias1, table2 AS alias2;

Use code with caution.

    column1, column2, ...: These represent the columns you want to retrieve from the tables.
    table1 AS alias1: This specifies the first table name and assigns it an optional alias.
    table2 AS alias2: This specifies the second table name and assigns it an optional alias.

Example: Combining Customers and Products (All Possible Combinations):

Imagine you have Customers and Products tables. To retrieve every possible combination of a customer with a product (even if they haven't purchased that product), you can use a cross join:
SQL

SELECT c.customer_name, p.product_name
FROM Customers AS c
CROSS JOIN Products AS p;

Use code with caution.

This query will result in a large dataset listing every customer paired with every product, regardless of actual purchases.

Benefits of Using Cross Joins:

    Generating All Possible Combinations: Cross joins are useful when you need to explicitly create all possible combinations of rows from two tables.
    Simulating Many-to-Many Relationships: In some cases, cross joins can be used to simulate many-to-many relationships between tables (which aren't directly supported by most relational databases).

When to Use Cross Joins (with Caution):

    Cross joins should be used cautiously due to the potential for generating a massive result set, especially for large tables.
    Consider alternative approaches like using joins with appropriate conditions or creating separate tables for many-to-many relationships if performance and data accuracy are critical.

Important Considerations:

    Carefully evaluate if a cross join is the most efficient way to achieve your desired results.
    Be mindful of the potential performance impact, especially when dealing with large datasets.
    In many cases, other join types with proper conditions might be more suitable for retrieving relevant data.

By understanding cross joins and their applications, you can make informed decisions when working with relational databases. They can be a valuable tool in specific situations, but it's crucial to weigh the benefits against the potential drawbacks to ensure efficient data retrieval and avoid overwhelming results.

---------------------------------------------------------------------
Module 15	Module 15: Sorting & Grouping
---------------------------------------------------------------------
lab: Try it
-------------
---------------------------------------
---------------------------------------

In SQL, sorting and grouping are essential techniques for organizing and summarizing data retrieved from your relational database. Let's explore these concepts:

Sorting

    Sorting allows you to arrange your query results in a particular order, making your data easier to analyze and interpret.
    You can sort based on one or more columns, either in ascending (A-Z or lowest to highest values) or descending (Z-A or highest to lowest values) order.

How to Sort:

The ORDER BY clause is used to specify the sorting criteria within your SQL query.

    Syntax:

SQL

SELECT column1, column2, ...
FROM table_name
ORDER BY column_name ASC|DESC;

Use code with caution.

    column_name: This represents the column you want to sort by.
    ASC|DESC: This specifies the sorting order (ascending or descending). You can sort by multiple columns by listing them comma-separated within the ORDER BY clause.

Example: Sorting Customers by Name:
SQL

SELECT customer_id, customer_name
FROM Customers
ORDER BY customer_name ASC;

Use code with caution.

This query sorts customers alphabetically by their customer_name in ascending order (A-Z).

Grouping

    Grouping helps you categorize and summarize your data based on shared characteristics.
    It involves grouping rows with similar values in a particular column and then performing aggregate functions (functions that operate on entire groups) to obtain meaningful insights.

How to Group:

The GROUP BY clause is used to define the grouping criteria within your SQL query.

    Syntax:

SQL

SELECT column1, aggregate_function(column2)
FROM table_name
GROUP BY column1;

Use code with caution.

    column1: This represents the column used to group the data.
    aggregate_function(column2): This specifies an aggregate function applied to another column (column2) within each group. Common aggregate functions include:
        COUNT(): Counts the number of rows in each group.
        SUM(): Calculates the total value for a numeric column in each group.
        AVG(): Computes the average value for a numeric column in each group.
        MIN(): Returns the minimum value for a column in each group.
        MAX(): Returns the maximum value for a column in each group.

Example: Counting Customers by Country:
SQL

SELECT country, COUNT(*) AS total_customers
FROM Customers
GROUP BY country;

Use code with caution.

This query groups customers by their country and uses the COUNT(*) function to count the number of customers in each country.

Combining Sorting and Grouping:

    You can often combine ORDER BY and GROUP BY clauses in the same query to achieve a more refined organization of your results.
    Typically, the GROUP BY clause is specified before the ORDER BY clause.

Benefits of Sorting and Grouping:

    Improved Data Readability: Sorting and grouping make your data easier to understand and interpret by presenting it in a structured and organized manner.
    Efficient Data Analysis: Organizing data into groups allows you to perform calculations and summarization more effectively, facilitating data analysis.

By mastering sorting and grouping techniques in SQL, you can unlock the power of organizing and summarizing your database information, transforming raw data into insightful and actionable knowledge.
---------------------------------------------------------------------
	ORDER BY Clause
---------------------------------------------------------------------
lab: Try it
-------------
---------------------------------------
---------------------------------------
The ORDER BY clause in SQL is used to sort the results of your query based on specific criteria. It allows you to arrange the retrieved data in a particular order, making it easier to analyze and interpret.

Understanding ORDER BY:

    You can sort by one or more columns in your query results.
    Sorting can be done in either ascending (A-Z or lowest to highest values) or descending (Z-A or highest to lowest values) order.

Syntax:
SQL

SELECT column1, column2, ...
FROM table_name
ORDER BY column_name1 ASC|DESC, column_name2 ASC|DESC, ...;

Use code with caution.

    column1, column2, ...: These represent the columns you want to retrieve from the table.
    table_name: This specifies the table from which you're retrieving data.
    column_name1, column_name2, ...: These represent the columns you want to sort by. You can specify multiple columns for sorting data hierarchically.
    ASC|DESC: This keyword indicates the sorting order for each column.
        ASC: Sorts in ascending order (default).
        DESC: Sorts in descending order.

Sorting by Multiple Columns:

The ORDER BY clause allows you to sort by multiple columns, enabling you to create a hierarchical sorting order. The columns listed first in the ORDER BY clause are used for the primary sorting, and subsequent columns are used for secondary or tertiary sorting within the already sorted groups based on the previous column(s).

Example: Sorting Customers by Name and Country:
SQL

SELECT customer_id, customer_name, country
FROM Customers
ORDER BY country ASC, customer_name DESC;

Use code with caution.

This query sorts customers first by country in ascending order (A-Z). Within each country, customers are then sorted by customer_name in descending order (Z-A).

Sorting with NULL Values:

By default, NULL values are usually treated as if they have the lowest value when using ORDER BY. You can use the ISNULL function to handle NULL values explicitly in your sorting criteria.

Important Considerations:

    Sorting data can impact performance, especially for large datasets. Consider sorting only the necessary data after retrieval in some cases.
    Ensure you're sorting by appropriate data types. Sorting by a mix of string and numeric values might not produce the expected outcome.

When to Use ORDER BY:

    Whenever you want to organize your query results in a specific order to improve readability or facilitate analysis based on specific criteria.
    For instance, sorting a list of products by price (ascending or descending) or sorting customer information by country and then by name within each country.

By effectively using the ORDER BY clause, you can significantly enhance the clarity and usability of your SQL query results, making it easier to draw meaningful insights from your data.


---------------------------------------------------------------------
	GROUP BY Clause
---------------------------------------------------------------------

lab: Try it
-------------
---------------------------------------
---------------------------------------

The GROUP BY clause in SQL is a powerful tool for organizing and summarizing data retrieved from relational databases. It allows you to categorize rows with similar values in a column and then perform aggregate functions to obtain meaningful insights.

Understanding GROUP BY:

    Grouping Data: GROUP BY groups rows in a table based on the values in a particular column (or set of columns). This creates partitions of data with shared characteristics.
    Aggregate Functions: After grouping, you can apply aggregate functions to each group. These functions operate on the entire group of rows, summarizing the data within each category.

Common Aggregate Functions:

    COUNT(*): Counts the number of rows in each group.
    SUM(column_name): Calculates the total value for a numeric column in each group.
    AVG(column_name): Computes the average value for a numeric column in each group.
    MIN(column_name): Returns the minimum value for a column in each group.
    MAX(column_name): Returns the maximum value for a column in each group.

Syntax:
SQL

SELECT column1, aggregate_function(column2)
FROM table_name
GROUP BY column1;

Use code with caution.

    column1: This represents the column used to group the data.
    aggregate_function(column2): This specifies an aggregate function applied to another column (column2) within each group.

Example: Counting Customers by Country:
SQL

SELECT country, COUNT(*) AS total_customers
FROM Customers
GROUP BY country;

Use code with caution.

This query groups customers by their country and uses the COUNT(*) function to count the number of customers in each country. The result might look like:
Country	Total Customers
USA	100
France	55
Germany	78

Benefits of Using GROUP BY:

    Improved Data Readability: Grouping data makes your results easier to understand and interpret by presenting information in categories.
    Efficient Data Analysis: By organizing data into groups, you can perform calculations and summarization more effectively, facilitating data analysis.
    Identifying Trends and Patterns: Grouping allows you to see how data is distributed across different categories, helping you identify trends and patterns within your data.

Using GROUP BY with SELECT and WHERE Clauses:

    GROUP BY is often used in conjunction with SELECT to specify the columns you want to retrieve after grouping.
    You can also use WHERE to filter data before grouping to focus on specific subsets of your data.

Important Considerations:

    You can group by multiple columns to create more granular categories for analysis.
    GROUP BY is typically used before ORDER BY in a query to organize data before sorting.

By mastering the GROUP BY clause and its applications, you can transform raw data into valuable summaries and insights, making it a cornerstone for effective data analysis using SQL.


---------------------------------------------------------------------
	HAVING Clause
---------------------------------------------------------------------

lab: Try it
-------------
---------------------------------------
---------------------------------------

The HAVING clause in SQL acts as a filter specifically for grouped data obtained using the GROUP BY clause. It allows you to refine your results by applying conditions after the grouping has been performed.

Understanding HAVING:

    The WHERE clause filters individual rows before grouping, while HAVING filters groups of rows based on aggregate functions applied within the groups.
    You can use various comparison operators (e.g., =, >, <) and logical operators (e.g., AND, OR, NOT) to define filter conditions for the groups.

Syntax:
SQL

SELECT column1, aggregate_function(column2)
FROM table_name
GROUP BY column1
HAVING condition_on_aggregate_function;

Use code with caution.

    column1: This represents the column used to group the data.
    aggregate_function(column2): This specifies an aggregate function applied to another column (column2) within each group.
    condition_on_aggregate_function: This defines the filtering condition using a comparison operator or logical operator on the aggregate function's result for each group.

Example: Finding Countries with More Than 100 Customers:
SQL

SELECT country, COUNT(*) AS total_customers
FROM Customers
GROUP BY country
HAVING COUNT(*) > 100;

Use code with caution.

This query groups customers by country and counts the number of customers in each country using COUNT(*). Then, the HAVING clause filters the results to include only countries with more than 100 customers.

Benefits of Using HAVING:

    Precise Filtering of Groups: HAVING enables you to filter grouped data based on aggregate values, providing more control over the results.
    Identifying Specific Trends: By filtering groups, you can focus on specific trends or patterns within your data that meet your criteria.

When to Use HAVING vs. WHERE:

    Use WHERE to filter individual rows before grouping if the condition applies to the raw data itself.
    Use HAVING to filter groups of rows after applying aggregate functions to focus on specific group characteristics.

Important Considerations:

    HAVING can only be used with aggregate functions and must follow the GROUP BY clause.
    You can use multiple conditions within the HAVING clause using logical operators.

By understanding the HAVING clause and its role alongside GROUP BY, you can effectively filter and analyze grouped data in your SQL queries, extracting the most relevant and insightful information from your database.

---------------------------------------------------------------------
Module 16	Module 16: Stored Procedures, Views & Triggers
---------------------------------------------------------------------


In a relational database management system (DBMS), Stored Procedures, Views, and Triggers are powerful tools that enhance database functionality and improve code organization, maintainability, and security. Here's an overview of each:

1. Stored Procedures:

    Concept: Stored procedures are pre-compiled modules containing a set of SQL statements and logic that can be executed as a single unit. They act like reusable functions within the database.
    Benefits:
        Code Reusability: Stored procedures eliminate the need to write the same SQL code repeatedly, reducing development time and improving consistency.
        Modularization: They break down complex tasks into smaller, manageable units, promoting better code organization and maintainability.
        Security: By centralizing logic in stored procedures, you can control access and permissions more effectively.
        Parameterization: Stored procedures can accept input parameters, making them flexible and adaptable for various use cases.

2. Views:

    Concept: Views are virtual tables that represent a subset or transformation of data from underlying base tables. They offer a simplified view of the data without modifying the actual structure.
    Benefits:
        Data Abstraction: Views shield users from the complexities of the underlying table structure, simplifying data access and reducing errors.
        Data Security: Views can restrict access to sensitive data columns or rows in the base tables.
        Logical Data Organization: They can present data in a way that aligns with user needs, improving data comprehension.

3. Triggers:

    Concept: Triggers are special database objects that automatically execute specific SQL statements in response to certain events (like INSERT, UPDATE, or DELETE) on a table. They act like event-driven programs within the database.
    Benefits:
        Enforcing Data Integrity: Triggers can automatically validate data, maintain referential integrity, and ensure data consistency within the database.
        Automating Tasks: They can automate repetitive tasks like logging changes, updating dependent tables, or sending notifications based on database events.
        Encapsulating Business Logic: Triggers can encapsulate complex business logic that should be applied whenever specific database events occur.

Choosing the Right Tool:

    Use Stored Procedures for complex data manipulation logic or tasks that need to be reused frequently.
    Use Views for simplifying data access, hiding complexities, or restricting data visibility.
    Use Triggers for enforcing data integrity, automating tasks based on database events, or implementing business logic tied to specific actions.

By effectively combining Stored Procedures, Views, and Triggers, you can create a more robust, secure, and maintainable database environment that caters to different data access and manipulation needs.


---------------------------------------------------------------------
	Definition and Purpose
---------------------------------------------------------------------


Stored Procedures, Views & Triggers in DBMS: Definition and Purpose

In a relational database management system (DBMS), Stored Procedures, Views, and Triggers are functionalities that extend the capabilities of your database beyond basic data retrieval and manipulation. Let's delve deeper into their definitions and purposes:

1. Stored Procedures:

    Definition: Stored procedures are essentially pre-written blocks of SQL statements along with control flow logic (like loops and conditional statements) that reside within the database. You can think of them as reusable functions stored in the database itself.
    Purpose: Stored procedures offer several advantages:
        Reusability: Eliminate repetitive coding by creating a central function that can be executed with different parameters for various use cases. This saves development time and improves code consistency.
        Modularization: Break down complex database operations into smaller, manageable units. This enhances code organization and maintainability.
        Security: Centralize data access logic within stored procedures, allowing for more granular control over user permissions and data security.
        Parameterization: Make procedures adaptable by enabling them to accept input parameters, allowing for dynamic behavior based on provided values.

2. Views:

    Definition: Views are essentially virtual tables that provide a simplified representation of data from one or more underlying base tables. They offer a specific lens to view the data without altering the actual structure of the base tables.
    Purpose: Views serve several valuable purposes:
        Data Abstraction: Shield users from the complexities of the underlying database schema. Views present data in a way that aligns with user needs, making it easier to understand and work with.
        Data Security: Views can restrict access to sensitive data columns or rows in the base tables, enhancing data security.
        Logical Data Organization: Allow you to present data in a way that aligns with user needs or specific functionalities, improving data comprehension and reducing complexity for end users.

3. Triggers:

    Definition: Triggers are special database objects that act like event-driven programs. They are essentially SQL statements that are automatically executed in response to specific events (like INSERT, UPDATE, or DELETE) occurring on a particular table.
    Purpose: Triggers offer a powerful way to automate tasks and enforce data integrity:
        Data Integrity Enforcement: Triggers can be used to validate data being inserted or updated, maintain referential integrity between tables (ensuring data consistency), and prevent invalid data from entering the database.
        Automation: Automate tasks like logging changes to a table, updating dependent tables based on modifications, or sending notifications based on database events.
        Encapsulating Business Logic: Triggers can encapsulate complex business logic that should be applied whenever specific database events occur. This centralizes the logic and improves maintainability.

In essence:

    Stored procedures focus on reusable data manipulation logic.
    Views provide a simplified lens to access and understand data.
    Triggers automate tasks and enforce data integrity based on database events.

By effectively using these functionalities together, you can create a more robust, secure, and well-organized database environment that caters to different data access and manipulation needs.

---------------------------------------------------------------------
	Syntax and Examples
---------------------------------------------------------------------



Stored Procedures
-----------------
lab: Try it
-------------
---------------------------------------
---------------------------------------


Views
-----------------
lab: Try it
-------------
---------------------------------------
---------------------------------------


Triggers
-----------------
lab: Try it
-------------
---------------------------------------
---------------------------------------

---------------------------------------------------------------------
	Permisssions
---------------------------------------------------------------------

lab: Try it
-------------
---------------------------------------


CREATE USER new_user_name@host_name IDENTIFIED BY 'password';

GRANT SELECT ON database_name.table_name TO new_user_name@host_name;

FLUSH PRIVILEGES;



CREATE USER new_user_name@host_name IDENTIFIED BY 'password';
GRANT SELECT, UPDATE, DELETE ON database_name.table_name TO new_user_name@host_name;
FLUSH PRIVILEGES;

check if select, update and delete can be done.

REVOKE UPDATE, DELETE ON database_name.table_name FROM new_user_name@host_name;

FLUSH PRIVILEGES;



---------------------------------------


In SQL, permissions are a crucial security mechanism that controls access to database objects (like tables, views, stored procedures, etc.) and the actions users can perform on them. They dictate who can:

    Read data from a table
    Insert new data into a table
    Update existing data in a table
    Delete data from a table
    Execute stored procedures
    Grant or Revoke permissions to other users

Understanding Permissions:

    Permissions are assigned to database users (or roles) and are associated with specific securables (database objects).
    There's a hierarchical structure, where permissions granted at a higher level (e.g., database) often apply to lower levels (e.g., tables within that database), unless explicitly denied.
    Permissions are typically managed using dedicated SQL statements like GRANT, DENY, and REVOKE.

Types of Permissions:

    Data Manipulation Language (DML) permissions: 
		Control actions like 
			SELECT, 
			INSERT, 
			UPDATE, and 
			DELETE on tables.
    Data Definition Language (DDL) permissions: 
		Allow users to 
			create, 
			alter, or 
			drop 
				database objects like 
					tables, 
					views, and 
					stored procedures.
    Control permissions: 
		Enable 
			granting, 
			denying, or 
			revoking permissions to other users.

Benefits of Using Permissions:

    Data Security: 
		Ensures only authorized users can access and modify data, protecting sensitive information.
    Data Integrity: 
		Helps maintain data consistency by restricting unauthorized modifications.
    Improved Manageability: 
		Granular control over user access simplifies database administration.

Common Permission Management Scenarios:

    Granting read-only access to a specific table for a reporting user.
    Allowing a data entry user to insert new data but not modify existing records.
    Restricting a developer from deleting tables but permitting them to create new ones.

Important Considerations:

    Permissions should be granted based on the principle of least privilege, giving users only the access they need to perform their tasks.
    Regularly review and adjust permissions to ensure they remain aligned with user roles and responsibilities.
    Be cautious when granting DDL permissions, as they can significantly impact the database structure.

By effectively managing permissions in SQL, you can create a secure and controlled environment for accessing and manipulating your valuable database information.


---------------------------------------------------------------------
	Grant permission
---------------------------------------------------------------------

already covered 
---------------

In SQL, granting permissions allows you to control what users can do with database objects like tables, views, and stored procedures. It's a fundamental security measure to ensure only authorized users have access to specific functionalities.

Here's an overview of how to grant permissions in SQL:

Understanding the GRANT Statement:

    The GRANT statement is used to assign permissions to database users or roles.
    It specifies the permissions being granted, the user or role receiving the permissions, and the object(s) on which the permissions are being granted.

Basic Syntax:
SQL

GRANT permission_name ON object_name TO {user_name | role_name} [WITH GRANT OPTION];

Use code with caution.

    permission_name: This specifies the specific permission being granted (e.g., SELECT, INSERT, UPDATE, DELETE, etc.).
    object_name: This refers to the database object on which the permission is being granted (e.g., a table name, view name, or stored procedure name).
    user_name: This is the name of the database user to whom the permission is being granted. Alternatively, you can use role_name to grant permissions to a database role.
    [WITH GRANT OPTION]: This optional clause allows the grantee (user or role) to grant the same permission to other users. Use this with caution, as it grants a higher level of privilege.

Example: Granting Read-Only Access to a Table:
SQL

GRANT SELECT ON MyTable TO accounting_user;

Use code with caution.

This statement grants the user named accounting_user the permission to only select data from the table MyTable. They won't be able to insert, update, or delete data in the table.

Granting Multiple Permissions:

You can grant multiple permissions within a single GRANT statement by separating them with commas:
SQL

GRANT SELECT, INSERT ON MyTable TO sales_user;

Use code with caution.

This grants the user sales_user both SELECT and INSERT permissions on the MyTable.

Granting Permissions on All Columns (or Specific Columns):

By default, permissions apply to all columns in a table. You can specify a list of columns within parentheses after the table name to grant permissions on specific columns:
SQL

GRANT UPDATE (column1, column2) ON MyTable TO editor_user;

Use code with caution.

This grants the user editor_user the ability to update only the column1 and column2 in the MyTable.

Important Considerations:

    Permissions are hierarchical. For instance, granting SELECT permission often implicitly grants READ permission as well.
    Be cautious when granting GRANT OPTION, as it allows the grantee to propagate permissions further.
    Consider using roles to simplify permission management for groups of users.

By effectively using the GRANT statement and understanding permission concepts, you can create a secure and controlled database environment where users have the necessary access to perform their tasks without compromising data integrity or security.



---------------------------------------------------------------------
	Revoke permission
---------------------------------------------------------------------
already covered 
---------------

In SQL, revoking permissions is essential for maintaining database security. It allows you to restrict user access to database objects (like tables, views, and stored procedures) when their privileges need to be changed or removed.

Understanding REVOKE:

    The REVOKE statement is used to withdraw previously granted permissions from database users or roles.
    It specifies the permissions being revoked, the user or role from whom the permissions are being taken away, and the object(s) on which the permissions were granted.

Basic Syntax:
SQL

REVOKE permission_name ON object_name FROM {user_name | role_name} [GRANT OPTION];

Use code with caution.

    permission_name: This specifies the specific permission being revoked (e.g., SELECT, INSERT, UPDATE, DELETE, etc.).
    object_name: This refers to the database object from which the permission is being revoked (e.g., a table name, view name, or stored procedure name).
    user_name: This is the name of the database user from whom the permission is being revoked. Alternatively, you can use role_name to revoke permissions from a database role.
    [GRANT OPTION]: This optional clause is used to revoke the ability to grant further permissions, if it was previously granted with the GRANT OPTION.

Example: Revoking Update Permission from a Table:
SQL

REVOKE UPDATE ON MyTable FROM sales_user;

Use code with caution.

This statement revokes the UPDATE permission from the user sales_user on the table MyTable. They will no longer be able to modify data in that table.

Revoking All Permissions:

    To revoke all permissions on an object for a user or role, you can use ALL in place of permission_name:

SQL

REVOKE ALL ON MyTable FROM marketing_user;

Use code with caution.

This removes all permissions (e.g., SELECT, INSERT, UPDATE, DELETE) that the user marketing_user had on the MyTable.

Important Considerations:

    Revoking permissions can cascade upwards. If a user has permissions granted through a role, revoking the permission from the role will also remove it from the user.
    Revoking GRANT OPTION only prevents the user from granting the permission further, but it doesn't revoke the permission itself from the user.
    Use REVOKE cautiously, as it can restrict users from performing their jobs if they lose necessary access.

By understanding how to revoke permissions using REVOKE and its implications, you can ensure that your database remains secure and that user access is aligned with their roles and responsibilities.








---------------------------------------------------------------------
Module 17	Module 17: Introduction to NoSQL
---------------------------------------------------------------------



Introduction to NoSQL
	NoSQL, which stands for "not only SQL" or "non-relational", refers to a category of database management systems (DBMS) that differ from traditional relational databases. Here's a breakdown of key concepts:

Relational vs. NoSQL Databases:

Relational Databases:

	Store data in structured tables with rows and columns.
	Enforce relationships between tables using foreign keys.
	Utilize SQL (Structured Query Language) for data manipulation.
	Well-suited for structured, pre-defined data models.
NoSQL Databases:

	Offer more flexible data models: document, key-value, graph, etc.
	Do not rely on rigid table structures.
	Often employ different query languages or APIs for data access.
	Provide scalability and performance advantages for large datasets or unstructured data.
Why Use NoSQL?

	Several factors contribute to the growing popularity of NoSQL databases:

		Scalability: NoSQL databases can easily scale horizontally by adding more servers to handle growing data volumes.
		Flexibility: NoSQL schemas can adapt to changing data structures without complex schema migrations.
		Performance: NoSQL databases can deliver faster performance for specific operations, particularly with large datasets.
		Big Data: NoSQL excels at handling massive datasets that might overwhelm traditional relational databases.
		Unstructured Data: NoSQL is well-suited for storing and managing unstructured data like JSON documents or social media content.
Types of NoSQL Databases:

	Document Stores: Data is stored as key-value pairs where the value is a JSON-like document containing rich data structures. (e.g., MongoDB, Couchbase)
	Key-Value Stores: Simplest NoSQL type, data is stored as key-value pairs with atomic operations (add, update, delete). (e.g., Redis, Memcached)
	Wide-Column Stores: Designed for large datasets with variable-length columns. Useful for time-series data or sensor readings. (e.g., Cassandra, ScyllaDB)
	Graph Databases: Represent data as nodes (entities) and edges (relationships) for efficient querying of interconnected data. (e.g., Neo4j, Titan)
Choosing the Right NoSQL Database:

The selection of a NoSQL database depends on your specific needs:

	Data Model: Consider the structure and complexity of your data.
	Scalability Requirements: How much data growth do you anticipate?
	Performance Needs: Prioritize read/write speeds or query complexity.
	Data Access Patterns: How will you typically access and manipulate the data?
Benefits of NoSQL:

	Scalability and Performance: Efficiently handle large datasets and high traffic.
	Flexibility: Adapt to evolving data models without schema restrictions.
	Improved Developer Experience: Simpler development for specific use cases.
Challenges of NoSQL:

	ACID Transactions: NoSQL databases often offer weaker consistency guarantees compared to relational databases.
	Querying: NoSQL querying languages might differ from familiar SQL, requiring a learning curve.
	Data Integrity: Maintaining data consistency across a distributed NoSQL system can be more complex.


---------------------------------------------------------------------
	Definition and Purpose
---------------------------------------------------------------------


NoSQL, standing for "not only SQL" or "non-relational," refers to a category of database management systems (DBMS) that differ from traditional relational databases in their structure and capabilities. Here's a breakdown of what NoSQL offers:

Core Concept:

Relational databases store data in rigid, predefined tables with rows and columns. They enforce relationships between tables using foreign keys and rely on SQL for data manipulation.
NoSQL databases, on the other hand, provide more flexibility. They offer various data models, including document, key-value, graph, and wide-column, allowing for a more dynamic approach to storing and managing data.
Why Use NoSQL?

Several factors make NoSQL a compelling choice for specific scenarios:

Scalability: NoSQL systems excel at horizontal scaling. By adding more servers, you can easily handle growing data volumes without performance bottlenecks.
Flexibility: Unlike relational databases with predefined schemas, NoSQL allows for evolving data structures. You can adapt your data model as your needs change without complex schema migrations.
Performance: NoSQL databases often deliver faster performance for specific operations, particularly when dealing with large or unstructured datasets.
Big Data: NoSQL is well-suited for managing massive datasets that might overwhelm traditional relational databases.
Unstructured Data: NoSQL excels at storing and handling unstructured data like JSON documents, social media content, or sensor readings.
---------------------------------------------------------------------
	Evolution and History of NoSQL
---------------------------------------------------------------------

The concept of NoSQL databases has evolved significantly over time, becoming a cornerstone for handling massive datasets and diverse data structures. Here's a glimpse into NoSQL's history and evolution:

Early Days (1960s - 1990s):

Non-relational Roots: The seeds of NoSQL were sown in the 1960s with the development of non-relational databases like IBM's IMS (Integrated Management System) and hierarchical databases like CODASYL (Conference on Data Systems Languages).
Focus on Specific Needs: These early systems catered to specific use cases, offering performance and flexibility advantages for certain data types.
The Birth of "NoSQL" (Late 1990s - Early 2000s):

Coining the Term: The term "NoSQL" first appeared in 1998, used by Carlo Strozzi to describe his lightweight relational database that didn't utilize the standard SQL interface.
Rise of Web 2.0: The explosion of web applications in the early 2000s, characterized by user-generated content and social interaction, demanded new data management solutions.
Relational Limitations: Traditional relational databases struggled to handle the scale, flexibility, and variety of data generated by Web 2.0 applications.
The NoSQL Boom (Mid 2000s - Present):

A Surge in Popularity: As web applications continued to grow, NoSQL databases gained significant traction. Open-source projects like MongoDB and Cassandra emerged, offering scalable and flexible alternatives for data storage.
Diversification of NoSQL Stores: Different types of NoSQL databases like document stores, key-value stores, and wide-column stores became prominent, each catering to specific data models and access patterns.
Cloud Adoption: The rise of cloud computing further fueled NoSQL adoption. Cloud platforms offered readily available NoSQL database services, making it easier for developers to leverage their capabilities.
The Present and Future of NoSQL:

Polyglot Persistence: Today, many applications utilize a combination of relational and NoSQL databases, choosing the right tool for the job based on data characteristics and access patterns.
Focus on Integration: Solutions are emerging to bridge the gap between relational and NoSQL databases, allowing for seamless data movement and querying across systems.
New Use Cases: NoSQL continues to evolve, finding applications in areas like Internet of Things (IoT) data management, real-time analytics, and complex graph-based relationships.
Key Takeaways:

NoSQL databases have come a long way, from niche solutions to mainstream data management tools.
Their evolution reflects the changing nature of data and the demands of modern applications.
The future of NoSQL lies in continuous innovation, integration with other systems, and addressing new data challenges.

---------------------------------------------------------------------
	Key-Value Stores
---------------------------------------------------------------------

Key-value stores, as you've learned, are a fundamental type of NoSQL database designed for simplicity, speed, and scalability. They function differently from relational databases and offer unique advantages for specific use cases.

Core Functionality:

Data Model: Unlike relational databases with structured tables and rows, key-value stores operate on a key-value pair model. Each data item is associated with a unique key that acts as an identifier.
Values: The value stored can be any data type, including strings, numbers, JSON documents, or even entire objects, providing flexibility in data representation.
Basic Operations:

Key-value stores provide essential operations for interacting with data:

SET: Add a new key-value pair or update an existing one.
GET: Retrieve the value associated with a specific key.
DELETE: Remove a key-value pair from the store.
Strengths of Key-Value Stores:

Simplicity: The key-value model is easy to understand and implement, reducing development complexity.


---------------------------------------------------------------------
	Document Stores
---------------------------------------------------------------------


Document stores are another essential category of NoSQL databases, offering a flexible and scalable approach to data storage and retrieval. Here's a breakdown of their key characteristics and use cases:

Core Concept:

Unlike relational databases with rigid tables and rows, document stores utilize a document-oriented data model.
Documents are self-contained units of data, often stored in JSON (JavaScript Object Notation) format or similar flexible schema formats like YAML or XML.
Each document can have its own structure, containing various data types like strings, numbers, arrays, and even nested documents. This flexibility allows for storing diverse data without the limitations of predefined tables.
Advantages of Document Stores:

Flexibility: Document stores excel at handling data with varying structures or that evolves over time. The schema-less nature allows for adding new fields to documents without altering the entire database structure.
Ease of Use: Working with documents often feels more intuitive for developers familiar with JSON or other similar data formats. This can simplify data manipulation and development.
Scalability: Document stores can scale horizontally by adding more servers, making them suitable for large datasets.
Rich Queries: Compared to key-value stores, document stores offer richer querying capabilities. You can query documents based on specific fields or their hierarchical structure using query languages like JSONiq or XPath.
Common Use Cases:

Content Management Systems (CMS): Documents containing website content, articles, or product information can be efficiently stored and managed in document stores.
E-commerce Applications: Product data with varying attributes, user profiles, and shopping cart information can be well-represented in document stores.
Internet of Things (IoT): Sensor data with timestamps, readings, and device-specific details can be flexibly stored using document stores.
Real-time Analytics: Document stores can be used to store and query real-time data for various analytics applications.
Social Networking: User profiles, posts, comments, and other social data with varying structures can be managed effectively in document stores.
Popular Document Stores:

MongoDB: A leading open-source document store known for its flexibility, scalability, and rich query capabilities.
Couchbase: Another popular open-source document store offering high performance and scalability features.
Amazon DocumentDB: A managed document database service compatible with MongoDB APIs, offered by Amazon Web Services (AWS).
---------------------------------------------------------------------
	Column Stores
---------------------------------------------------------------------

Column stores, also known as wide-column stores, are a type of NoSQL database designed specifically for handling large datasets with variable-length columns. They offer efficient storage and retrieval mechanisms for data that might not fit well in traditional row-oriented databases.

Core Concept:

Unlike relational databases that store data in rows and columns, column stores organize data by columns instead of rows.
Each column is stored separately, allowing for efficient compression and optimization of data access. This is particularly beneficial for datasets with many columns but sparse data (where most entries in a column might be empty).
Column stores are well-suited for scenarios where you primarily query data based on specific columns or perform aggregations across large datasets.
Advantages of Column Stores:

Efficient Storage: By storing columns separately, column stores can compress data effectively, especially for sparse data. This reduces storage requirements and improves overall efficiency.
Fast Reads for Specific Columns: Retrieving data from specific columns is significantly faster in column stores compared to row-oriented databases. This is because only the relevant columns need to be accessed, minimizing data transfer.
Scalability: Column stores can scale horizontally by adding more servers to the cluster, making them ideal for handling massive datasets that grow over time.
Aggregations and Analytics: Column stores excel at performing aggregations (e.g., sum, average, count) across large datasets due to their column-based organization.
Common Use Cases:

Time-Series Data:
---------------------------------------------------------------------
	Graph Databases
---------------------------------------------------------------------
In the realm of NoSQL databases, graph databases stand out for their unique ability to represent and manage interconnected data. Here's a deep dive into what graph databases are and how they excel in specific use cases:

Core Concept:

Unlike relational databases that focus on tables and rows, graph databases utilize a graph data model. This model consists of nodes (entities) and edges (relationships) between those entities.
Nodes represent real-world objects like people, products, or social media posts.
Edges depict the connections and relationships between these nodes, allowing you to capture the inherent interconnectedness of your data.
Why Use Graph Databases?

Graph databases offer distinct advantages for specific scenarios:

Modeling Relationships: They excel at representing complex relationships between data entities. This is particularly valuable for tasks like social network analysis, recommendation systems, or fraud detection.
Efficient Queries: Traversing interconnected data in a graph database is often faster and more efficient compared to joining multiple tables in a relational database.
Scalability: Graph databases can scale horizontally by adding more servers to handle growing datasets and complex queries involving relationships.
Common Use Cases:

Social Networks: Graph databases are a natural fit for modeling social connections between users, analyzing user behavior, and recommending content or friends.
Fraud Detection: By analyzing relationships between transactions, entities, and locations, graph databases can help identify fraudulent activities.
Knowledge Graphs: Complex knowledge graphs containing entities, relationships, and properties can be effectively stored and queried using graph databases. This can be useful for building intelligent systems or chatbots.
Supply Chain Management: Tracking the movement of goods, materials, and their relationships with suppliers and vendors can be efficiently managed in graph databases.
Recommendation Systems: Analyzing user interactions and relationships between items can power personalized recommendations for products, content, or services.
Popular Graph Databases:

Neo4j: A leading open-source graph database known for its ease of use, scalability, and powerful query language (Cypher).
Titan: Another open-source graph database offering high performance and flexibility for large graphs.
Amazon Neptune: A managed graph database service from Amazon Web Services (AWS) designed for ease of use and scalability.
Things to Consider:

Learning Curve: Understanding the graph data model and query languages like Cypher might require additional learning compared to working with relational databases.
Schema Design: Designing an effective graph schema is crucial for optimal performance and query efficiency.
Data Model Compatibility: Not all data can be easily represented in a graph model. Evaluate if your data lends itself well to relationships and interconnectedness.
Choosing Graph Databases:

Graph databases are a powerful tool when:

Your data has inherent relationships and interconnectedness that you want to model and analyze.
You need to perform complex queries that involve traversing relationships between data entities.
Scalability and efficient querying of interconnected data are crucial for your application.
If your data primarily 

---------------------------------------------------------------------
New Module Added: Backup and Recovery in database 
---------------------------------------------------------------------


Backup and Recovery in Databases

1. What is Backup?

	Definition: A backup is a copy of your database files, stored separately from the primary database server.
	Purpose: To protect your data from loss due to:
	Hardware failures: Server crashes, disk failures
	Software errors: Corrupted data files, software bugs
	Human error: Accidental deletions, data modifications
	Natural disasters: Fires, floods, earthquakes
	Cyberattacks: Ransomware, hacking attempts
2. Types of Backups

	Full Backup: Copies the entire database, including all data files, logs, and control files.
	Pros: Easy to restore, provides a complete recovery point.
	Cons: Time-consuming, requires significant storage space.
	Differential Backup: Copies only the data that has changed since the last full backup.
	Pros: Faster and smaller than full backups.
	Cons: Requires a recent full backup for recovery.
	Incremental Backup: Copies only the data that has changed since the last backup (full or incremental).
	Pros: Fastest backup type, requires minimal storage space.
	Cons: Requires multiple backups for full recovery.
	Log-Based Backup: Copies the transaction log, which records all changes made to the database.
	Pros: Fast and efficient for point-in-time recovery.
	Cons: Requires a recent full or differential backup for recovery.
3. Recovery Strategies

	Recovery Point Objective (RPO): The maximum amount of data loss that an organization can tolerate.

	Recovery Time Objective (RTO): The target time within which a system must be restored to operation after a failure.

	Recovery Methods:

	Rollback: Reverts the database to a previous consistent state using backups and transaction logs.
	Rollforward: Applies the transaction log to a backup to bring the database to the current state.
	Media Recovery: Recovers the database from a complete backup and applies necessary log files.
4. Best Practices

	Regular Backups: Schedule regular backups (e.g., daily, weekly, monthly) based on data criticality and RPO/RTO requirements.
	Off-site Storage: Store backups off-site (e.g., in a cloud service, on a separate server) to protect against local disasters.
	Test Backups: Regularly test backup and recovery procedures to ensure they work as expected.
	Data Encryption: Encrypt backups to protect sensitive data during storage and transmission.
	Version Control: Keep multiple versions of backups to enable point-in-time recovery.
	Disaster Recovery Plan: Develop and maintain a comprehensive disaster recovery plan that outlines procedures for responding to different types of failures.
5. Database-Specific Features

	Most modern databases (e.g., Oracle, MySQL, SQL Server) provide built-in features for backup and recovery, including:
	Backup and recovery utilities.
	Log management tools.
	Point-in-time recovery options.





----------
Instruction to do database backup and recovery in mysql
----------------------------------------------------------------

1. Full Backup

Logical Backup:

 

mysqldump -u <username> -p<password> <database_name> > /path/to/backup/full_backup_<date>.sql
This command creates a SQL script containing all the database objects (tables, views, stored procedures, etc.) and their data.

Physical Backup:

 

mysqldump -u <username> -p<password> --single-transaction <database_name> > /path/to/backup/full_backup_<date>.sql
This option minimizes the risk of inconsistencies by performing the backup within a single transaction.

2. Differential Backup

Create a script (e.g., differential_backup.sh):

 

#!/bin/ 

timestamp=$(date +%Y%m%d%H%M%S) 
mysqldump -u <username> -p<password> --databases <database_name> --master-data=2 > /path/to/backup/differential_backup_$timestamp.sql 
3. Incremental Backup

Create a script (e.g., incremental_backup.sh):

 

#!/bin/ 

timestamp=$(date +%Y%m%d%H%M%S) 
mysqldump -u <username> -p<password> --databases <database_name> --master-data=2 --incremental > /path/to/backup/incremental_backup_$timestamp.sql 
4. Scheduling Backups (with cron)

Full backups daily:

Code snippet

0 2 * * * /path/to/full_backup.sh
Differential backups hourly:

Code snippet

0 * * * * /path/to/differential_backup.sh
5. Database Recovery

Full Recovery:

 

mysql -u <username> -p<password> <database_name> < /path/to/backup/full_backup_<date>.sql
Differential Recovery:

 

mysql -u <username> -p<password> <database_name> < /path/to/backup/full_backup_<date>.sql 
mysql -u <username> -p<password> <database_name> < /path/to/backup/differential_backup_<date>.sql
Incremental Recovery:

 

mysql -u <username> -p<password> <database_name> < /path/to/backup/full_backup_<date>.sql
mysql -u <username> -p<password> <database_name> < /path/to/backup/incremental_backup_<date1>.sql
mysql -u <username> -p<password> <database_name> < /path/to/backup/incremental_backup_<date2>.sql 
...
Important Notes:

Replace placeholders:
<username>
<password>
<database_name>
/path/to/backup/
Permissions: Ensure the user running the backup scripts has the necessary permissions.
Test Backups: Regularly test your backups to ensure they can be restored successfully.
Security: Consider encrypting your backups to enhance security.
Off-site Storage: Store backups off-site for disaster recovery.
MySQL Documentation: Refer to the official MySQL documentation for the most up-to-date and detailed information on backup and recovery procedures.
This detailed guide should help you effectively implement database backup and recovery strategies for your MySQL environment.


More details: https://phoenixnap.com/kb/how-to-backup-restore-a-mysql-database

---------------------------------------------------------------------
Module 18	Module 18: Types of NoSQL databases
---------------------------------------------------------------------

NoSQL databases come in various types, each with its own strengths and weaknesses. Here are some of the most common types: 

1. Document Databases:

	Data Model: 
		Stores data in flexible, 
		document-like structures (e.g., JSON, BSON). 
	Key Features:
		Schema-less or flexible schemas. 
	Easy to 
		store complex, hierarchical data. 
	Good for applications with 
		frequent data updates and 
		evolving data structures. 
	Examples: 
		MongoDB, 
		Couchbase, 
		DocumentDB 
2. Key-Value Stores:

	Data Model: 
		Stores data as key-value pairs. 
	Key Features:
		Simple and fast for 
			basic data retrieval and storage. 
		Ideal for 
			caching, 
			session management, and 
			storing simple data structures. 
		High performance and scalability. 
	Examples: 
		Redis, 
		Memcached, 
		Amazon ElastiCache 
3. Column-Family Stores (Wide-Column Stores): 

	Data Model: 
		Stores data in columns, 
			where each column represents a specific attribute. 
	Key Features:
		Efficient for storing and querying large amounts of data columnwise. 
		Well-suited for analytics and time-series data.
	Examples: 
		Cassandra, 
		HBase, 
		Amazon DynamoDB 
4. Graph Databases:

	Data Model: 
		Stores data as a 
			network of interconnected nodes (entities) and 
			relationships. 
	Key Features:
		Excellent for representing and querying complex relationships between data points. 
		Used for social networks, recommendation engines, and knowledge graphs. 
	Examples: 
		Neo4j, 
		JanusGraph, 
		Dgraph 
5. Time-Series Databases:

	Data Model:
		Specifically designed for handling time-stamped data. 
	Key Features:
		High write throughput and efficient querying of time-series data. 
		Used for 
			IoT data, 
			financial data, and 
			monitoring applications. 
	Examples: 
		InfluxDB, 
		TimescaleDB

 
---------------------------------------------------------------------
	Key-Value Stores (e.g., Redis)
---------------------------------------------------------------------



---------------------------------------------------------------------
	Document Stores (e.g., MongoDB)
---------------------------------------------------------------------


---------------------------------------------------------------------
	Column Stores (e.g., Cassandra)
---------------------------------------------------------------------







---------------------------------------------------------------------
	Graph Databases (e.g., Neo4j)
---------------------------------------------------------------------
D:\PraiseTheLord\HSBGInfotech\Others\vilas\nosql\neo4j

---------------------------------------------------------------------
Module 19	Module 19: MongoDB: An introduction
---------------------------------------------------------------------



---------------------------------------------------------------------
	Overview of MongoDB
---------------------------------------------------------------------



---------------------------------------------------------------------
	Features and Advantages
---------------------------------------------------------------------



---------------------------------------------------------------------
---------------------------------------------------------------------
---------------------------------------------------------------------
	Comparison with RDBMS
---------------------------------------------------------------------



---------------------------------------------------------------------



Reference: 	
https://www.w3schools.com/MySQL/default.asp
https://www.mysqltutorial.org/mysql-basics/

	
	select ifnull(courses.course_name, 'No Course') from students right left outer join courses where students.course_id = courses.course_id;